# Celery å®æ—¶ç›‘æ§ä½¿ç”¨æŒ‡å—

## ğŸ“‹ åŠŸèƒ½è¯´æ˜

Phase 1.5 å·²å®ç°è‡ªåŠ¨åŒ–æ•°æ®é‡‡é›†åŠŸèƒ½ï¼Œæ”¯æŒï¼š
- âš¡ **å®æ—¶ç›‘æ§**ï¼šæ¯5åˆ†é’Ÿè‡ªåŠ¨çˆ¬å–æœ€æ–°æ–°é—»
- ğŸ¥¶ **å†·å¯åŠ¨**ï¼šæ‰¹é‡å›æº¯å†å²æ•°æ®ï¼ˆ1-100é¡µï¼‰
- ğŸ“Š **ä»»åŠ¡è¿½è¸ª**ï¼šå®æ—¶æŸ¥çœ‹ä»»åŠ¡è¿›åº¦å’Œç»Ÿè®¡

---

## ğŸš€ å¿«é€Ÿå¯åŠ¨

### 1. åˆå§‹åŒ–æ•°æ®åº“ï¼ˆæ·»åŠ  crawl_tasks è¡¨ï¼‰

```bash
cd backend
python init_db.py
```

### 2. å¯åŠ¨ Redisï¼ˆå¦‚æœè¿˜æ²¡å¯åŠ¨ï¼‰

```bash
# æ£€æŸ¥ Docker Compose ä¸­çš„ Redis æ˜¯å¦è¿è¡Œ
docker ps | grep redis

# æˆ–è€…ç›´æ¥å¯åŠ¨æ‰€æœ‰æœåŠ¡
cd ../deploy
docker-compose -f docker-compose.dev.yml up -d
```

### 3. å¯åŠ¨ Celery Workerï¼ˆå¤„ç†ä»»åŠ¡ï¼‰

```bash
# Terminal 1: å¯åŠ¨ Worker
cd backend
celery -A app.core.celery_app worker --loglevel=info
```

### 4. å¯åŠ¨ Celery Beatï¼ˆå®šæ—¶è°ƒåº¦ï¼‰

```bash
# Terminal 2: å¯åŠ¨ Beatï¼ˆè°ƒåº¦å™¨ï¼‰
cd backend
celery -A app.core.celery_app beat --loglevel=info
```

### 5. å¯åŠ¨ FastAPI åç«¯

```bash
# Terminal 3: å¯åŠ¨åç«¯
cd backend
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

---

## ğŸ“¡ API ä½¿ç”¨è¯´æ˜

### 1. æŸ¥çœ‹ä»»åŠ¡åˆ—è¡¨

```bash
# æŸ¥çœ‹æ‰€æœ‰ä»»åŠ¡
curl http://localhost:8000/api/v1/tasks/

# æŒ‰æ¨¡å¼ç­›é€‰ï¼ˆrealtime, cold_startï¼‰
curl http://localhost:8000/api/v1/tasks/?mode=realtime

# æŒ‰çŠ¶æ€ç­›é€‰ï¼ˆpending, running, completed, failedï¼‰
curl http://localhost:8000/api/v1/tasks/?status=completed

# åˆ†é¡µ
curl http://localhost:8000/api/v1/tasks/?skip=0&limit=10
```

### 2. æŸ¥çœ‹ä»»åŠ¡è¯¦æƒ…

```bash
curl http://localhost:8000/api/v1/tasks/1
```

å“åº”ç¤ºä¾‹ï¼š
```json
{
  "id": 1,
  "celery_task_id": "3a8d2b1c-4f5e-6a7b-8c9d-0e1f2a3b4c5d",
  "mode": "realtime",
  "status": "completed",
  "source": "sina",
  "progress": {"current_page": 1, "total_pages": 1, "percentage": 100},
  "crawled_count": 25,
  "saved_count": 8,
  "execution_time": 12.34,
  "result": {
    "total_crawled": 50,
    "filtered": 25,
    "saved": 8,
    "duplicates": 17
  },
  "created_at": "2025-12-01T14:30:00",
  "completed_at": "2025-12-01T14:30:12"
}
```

### 3. è§¦å‘å†·å¯åŠ¨ï¼ˆæ‰¹é‡çˆ¬å–ï¼‰

```bash
# çˆ¬å–æ–°æµªè´¢ç» 1-50 é¡µ
curl -X POST http://localhost:8000/api/v1/tasks/cold-start \
  -H "Content-Type: application/json" \
  -d '{
    "source": "sina",
    "start_page": 1,
    "end_page": 50
  }'
```

å“åº”ï¼š
```json
{
  "success": true,
  "message": "å†·å¯åŠ¨ä»»åŠ¡å·²å¯åŠ¨: sina, é¡µç  1-50",
  "celery_task_id": "abc123..."
}
```

### 4. æŸ¥çœ‹ä»»åŠ¡ç»Ÿè®¡

```bash
curl http://localhost:8000/api/v1/tasks/stats/summary
```

å“åº”ï¼š
```json
{
  "total": 120,
  "by_status": {
    "completed": 115,
    "running": 2,
    "failed": 3
  },
  "by_mode": {
    "realtime": 100,
    "cold_start": 20
  },
  "recent_completed": 45,
  "total_news_crawled": 5000,
  "total_news_saved": 3200
}
```

---

## ğŸ” ç›‘æ§å’Œè°ƒè¯•

### 1. æŸ¥çœ‹ Celery Worker æ—¥å¿—

```bash
# Worker æ—¥å¿—ä¼šå®æ—¶æ˜¾ç¤ºä»»åŠ¡æ‰§è¡Œæƒ…å†µ
[2025-12-01 14:30:00] Task app.tasks.crawl_tasks.realtime_crawl_task[abc123] received
[2025-12-01 14:30:01] [Task 15] å¼€å§‹å®æ—¶çˆ¬å–: sina
[2025-12-01 14:30:10] [Task 15] çˆ¬å–åˆ° 50 æ¡æ–°é—»
[2025-12-01 14:30:10] [Task 15] è¿‡æ»¤åå‰©ä½™ 25 æ¡æ–°é—»
[2025-12-01 14:30:12] [Task 15] å®Œæˆ! çˆ¬å–: 50, è¿‡æ»¤: 25, ä¿å­˜: 8, è€—æ—¶: 11.23s
[2025-12-01 14:30:12] Task app.tasks.crawl_tasks.realtime_crawl_task[abc123] succeeded
```

### 2. æŸ¥çœ‹ Celery Beat è°ƒåº¦æ—¥å¿—

```bash
# Beat æ—¥å¿—ä¼šæ˜¾ç¤ºå®šæ—¶ä»»åŠ¡è§¦å‘
[2025-12-01 14:30:00] Scheduler: Sending due task crawl-sina-every-5min
[2025-12-01 14:35:00] Scheduler: Sending due task crawl-sina-every-5min
```

### 3. ç›‘æ§ Redis é˜Ÿåˆ—

```bash
# æŸ¥çœ‹é˜Ÿåˆ—é•¿åº¦
docker exec finnews_redis redis-cli LLEN celery

# æŸ¥çœ‹é˜Ÿåˆ—å†…å®¹
docker exec finnews_redis redis-cli LRANGE celery 0 -1
```

### 4. æŸ¥çœ‹æ•°æ®åº“ä¸­çš„ä»»åŠ¡

```bash
# è¿›å…¥ PostgreSQL
docker exec -it finnews_postgres psql -U finnews -d finnews_db

# æŸ¥è¯¢ä»»åŠ¡
SELECT id, mode, status, source, crawled_count, saved_count, created_at 
FROM crawl_tasks 
ORDER BY created_at DESC 
LIMIT 10;
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### ä¿®æ”¹å®šæ—¶ä»»åŠ¡é¢‘ç‡

ç¼–è¾‘ `backend/app/core/celery_app.py`:

```python
beat_schedule={
    "crawl-sina-every-5min": {
        "task": "app.tasks.crawl_tasks.realtime_crawl_task",
        "schedule": crontab(minute="*/5"),  # â† æ”¹è¿™é‡Œ
        "args": ("sina",),
    },
}
```

é¢‘ç‡é€‰é¡¹ï¼š
- `crontab(minute="*/1")` - æ¯1åˆ†é’Ÿ
- `crontab(minute="*/5")` - æ¯5åˆ†é’Ÿ
- `crontab(minute="*/15")` - æ¯15åˆ†é’Ÿ
- `crontab(hour="*/1")` - æ¯å°æ—¶
- `crontab(minute=0, hour="*/2")` - æ¯2å°æ—¶æ•´ç‚¹

### æ·»åŠ æ–°çš„æ–°é—»æº

1. åˆ›å»ºçˆ¬è™«å·¥å…·ï¼ˆå¦‚ `jrj_crawler.py`ï¼‰
2. åœ¨ `crawl_tasks.py` ä¸­æ·»åŠ æ”¯æŒ
3. åœ¨ `celery_app.py` ä¸­æ·»åŠ å®šæ—¶ä»»åŠ¡

---

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: é¦–æ¬¡å¯åŠ¨ï¼ˆå†·å¯åŠ¨ï¼‰

```bash
# 1. å¯åŠ¨æ‰€æœ‰æœåŠ¡
cd backend
celery -A app.core.celery_app worker &
celery -A app.core.celery_app beat &
uvicorn app.main:app &

# 2. è§¦å‘å†·å¯åŠ¨ï¼Œå›æº¯ 30 é¡µå†å²æ•°æ®
curl -X POST http://localhost:8000/api/v1/tasks/cold-start \
  -H "Content-Type: application/json" \
  -d '{"source":"sina","start_page":1,"end_page":30}'

# 3. ç­‰å¾…çº¦ 20-30 åˆ†é’Ÿï¼ŒæŸ¥çœ‹è¿›åº¦
watch -n 5 'curl -s http://localhost:8000/api/v1/tasks/1'

# 4. å®Œæˆåï¼Œå®æ—¶ç›‘æ§è‡ªåŠ¨æ¥ç®¡ï¼ˆæ¯5åˆ†é’Ÿï¼‰
```

### åœºæ™¯ 2: æ—¥å¸¸è¿è¡Œï¼ˆå®æ—¶ç›‘æ§ï¼‰

```bash
# ä¿æŒ Worker å’Œ Beat è¿è¡Œ
# ç³»ç»Ÿä¼šæ¯5åˆ†é’Ÿè‡ªåŠ¨çˆ¬å–æœ€æ–°æ–°é—»
# æ— éœ€æ‰‹åŠ¨å¹²é¢„

# æŸ¥çœ‹ä»Šå¤©é‡‡é›†äº†å¤šå°‘æ–°é—»
curl http://localhost:8000/api/v1/news/?limit=100

# æŸ¥çœ‹å®æ—¶ç›‘æ§ä»»åŠ¡ç»Ÿè®¡
curl http://localhost:8000/api/v1/tasks/?mode=realtime&limit=20
```

### åœºæ™¯ 3: è¡¥å……å†å²æ•°æ®

```bash
# å‘ç°æŸå¤©æ•°æ®ç¼ºå¤±ï¼Œè¡¥å……çˆ¬å–
curl -X POST http://localhost:8000/api/v1/tasks/cold-start \
  -H "Content-Type: application/json" \
  -d '{"source":"sina","start_page":10,"end_page":20}'
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: Worker å¯åŠ¨å¤±è´¥

**é”™è¯¯**: `Connection refused` æˆ– `Error 111`

**è§£å†³**:
```bash
# æ£€æŸ¥ Redis æ˜¯å¦è¿è¡Œ
docker ps | grep redis

# æ£€æŸ¥ Redis è¿æ¥
docker exec finnews_redis redis-cli ping
# åº”è¯¥è¿”å› PONG

# æ£€æŸ¥ .env ä¸­çš„ REDIS_HOST é…ç½®
grep REDIS_HOST .env
```

### Q2: å®šæ—¶ä»»åŠ¡ä¸æ‰§è¡Œ

**è§£å†³**:
```bash
# 1. ç¡®è®¤ Beat æ­£åœ¨è¿è¡Œ
ps aux | grep "celery.*beat"

# 2. æŸ¥çœ‹ Beat æ—¥å¿—
celery -A app.core.celery_app beat --loglevel=debug

# 3. æ£€æŸ¥ç³»ç»Ÿæ—¶é—´
date
# åº”è¯¥ä¸æœåŠ¡å™¨æ—¶é—´ä¸€è‡´
```

### Q3: ä»»åŠ¡å¡åœ¨ PENDING çŠ¶æ€

**è§£å†³**:
```bash
# 1. æ£€æŸ¥ Worker æ˜¯å¦è¿è¡Œ
ps aux | grep "celery.*worker"

# 2. é‡å¯ Worker
pkill -9 celery
celery -A app.core.celery_app worker --loglevel=info

# 3. æ¸…ç† Redis é˜Ÿåˆ—ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰
docker exec finnews_redis redis-cli FLUSHALL
```

### Q4: çˆ¬å–åˆ°çš„æ–°é—»æ•°ä¸º 0

**å¯èƒ½åŸå› **:
1. æ–°é—»æºç½‘ç«™ç»“æ„å˜åŒ–ï¼ˆéœ€è¦æ›´æ–°çˆ¬è™«ï¼‰
2. è¢«åçˆ¬å°ç¦ï¼ˆIP/User-Agentï¼‰
3. æ—¶é—´è¿‡æ»¤å¤ªä¸¥æ ¼ï¼ˆå®æ—¶ç›‘æ§åªä¿ç•™1å°æ—¶å†…çš„ï¼‰

**è°ƒè¯•**:
```bash
# æŸ¥çœ‹ä»»åŠ¡è¯¦æƒ…
curl http://localhost:8000/api/v1/tasks/{task_id}

# æŸ¥çœ‹ Worker æ—¥å¿—ä¸­çš„é”™è¯¯ä¿¡æ¯

# æ‰‹åŠ¨æµ‹è¯•çˆ¬è™«
cd backend
python -c "
from app.tools import SinaCrawlerTool
crawler = SinaCrawlerTool()
news = crawler.crawl(1, 1)
print(f'çˆ¬å–åˆ° {len(news)} æ¡æ–°é—»')
"
```

---

## ğŸ‰ éªŒæ”¶æ£€æŸ¥

âœ… **ç³»ç»Ÿæ­£å¸¸è¿è¡Œçš„æ ‡å¿—**:

1. **Worker è¿è¡Œä¸­**
```bash
ps aux | grep "celery.*worker"
# åº”è¯¥çœ‹åˆ°è¿›ç¨‹
```

2. **Beat è°ƒåº¦æ­£å¸¸**
```bash
curl http://localhost:8000/api/v1/tasks/ | jq '.[0].mode'
# åº”è¯¥å®šæœŸå‡ºç° "realtime" ä»»åŠ¡
```

3. **ä»»åŠ¡æˆåŠŸç‡ > 95%**
```bash
curl http://localhost:8000/api/v1/tasks/stats/summary | jq
# by_status.completed / total > 0.95
```

4. **æ–°é—»æŒç»­å¢é•¿**
```bash
curl http://localhost:8000/api/v1/news/?limit=1 | jq '.[0].created_at'
# åº”è¯¥æ˜¯æœ€è¿‘5-10åˆ†é’Ÿå†…çš„
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [CRAWL_STRATEGY.md](../CRAWL_STRATEGY.md) - å®Œæ•´çˆ¬å–ç­–ç•¥è®¾è®¡
- [planning.md](../../planning.md) - é¡¹ç›®æ•´ä½“è§„åˆ’
- [Celery å®˜æ–¹æ–‡æ¡£](https://docs.celeryq.dev/)

---

**ğŸŠ Phase 1.5 å®Œæˆï¼ç³»ç»Ÿç°åœ¨å¯ä»¥è‡ªåŠ¨ç§¯ç´¯æ•°æ®äº†ï¼**

