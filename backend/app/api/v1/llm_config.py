"""
LLM é…ç½® API è·¯ç”±
è¿”å›å¯ç”¨çš„ LLM å‚å•†å’Œæ¨¡å‹åˆ—è¡¨
"""
import logging
from typing import List, Dict, Optional
from fastapi import APIRouter
from pydantic import BaseModel, Field

from ...core.config import settings

logger = logging.getLogger(__name__)

router = APIRouter()


class ModelInfo(BaseModel):
    """æ¨¡å‹ä¿¡æ¯"""
    value: str = Field(..., description="æ¨¡å‹æ ‡è¯†")
    label: str = Field(..., description="æ¨¡å‹æ˜¾ç¤ºåç§°")
    description: str = Field(default="", description="æ¨¡å‹æè¿°")


class ProviderInfo(BaseModel):
    """å‚å•†ä¿¡æ¯"""
    value: str = Field(..., description="å‚å•†æ ‡è¯†")
    label: str = Field(..., description="å‚å•†æ˜¾ç¤ºåç§°")
    icon: str = Field(..., description="å‚å•†å›¾æ ‡")
    models: List[ModelInfo] = Field(..., description="å¯ç”¨æ¨¡å‹åˆ—è¡¨")
    has_api_key: bool = Field(..., description="æ˜¯å¦å·²é…ç½®API Key")


class LLMConfigResponse(BaseModel):
    """LLM é…ç½®å“åº”"""
    default_provider: str = Field(..., description="é»˜è®¤å‚å•†")
    default_model: str = Field(..., description="é»˜è®¤æ¨¡å‹")
    providers: List[ProviderInfo] = Field(..., description="å¯ç”¨å‚å•†åˆ—è¡¨")


def parse_models(models_str: str, provider_label: str) -> List[ModelInfo]:
    """
    è§£æé€—å·åˆ†éš”çš„æ¨¡å‹å­—ç¬¦ä¸²
    
    Args:
        models_str: é€—å·åˆ†éš”çš„æ¨¡å‹å­—ç¬¦ä¸²
        provider_label: å‚å•†æ˜¾ç¤ºåç§°
        
    Returns:
        æ¨¡å‹ä¿¡æ¯åˆ—è¡¨
    """
    if not models_str:
        return []
    
    models = []
    for model in models_str.split(','):
        model = model.strip()
        if model:
            models.append(ModelInfo(
                value=model,
                label=model,
                description=f"{provider_label} æ¨¡å‹"
            ))
    return models


@router.get("/config", response_model=LLMConfigResponse)
async def get_llm_config():
    """
    è·å– LLM é…ç½®ä¿¡æ¯
    
    è¿”å›æ‰€æœ‰å¯ç”¨çš„å‚å•†å’Œæ¨¡å‹åˆ—è¡¨ï¼Œä»¥åŠæ˜¯å¦å·²é…ç½® API Key
    """
    try:
        providers = []
        
        # 1. ç™¾ç‚¼
        if settings.BAILIAN_MODELS:
            providers.append(ProviderInfo(
                value="bailian",
                label="ç™¾ç‚¼ï¼ˆé˜¿é‡Œäº‘ï¼‰",
                icon="ğŸ“¦",
                models=parse_models(settings.BAILIAN_MODELS, "ç™¾ç‚¼"),
                has_api_key=bool(settings.DASHSCOPE_API_KEY or settings.BAILIAN_API_KEY)
            ))
        
        # 2. OpenAI
        if settings.OPENAI_MODELS:
            providers.append(ProviderInfo(
                value="openai",
                label="OpenAI",
                icon="ğŸ¤–",
                models=parse_models(settings.OPENAI_MODELS, "OpenAI"),
                has_api_key=bool(settings.OPENAI_API_KEY)
            ))
        
        # 3. DeepSeek
        if settings.DEEPSEEK_MODELS:
            providers.append(ProviderInfo(
                value="deepseek",
                label="DeepSeek",
                icon="ğŸ§ ",
                models=parse_models(settings.DEEPSEEK_MODELS, "DeepSeek"),
                has_api_key=bool(settings.DEEPSEEK_API_KEY)
            ))
        
        # 4. Kimi
        if settings.MOONSHOT_MODELS:
            providers.append(ProviderInfo(
                value="kimi",
                label="Kimi (Moonshot)",
                icon="ğŸŒ™",
                models=parse_models(settings.MOONSHOT_MODELS, "Kimi"),
                has_api_key=bool(settings.MOONSHOT_API_KEY)
            ))
        
        # 5. æ™ºè°±
        if settings.ZHIPU_MODELS:
            providers.append(ProviderInfo(
                value="zhipu",
                label="æ™ºè°±",
                icon="ğŸ”®",
                models=parse_models(settings.ZHIPU_MODELS, "æ™ºè°±"),
                has_api_key=bool(settings.ZHIPU_API_KEY)
            ))
        
        return LLMConfigResponse(
            default_provider=settings.LLM_PROVIDER,
            default_model=settings.LLM_MODEL,
            providers=providers
        )
    
    except Exception as e:
        logger.error(f"Failed to get LLM config: {e}", exc_info=True)
        # è¿”å›é»˜è®¤é…ç½®
        return LLMConfigResponse(
            default_provider="bailian",
            default_model="qwen-plus",
            providers=[]
        )

