"""
Celery çˆ¬å–ä»»åŠ¡ - Phase 2: å®æ—¶ç›‘æ§å‡çº§ç‰ˆ + å¤šæºæ”¯æŒ
"""
import logging
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
from sqlalchemy import select, create_engine, text
from sqlalchemy.orm import Session
import asyncio

from ..core.celery_app import celery_app
from ..core.config import settings
from ..core.redis_client import redis_client
from ..models.crawl_task import CrawlTask, CrawlMode, TaskStatus
from ..models.news import News
from ..tools import (
    SinaCrawlerTool,
    TencentCrawlerTool,
    JwviewCrawlerTool,
    EeoCrawlerTool,
    CaijingCrawlerTool,
    Jingji21CrawlerTool,
    NbdCrawlerTool,
    YicaiCrawlerTool,
    Netease163CrawlerTool,
    EastmoneyCrawlerTool,
    bochaai_search,
    NewsItem,
)
from ..tools.crawler_enhanced import EnhancedCrawler, crawl_url

logger = logging.getLogger(__name__)


def get_crawler_tool(source: str):
    """
    çˆ¬è™«å·¥å‚å‡½æ•°
    
    Args:
        source: æ–°é—»æºåç§°
        
    Returns:
        å¯¹åº”çš„çˆ¬è™«å®ä¾‹
    """
    crawlers = {
        "sina": SinaCrawlerTool,
        "tencent": TencentCrawlerTool,
        "jwview": JwviewCrawlerTool,
        "eeo": EeoCrawlerTool,
        "caijing": CaijingCrawlerTool,
        "jingji21": Jingji21CrawlerTool,
        "nbd": NbdCrawlerTool,
        "yicai": YicaiCrawlerTool,
        "163": Netease163CrawlerTool,
        "eastmoney": EastmoneyCrawlerTool,
    }
    
    crawler_class = crawlers.get(source)
    if not crawler_class:
        raise ValueError(f"Unknown news source: {source}")
    
    return crawler_class()


def get_sync_db_session():
    """è·å–åŒæ­¥æ•°æ®åº“ä¼šè¯ï¼ˆCeleryä»»åŠ¡ä¸­ä½¿ç”¨ï¼‰"""
    engine = create_engine(settings.SYNC_DATABASE_URL)
    return Session(engine)


@celery_app.task(bind=True, name="app.tasks.crawl_tasks.realtime_crawl_task")
def realtime_crawl_task(self, source: str = "sina", force_refresh: bool = False):
    """
    å®æ—¶çˆ¬å–ä»»åŠ¡ (Phase 2 å‡çº§ç‰ˆ)
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. Redis ç¼“å­˜æ£€æŸ¥ï¼ˆé¿å…é¢‘ç¹çˆ¬å–ï¼‰
    2. æ™ºèƒ½æ—¶é—´è¿‡æ»¤ï¼ˆåŸºäºé…ç½®çš„ NEWS_RETENTION_HOURSï¼‰
    3. åªçˆ¬å–æœ€æ–°ä¸€é¡µ
    
    Args:
        source: æ–°é—»æºï¼ˆsina, jrjç­‰ï¼‰
        force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆè·³è¿‡ç¼“å­˜ï¼‰
    """
    db = get_sync_db_session()
    task_record = None
    cache_key = f"news:{source}:latest"
    cache_time_key = f"{cache_key}:timestamp"
    
    try:
        # ===== Phase 2.1: æ£€æŸ¥ Redis ç¼“å­˜ =====
        if not force_refresh and redis_client.is_available():
            cache_metadata = redis_client.get_cache_metadata(cache_key)
            
            if cache_metadata:
                age_seconds = cache_metadata['age_seconds']
                # æ ¹æ®ä¸åŒæºè·å–å¯¹åº”çš„çˆ¬å–é—´éš”
                interval_map = {
                    "sina": settings.CRAWL_INTERVAL_SINA,
                    "tencent": settings.CRAWL_INTERVAL_TENCENT,
                    "jwview": settings.CRAWL_INTERVAL_JWVIEW,
                    "eeo": settings.CRAWL_INTERVAL_EEO,
                    "caijing": settings.CRAWL_INTERVAL_CAIJING,
                    "jingji21": settings.CRAWL_INTERVAL_JINGJI21,
                    "nbd": 60,  # æ¯æ—¥ç»æµæ–°é—»
                    "yicai": 60,  # ç¬¬ä¸€è´¢ç»
                    "163": 60,  # ç½‘æ˜“è´¢ç»
                    "eastmoney": 60,  # ä¸œæ–¹è´¢å¯Œ
                }
                interval = interval_map.get(source, 60)  # é»˜è®¤60ç§’
                
                # å¦‚æœç¼“å­˜æ—¶é—´ < çˆ¬å–é—´éš”ï¼Œä½¿ç”¨ç¼“å­˜
                if age_seconds < interval:
                    logger.info(
                        f"[{source}] ä½¿ç”¨ç¼“å­˜æ•°æ® (age: {age_seconds:.0f}s < {interval}s)"
                    )
                    return {
                        "status": "cached",
                        "source": source,
                        "cache_age": age_seconds,
                        "message": f"ç¼“å­˜æ•°æ®ä»ç„¶æœ‰æ•ˆï¼Œè·ä¸Šæ¬¡çˆ¬å– {age_seconds:.0f} ç§’"
                    }
        
        # ===== 1. åˆ›å»ºä»»åŠ¡è®°å½• =====
        task_record = CrawlTask(
            celery_task_id=self.request.id,
            mode=CrawlMode.REALTIME,
            status=TaskStatus.RUNNING,
            source=source,
            config={
                "page_limit": 1, 
                "retention_hours": settings.NEWS_RETENTION_HOURS,
                "force_refresh": force_refresh
            },
            started_at=datetime.utcnow(),
        )
        db.add(task_record)
        db.commit()
        db.refresh(task_record)
        
        logger.info(f"[Task {task_record.id}] ğŸš€ å¼€å§‹å®æ—¶çˆ¬å–: {source}")
        
        # ===== 2. åˆ›å»ºçˆ¬è™«ï¼ˆä½¿ç”¨å·¥å‚å‡½æ•°ï¼‰ =====
        try:
            crawler = get_crawler_tool(source)
        except ValueError as e:
            logger.error(f"[Task {task_record.id}] âŒ {e}")
            raise
        
        # ===== 3. æ‰§è¡Œçˆ¬å–ï¼ˆåªçˆ¬ç¬¬ä¸€é¡µï¼‰ =====
        start_time = datetime.utcnow()
        news_list = crawler.crawl(start_page=1, end_page=1)
        
        logger.info(f"[Task {task_record.id}] ğŸ“° çˆ¬å–åˆ° {len(news_list)} æ¡æ–°é—»")
        
        # ===== Phase 2.2: æ™ºèƒ½æ—¶é—´è¿‡æ»¤ =====
        cutoff_time = datetime.utcnow() - timedelta(hours=settings.NEWS_RETENTION_HOURS)
        recent_news = [
            news for news in news_list
            if news.publish_time and news.publish_time > cutoff_time
        ] if news_list else []
        
        logger.info(
            f"[Task {task_record.id}] â±ï¸  è¿‡æ»¤åå‰©ä½™ {len(recent_news)} æ¡æ–°é—» "
            f"(ä¿ç•™ {settings.NEWS_RETENTION_HOURS} å°æ—¶å†…)"
        )
        
        # ===== 4. å»é‡å¹¶ä¿å­˜ =====
        saved_count = 0
        duplicate_count = 0
        
        for news_item in recent_news:
            # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
            existing = db.execute(
                select(News).where(News.url == news_item.url)
            ).scalar_one_or_none()
            
            if existing:
                duplicate_count += 1
                logger.debug(f"[Task {task_record.id}] â­ï¸  è·³è¿‡é‡å¤æ–°é—»: {news_item.title[:30]}...")
                continue
            
            # åˆ›å»ºæ–°è®°å½•
            news = News(
                title=news_item.title,
                content=news_item.content,
                raw_html=news_item.raw_html,  # ä¿å­˜åŸå§‹ HTML
                url=news_item.url,
                source=news_item.source,
                publish_time=news_item.publish_time,
                author=news_item.author,
                keywords=news_item.keywords,
                stock_codes=news_item.stock_codes,
            )
            
            db.add(news)
            saved_count += 1
        
        db.commit()
        
        logger.info(
            f"[Task {task_record.id}] ğŸ’¾ ä¿å­˜ {saved_count} æ¡æ–°æ–°é—» "
            f"(é‡å¤: {duplicate_count})"
        )
        
        # ===== Phase 2.3: æ›´æ–° Redis ç¼“å­˜ =====
        if redis_client.is_available() and recent_news:
            # å°†æ–°é—»åˆ—è¡¨åºåˆ—åŒ–åå­˜å…¥ç¼“å­˜
            cache_data = [
                {
                    "title": n.title,
                    "url": n.url,
                    "publish_time": n.publish_time.isoformat() if n.publish_time else None,
                    "source": n.source,
                }
                for n in recent_news
            ]
            success = redis_client.set_with_metadata(
                cache_key, 
                cache_data, 
                ttl=settings.CACHE_TTL
            )
            if success:
                logger.info(f"[Task {task_record.id}] ğŸ’¾ Redis ç¼“å­˜å·²æ›´æ–° (TTL: {settings.CACHE_TTL}s)")
        
        # ===== 5. æ›´æ–°ä»»åŠ¡çŠ¶æ€ =====
        end_time = datetime.utcnow()
        execution_time = (end_time - start_time).total_seconds()
        
        task_record.status = TaskStatus.COMPLETED
        task_record.completed_at = end_time
        task_record.execution_time = execution_time
        task_record.crawled_count = len(recent_news)
        task_record.saved_count = saved_count
        task_record.result = {
            "total_crawled": len(news_list),
            "filtered": len(recent_news),
            "saved": saved_count,
            "duplicates": duplicate_count,
            "retention_hours": settings.NEWS_RETENTION_HOURS,
        }
        db.commit()
        
        logger.info(
            f"[Task {task_record.id}] âœ… å®Œæˆ! "
            f"çˆ¬å–: {len(news_list)} â†’ è¿‡æ»¤: {len(recent_news)} â†’ ä¿å­˜: {saved_count}, "
            f"è€—æ—¶: {execution_time:.2f}s"
        )
        
        return {
            "task_id": task_record.id,
            "status": "completed",
            "source": source,
            "crawled": len(news_list),
            "filtered": len(recent_news),
            "saved": saved_count,
            "duplicates": duplicate_count,
            "execution_time": execution_time,
            "timestamp": datetime.utcnow().isoformat(),
        }
        
    except Exception as e:
        logger.error(f"[Task {task_record.id if task_record else 'unknown'}] çˆ¬å–å¤±è´¥: {e}", exc_info=True)
        
        if task_record:
            task_record.status = TaskStatus.FAILED
            task_record.completed_at = datetime.utcnow()
            task_record.error_message = str(e)[:1000]
            db.commit()
        
        # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®© Celery è®°å½•
        raise
    
    finally:
        db.close()


@celery_app.task(bind=True, name="app.tasks.crawl_tasks.cold_start_crawl_task")
def cold_start_crawl_task(
    self,
    source: str = "sina",
    start_page: int = 1,
    end_page: int = 50,
):
    """
    å†·å¯åŠ¨æ‰¹é‡çˆ¬å–ä»»åŠ¡
    
    Args:
        source: æ–°é—»æº
        start_page: èµ·å§‹é¡µ
        end_page: ç»“æŸé¡µ
    """
    db = get_sync_db_session()
    task_record = None
    
    try:
        # 1. åˆ›å»ºä»»åŠ¡è®°å½•
        task_record = CrawlTask(
            celery_task_id=self.request.id,
            mode=CrawlMode.COLD_START,
            status=TaskStatus.RUNNING,
            source=source,
            config={
                "start_page": start_page,
                "end_page": end_page,
            },
            total_pages=end_page - start_page + 1,
            started_at=datetime.utcnow(),
        )
        db.add(task_record)
        db.commit()
        db.refresh(task_record)
        
        logger.info(f"[Task {task_record.id}] å¼€å§‹å†·å¯åŠ¨çˆ¬å–: {source}, é¡µç  {start_page}-{end_page}")
        
        # 2. åˆ›å»ºçˆ¬è™«
        if source == "sina":
            crawler = SinaCrawlerTool()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–°é—»æº: {source}")
        
        # 3. åˆ†é¡µçˆ¬å–
        start_time = datetime.utcnow()
        total_crawled = 0
        total_saved = 0
        
        for page in range(start_page, end_page + 1):
            try:
                # æ›´æ–°è¿›åº¦
                task_record.current_page = page
                task_record.progress = {
                    "current_page": page,
                    "total_pages": task_record.total_pages,
                    "percentage": round((page - start_page + 1) / task_record.total_pages * 100, 2),
                }
                db.commit()
                
                # çˆ¬å–å•é¡µ
                news_list = crawler.crawl(start_page=page, end_page=page)
                total_crawled += len(news_list)
                
                # ä¿å­˜æ–°é—»
                page_saved = 0
                for news_item in news_list:
                    existing = db.execute(
                        select(News).where(News.url == news_item.url)
                    ).scalar_one_or_none()
                    
                    if not existing:
                        news = News(
                            title=news_item.title,
                            content=news_item.content,
                            raw_html=news_item.raw_html,  # ä¿å­˜åŸå§‹ HTML
                            url=news_item.url,
                            source=news_item.source,
                            publish_time=news_item.publish_time,
                            author=news_item.author,
                            keywords=news_item.keywords,
                            stock_codes=news_item.stock_codes,
                        )
                        db.add(news)
                        page_saved += 1
                
                db.commit()
                total_saved += page_saved
                
                logger.info(
                    f"[Task {task_record.id}] é¡µ {page}/{end_page}: "
                    f"çˆ¬å– {len(news_list)} æ¡, ä¿å­˜ {page_saved} æ¡"
                )
                
            except Exception as e:
                logger.error(f"[Task {task_record.id}] é¡µ {page} çˆ¬å–å¤±è´¥: {e}")
                continue
        
        # 4. æ›´æ–°ä»»åŠ¡çŠ¶æ€
        end_time = datetime.utcnow()
        execution_time = (end_time - start_time).total_seconds()
        
        task_record.status = TaskStatus.COMPLETED
        task_record.completed_at = end_time
        task_record.execution_time = execution_time
        task_record.crawled_count = total_crawled
        task_record.saved_count = total_saved
        task_record.result = {
            "pages_crawled": end_page - start_page + 1,
            "total_crawled": total_crawled,
            "total_saved": total_saved,
            "duplicates": total_crawled - total_saved,
        }
        db.commit()
        
        logger.info(
            f"[Task {task_record.id}] å†·å¯åŠ¨å®Œæˆ! "
            f"é¡µæ•°: {end_page - start_page + 1}, çˆ¬å–: {total_crawled}, ä¿å­˜: {total_saved}, "
            f"è€—æ—¶: {execution_time:.2f}s"
        )
        
        return {
            "task_id": task_record.id,
            "status": "completed",
            "crawled": total_crawled,
            "saved": total_saved,
            "execution_time": execution_time,
        }
        
    except Exception as e:
        logger.error(f"[Task {task_record.id if task_record else 'unknown'}] å†·å¯åŠ¨å¤±è´¥: {e}", exc_info=True)
        
        if task_record:
            task_record.status = TaskStatus.FAILED
            task_record.completed_at = datetime.utcnow()
            task_record.error_message = str(e)[:1000]
            db.commit()
        
        raise
    
    finally:
        db.close()


@celery_app.task(bind=True, name="app.tasks.crawl_tasks.targeted_stock_crawl_task")
def targeted_stock_crawl_task(
    self,
    stock_code: str,
    stock_name: str,
    days: int = 30,
    task_record_id: int = None
):
    """
    å®šå‘çˆ¬å–æŸåªè‚¡ç¥¨çš„ç›¸å…³æ–°é—»ï¼ˆç²¾ç®€ç‰ˆ - åªä½¿ç”¨ BochaAIï¼‰
    
    æ•°æ®æ¥æºï¼šBochaAI æœç´¢å¼•æ“ API
    
    å›¾è°±æ„å»ºé€»è¾‘ï¼š
    - æœ‰å†å²æ–°é—»æ•°æ® â†’ å…ˆæ„å»º/ä½¿ç”¨å›¾è°± â†’ åŸºäºå›¾è°±æ‰©å±•å…³é”®è¯æœç´¢
    - æ— å†å²æ–°é—»æ•°æ® â†’ å…ˆç”¨ BochaAI çˆ¬å– â†’ çˆ¬å–å®Œæˆåå¼‚æ­¥æ„å»ºå›¾è°±
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ SH600519ï¼‰
        stock_name: è‚¡ç¥¨åç§°ï¼ˆå¦‚ è´µå·èŒ…å°ï¼‰
        days: æœç´¢æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼‰ï¼Œé»˜è®¤30å¤©
        task_record_id: æ•°æ®åº“ä¸­çš„ä»»åŠ¡è®°å½•IDï¼ˆå¦‚æœå·²åˆ›å»ºï¼‰
    """
    db = get_sync_db_session()
    task_record = None
    
    try:
        # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
        code = stock_code.upper()
        if code.startswith("SH") or code.startswith("SZ"):
            pure_code = code[2:]
        else:
            pure_code = code
            code = f"SH{code}" if code.startswith("6") else f"SZ{code}"
        
        # 1. è·å–æˆ–åˆ›å»ºä»»åŠ¡è®°å½•
        if task_record_id:
            task_record = db.query(CrawlTask).filter(CrawlTask.id == task_record_id).first()
            if task_record:
                task_record.status = TaskStatus.RUNNING
                task_record.started_at = datetime.utcnow()
                db.commit()
                db.refresh(task_record)
            else:
                logger.warning(f"Task record {task_record_id} not found, creating new one")
                task_record_id = None
        
        if not task_record:
            task_record = CrawlTask(
                celery_task_id=self.request.id,
                mode=CrawlMode.TARGETED,
                status=TaskStatus.RUNNING,
                source="targeted",
                config={
                    "stock_code": code,
                    "stock_name": stock_name,
                    "days": days,
                },
                started_at=datetime.utcnow(),
            )
            db.add(task_record)
            db.commit()
            db.refresh(task_record)
        
        logger.info(f"[Task {task_record.id}] ğŸ¯ å¼€å§‹å®šå‘çˆ¬å–: {stock_name}({code}), æ—¶é—´èŒƒå›´: {days}å¤©")
        
        start_time = datetime.utcnow()
        all_news = []
        search_results = []
        
        # ========================================
        # ã€æ ¸å¿ƒé€»è¾‘ã€‘å…ˆç”¨ akshare è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ï¼Œæ„å»ºç®€å•å›¾è°±
        # ========================================
        task_record.progress = {"current": 5, "total": 100, "message": "è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯..."}
        db.commit()
        
        from ..knowledge.knowledge_extractor import AkshareKnowledgeExtractor
        
        # 1. ä» akshare è·å–å…¬å¸åŸºç¡€ä¿¡æ¯
        logger.info(f"[Task {task_record.id}] ğŸ” ä» akshare è·å– {stock_name}({pure_code}) åŸºç¡€ä¿¡æ¯...")
        akshare_info = None
        try:
            akshare_info = AkshareKnowledgeExtractor.extract_company_info(pure_code)
            if akshare_info:
                logger.info(f"[Task {task_record.id}] âœ… akshare è¿”å›: è¡Œä¸š={akshare_info.get('industry')}, ä¸»è¥={akshare_info.get('main_business', '')[:50]}...")
            else:
                logger.warning(f"[Task {task_record.id}] âš ï¸ akshare æœªè¿”å›æ•°æ®ï¼Œå°†ä½¿ç”¨è‚¡ç¥¨åç§°ç”Ÿæˆå…³é”®è¯")
        except Exception as e:
            logger.warning(f"[Task {task_record.id}] âš ï¸ akshare æŸ¥è¯¢å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨è‚¡ç¥¨åç§°ç”Ÿæˆå…³é”®è¯")
        
        # 2. æ„å»ºç®€å•å›¾è°±å¹¶ç”Ÿæˆæœç´¢å…³é”®è¯
        task_record.progress = {"current": 10, "total": 100, "message": "æ„å»ºçŸ¥è¯†å›¾è°±..."}
        db.commit()
        
        simple_graph = AkshareKnowledgeExtractor.build_simple_graph_from_info(
            stock_code=code,
            stock_name=stock_name,
            akshare_info=akshare_info
        )
        
        # è·å–åˆ†å±‚å…³é”®è¯
        core_keywords = simple_graph.get("core_keywords", [stock_name])
        extension_keywords = simple_graph.get("extension_keywords", [])
        
        logger.info(
            f"[Task {task_record.id}] ğŸ“‹ å…³é”®è¯åˆ†å±‚: "
            f"æ ¸å¿ƒ={len(core_keywords)}ä¸ª{core_keywords[:4]}, "
            f"æ‰©å±•={len(extension_keywords)}ä¸ª{extension_keywords[:4]}"
        )
        logger.info(f"[Task {task_record.id}] ğŸ”‘ å®Œæ•´æ ¸å¿ƒå…³é”®è¯åˆ—è¡¨: {core_keywords}")
        logger.info(f"[Task {task_record.id}] ğŸ”‘ å®Œæ•´æ‰©å±•å…³é”®è¯åˆ—è¡¨: {extension_keywords}")
        
        # ========================================
        # ã€æœç´¢é˜¶æ®µã€‘ä½¿ç”¨ç»„åˆå…³é”®è¯è°ƒç”¨ BochaAI æœç´¢
        # ========================================
        task_record.progress = {"current": 20, "total": 100, "message": "BochaAI ç»„åˆæœç´¢ä¸­..."}
        db.commit()
        
        if not bochaai_search.is_available():
            logger.error(f"[Task {task_record.id}] âŒ BochaAI API Key æœªé…ç½®ï¼Œæ— æ³•æ‰§è¡Œæœç´¢")
            raise ValueError("BochaAI API Key æœªé…ç½®")
        
        # ========================================
        # ã€ç»„åˆæœç´¢ç­–ç•¥ã€‘
        # 1. å¿…é¡»æœç´¢ï¼šæ ¸å¿ƒå…³é”®è¯ï¼ˆå…¬å¸åã€ä»£ç ï¼‰
        # 2. å¯é€‰ç»„åˆï¼šæ ¸å¿ƒè¯ + æ‰©å±•è¯ï¼ˆè¡Œä¸šã€ä¸šåŠ¡ã€äººåï¼‰
        # ========================================
        all_search_results = []
        search_queries = []
        
        # ç­–ç•¥1ï¼šæ ¸å¿ƒå…³é”®è¯å•ç‹¬æœç´¢ï¼ˆå–å‰3ä¸ªæœ€é‡è¦çš„ï¼‰
        for core_kw in core_keywords[:3]:
            # è·³è¿‡çº¯æ•°å­—ä»£ç ï¼ˆå•ç‹¬æœä¼šå¾ˆæ³›ï¼‰
            if not (core_kw.isdigit() or core_kw.startswith("SH") or core_kw.startswith("SZ")):
                search_queries.append(core_kw)
        
        # ç­–ç•¥2ï¼šæ ¸å¿ƒè¯ + æ‰©å±•è¯ç»„åˆæœç´¢ï¼ˆæœ€å¤š3ä¸ªç»„åˆï¼‰
        if extension_keywords:
            # å–æœ€ä¸»è¦çš„æ ¸å¿ƒè¯ï¼ˆé€šå¸¸æ˜¯è‚¡ç¥¨ç®€ç§°ï¼‰
            main_core = core_keywords[0] if core_keywords else stock_name
            
            for ext_kw in extension_keywords[:3]:
                # ç»„åˆæœç´¢ï¼šå¦‚ "*STå›½å è½¯ä»¶å¼€å‘"
                combined_query = f"{main_core} {ext_kw}"
                search_queries.append(combined_query)
        
        # é™åˆ¶æ€»æŸ¥è¯¢æ•°ï¼ˆé¿å…è¿‡å¤šè¯·æ±‚ï¼‰
        search_queries = search_queries[:5]
        
        logger.info(f"[Task {task_record.id}] ğŸš€ ç”Ÿæˆ {len(search_queries)} ä¸ªæœç´¢æŸ¥è¯¢:")
        for i, q in enumerate(search_queries):
            logger.info(f"  [{i+1}] {q}")
        
        # æ‰§è¡Œæœç´¢
        for query in search_queries:
            try:
                logger.info(f"[Task {task_record.id}] ğŸ” æœç´¢: '{query}'")
                kw_results = bochaai_search.search_stock_news(
                    stock_name=query,  # ä½¿ç”¨ç»„åˆæŸ¥è¯¢
                    stock_code=pure_code,
                    days=days,
                    count=50,  # æ¯ä¸ªæŸ¥è¯¢æœ€å¤š 50 æ¡
                    max_age_days=365
                )
                logger.info(f"[Task {task_record.id}] ğŸ“° æŸ¥è¯¢ '{query}' æœç´¢åˆ° {len(kw_results)} æ¡ç»“æœ")
                all_search_results.extend(kw_results)
            except Exception as e:
                logger.warning(f"[Task {task_record.id}] âš ï¸ æŸ¥è¯¢ '{query}' æœç´¢å¤±è´¥: {e}")
        
        # å»é‡ï¼ˆæŒ‰ URLï¼‰
        seen_urls = set()
        search_results = []
        for r in all_search_results:
            if r.url not in seen_urls:
                seen_urls.add(r.url)
                search_results.append(r)
        
        logger.info(f"[Task {task_record.id}] ğŸ“Š åˆå¹¶ {len(all_search_results)} æ¡ï¼Œå»é‡å {len(search_results)} æ¡")
        
        # ========================================
        # ã€å¤„ç†é˜¶æ®µã€‘è½¬æ¢æœç´¢ç»“æœä¸º NewsItem
        # ========================================
        task_record.progress = {"current": 50, "total": 100, "message": "å¤„ç†æœç´¢ç»“æœ..."}
        db.commit()
        
        bochaai_matched = 0
        bochaai_filtered = 0
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¯ç”¨å®½æ¾è¿‡æ»¤æ¨¡å¼
        # å¦‚æœæ ¸å¿ƒå…³é”®è¯å¤ªå°‘ï¼ˆ<= 2ä¸ªï¼‰ï¼Œæˆ–è€…æœç´¢ç»“æœå¾ˆå°‘ï¼ˆ<10æ¡ï¼‰ï¼Œä½¿ç”¨å®½æ¾è¿‡æ»¤
        use_relaxed_filter = len(core_keywords) <= 2 or len(search_results) < 10
        if use_relaxed_filter:
            logger.info(f"[Task {task_record.id}] ğŸ”“ å¯ç”¨å®½æ¾è¿‡æ»¤æ¨¡å¼ï¼ˆæ ¸å¿ƒè¯={len(core_keywords)}ä¸ª, ç»“æœ={len(search_results)}æ¡ï¼‰")
        
        # æ‰“å° BochaAI è¿”å›çš„å‰ 10 æ¡æ•°æ®ç”¨äºè°ƒè¯•
        logger.info(f"[Task {task_record.id}] ğŸ“‹ BochaAI è¿”å›æ•°æ®é¢„è§ˆ (å‰10æ¡):")
        for i, r in enumerate(search_results[:10]):
            logger.info(f"  [{i+1}] æ ‡é¢˜: {r.title[:60]}...")
            logger.info(f"      æ¥æº: {r.site_name}, æ—¥æœŸ: {r.date_published}")
            logger.info(f"      URL: {r.url[:80]}...")
        
        for idx, result in enumerate(search_results):
            # è§£æå‘å¸ƒæ—¶é—´
            publish_time = None
            if result.date_published:
                try:
                    publish_time = datetime.fromisoformat(
                        result.date_published.replace('Z', '+00:00')
                    )
                except (ValueError, AttributeError):
                    pass
            
            # ã€æ³¨æ„ã€‘ä¸å†äºŒæ¬¡çˆ¬å–å®Œæ•´å†…å®¹ï¼Œç›´æ¥ä½¿ç”¨æ‘˜è¦ï¼ˆæå‡é€Ÿåº¦ï¼‰
            full_content = result.snippet
            
            # ç›¸å…³æ€§è¿‡æ»¤ï¼šå¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªæ ¸å¿ƒå…³é”®è¯
            text_to_check = result.title + " " + result.snippet
            text_to_check_lower = text_to_check.lower()
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•æ ¸å¿ƒå…³é”®è¯
            is_match = False
            matched_keyword = None
            for kw in core_keywords:
                if not kw or len(kw) < 2:
                    continue
                
                kw_lower = kw.lower()
                
                # å®½æ¾åŒ¹é…ç­–ç•¥ï¼š
                # 1. å®Œæ•´åŒ¹é…ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
                if kw in text_to_check or kw_lower in text_to_check_lower:
                    is_match = True
                    matched_keyword = kw
                    break
                
                # 2. å»é™¤ç‰¹æ®Šå­—ç¬¦ååŒ¹é…ï¼ˆå¤„ç† *ST ç­‰æƒ…å†µï¼‰
                import re
                kw_clean = re.sub(r'[*\s]', '', kw)
                if len(kw_clean) >= 2 and kw_clean.lower() in text_to_check_lower:
                    is_match = True
                    matched_keyword = f"{kw} (cleaned: {kw_clean})"
                    break
            
            if not is_match:
                # å®½æ¾æ¨¡å¼ä¸‹ï¼Œå¦‚æœæ ‡é¢˜åŒ…å«è‚¡ç¥¨ä»£ç æ•°å­—ï¼Œä¹Ÿè®¤ä¸ºç›¸å…³
                if use_relaxed_filter and pure_code in text_to_check:
                    is_match = True
                    matched_keyword = f"{pure_code} (relaxed mode)"
                    logger.debug(f"[Task {task_record.id}] ğŸ”“ å®½æ¾æ¨¡å¼åŒ¹é…: {result.title[:40]}... (åŒ…å«ä»£ç )")
                else:
                    bochaai_filtered += 1
                    # æ‰“å°å‰ 5 æ¡è¢«è¿‡æ»¤çš„åŸå› 
                    if bochaai_filtered <= 5:
                        logger.info(f"[Task {task_record.id}] âŒ è¿‡æ»¤[{idx+1}]: ä¸åŒ…å«æ ¸å¿ƒå…³é”®è¯")
                        logger.info(f"      æ ‡é¢˜: {result.title[:80]}")
                        logger.info(f"      æ‘˜è¦: {result.snippet[:100]}...")
                        logger.info(f"      æ ¸å¿ƒè¯: {core_keywords}")
                    continue
            
            # å¦‚æœå®½æ¾æ¨¡å¼è·³è¿‡äº†ä¸Šé¢çš„ continueï¼Œéœ€è¦ç¡®ä¿ is_match ä¸º True
            if not is_match:
                continue
            
            logger.debug(f"[Task {task_record.id}] âœ… åŒ¹é…æ ¸å¿ƒè¯ '{matched_keyword}': {result.title[:40]}...")
            
            bochaai_matched += 1
            news_item = NewsItem(
                title=result.title,
                content=full_content,
                url=result.url,
                source=result.site_name or "web_search",
                publish_time=publish_time,
                stock_codes=[pure_code, code],
                raw_html=None,
            )
            all_news.append(news_item)
            
            # æ¯å¤„ç† 20 æ¡æ›´æ–°ä¸€æ¬¡è¿›åº¦
            if (idx + 1) % 20 == 0:
                progress_pct = 50 + int((idx + 1) / len(search_results) * 30)
                task_record.progress = {"current": progress_pct, "total": 100, "message": f"å¤„ç†ä¸­ {idx+1}/{len(search_results)}..."}
                db.commit()
        
        logger.info(f"[Task {task_record.id}] ğŸ” æœç´¢åˆ° {len(search_results)} æ¡ï¼ŒåŒ¹é… {bochaai_matched} æ¡ï¼Œè¿‡æ»¤ {bochaai_filtered} æ¡")
        
        # ========================================
        # ã€äº¤äº’å¼çˆ¬è™«è¡¥å……ã€‘å¦‚æœç›¸å…³æ€§åŒ¹é…ç»“æœå¤ªå°‘ï¼Œä½¿ç”¨äº¤äº’å¼çˆ¬è™«è¡¥å……
        # ========================================
        if bochaai_matched < 5:  # åŒ¹é…ç»“æœå¤ªå°‘æ—¶å¯åŠ¨äº¤äº’å¼çˆ¬è™«
            logger.info(f"[Task {task_record.id}] ğŸŒ ç›¸å…³ç»“æœè¾ƒå°‘({bochaai_matched}æ¡)ï¼Œå¯ç”¨äº¤äº’å¼çˆ¬è™«è¡¥å……...")
            
            try:
                from ..tools.interactive_crawler import create_interactive_crawler
                
                # ä½¿ç”¨æ ¸å¿ƒå…³é”®è¯è¿›è¡Œæœç´¢
                # å–æœ€ä¸»è¦çš„æ ¸å¿ƒè¯ï¼ˆé€šå¸¸æ˜¯è‚¡ç¥¨ç®€ç§°ï¼‰
                interactive_query = core_keywords[0] if core_keywords else stock_name
                
                logger.info(f"[Task {task_record.id}] ğŸ” ä½¿ç”¨äº¤äº’å¼çˆ¬è™«æœç´¢: '{interactive_query}'")
                
                crawler = create_interactive_crawler(headless=True)
                interactive_results = crawler.interactive_search(
                    interactive_query,
                    engines=["bing"],  # ä¼˜å…ˆ Bingï¼ˆæ›´ç¨³å®šï¼‰
                    num_results=15,
                    headless=True
                )
                
                logger.info(f"[Task {task_record.id}] âœ… äº¤äº’å¼çˆ¬è™«è¿”å› {len(interactive_results)} æ¡ç»“æœ")
                
                # çˆ¬å–é¡µé¢å†…å®¹
                interactive_crawled = crawler.crawl_search_results(
                    interactive_results,
                    max_results=5
                )
                
                logger.info(f"[Task {task_record.id}] ğŸ“„ äº¤äº’å¼çˆ¬è™«çˆ¬å–æˆåŠŸ: {len(interactive_crawled)} ä¸ªé¡µé¢")
                
                # æ·»åŠ åˆ°æ–°é—»åˆ—è¡¨
                for page in interactive_crawled:
                    if page['url'] not in {item.url for item in all_news}:
                        news_item = NewsItem(
                            title=page['title'],
                            content=page['content'][:500],  # åªå–å‰ 500 å­—ä½œä¸ºæ‘˜è¦
                            url=page['url'],
                            source=page.get('source', 'web_search'),
                            publish_time=None,  # äº¤äº’çˆ¬è™«æ²¡æœ‰å‘å¸ƒæ—¶é—´
                            stock_codes=[pure_code, code],
                            raw_html=None,
                        )
                        all_news.append(news_item)
                        bochaai_matched += 1
                
                logger.info(f"[Task {task_record.id}] ğŸ“Š äº¤äº’å¼çˆ¬è™«è¡¥å……åæ€»è®¡: {bochaai_matched} æ¡åŒ¹é…ç»“æœ")
                
            except ImportError:
                logger.warning(f"[Task {task_record.id}] âš ï¸ äº¤äº’å¼çˆ¬è™«æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡è¡¥å……æœç´¢")
            except Exception as e:
                logger.error(f"[Task {task_record.id}] âŒ äº¤äº’å¼çˆ¬è™«è¡¥å……å¤±è´¥: {e}", exc_info=True)
        
        # ========================================
        # ã€ä¿å­˜é˜¶æ®µã€‘å»é‡å¹¶ä¿å­˜æ–°é—»
        # ========================================
        task_record.progress = {"current": 80, "total": 100, "message": "ä¿å­˜æ–°é—»..."}
        db.commit()
        saved_count = 0
        duplicate_count = 0
        
        logger.info(f"[Task {task_record.id}] ğŸ’¾ å¼€å§‹ä¿å­˜ {len(all_news)} æ¡æ–°é—»...")
        
        for news_item in all_news:
            # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
            existing = db.execute(
                select(News).where(News.url == news_item.url)
            ).scalar_one_or_none()
            
            if existing:
                duplicate_count += 1
                # å¦‚æœå·²å­˜åœ¨ä½†æ²¡æœ‰å…³è”è¿™ä¸ªè‚¡ç¥¨ï¼Œæ›´æ–°å…³è”
                if existing.stock_codes is None:
                    existing.stock_codes = []
                if pure_code not in existing.stock_codes:
                    existing.stock_codes = existing.stock_codes + [pure_code]
                    db.commit()
                continue
            
            # åˆ›å»ºæ–°è®°å½•
            news = News(
                title=news_item.title,
                content=news_item.content,
                raw_html=news_item.raw_html,  # ä¿å­˜åŸå§‹ HTML
                url=news_item.url,
                source=news_item.source,
                publish_time=news_item.publish_time,
                author=news_item.author,
                keywords=news_item.keywords,
                stock_codes=news_item.stock_codes or [pure_code, code],
            )
            
            db.add(news)
            saved_count += 1
        
        db.commit()
        
        logger.info(
            f"[Task {task_record.id}] ğŸ’¾ ä¿å­˜ {saved_count} æ¡æ–°é—» "
            f"(é‡å¤: {duplicate_count})"
        )
        
        # ========================================
        # ã€å›¾è°±æ›´æ–°é˜¶æ®µã€‘å¼‚æ­¥æ„å»ºå®Œæ•´å›¾è°±ï¼ˆåŸºäº Neo4jï¼‰
        # ========================================
        task_record.progress = {"current": 90, "total": 100, "message": "è§¦å‘å¼‚æ­¥å›¾è°±æ„å»º..."}
        db.commit()
        
        if saved_count > 0:
            # æœ‰æ–°é—»ä¿å­˜æˆåŠŸåï¼Œè§¦å‘å¼‚æ­¥å›¾è°±æ„å»ºä»»åŠ¡
            logger.info(f"[Task {task_record.id}] ğŸ§  è§¦å‘å¼‚æ­¥å›¾è°±æ„å»ºä»»åŠ¡...")
            try:
                build_knowledge_graph_task.delay(code, stock_name)
                logger.info(f"[Task {task_record.id}] âœ… å¼‚æ­¥å›¾è°±æ„å»ºä»»åŠ¡å·²è§¦å‘")
            except Exception as e:
                logger.error(f"[Task {task_record.id}] âŒ è§¦å‘å¼‚æ­¥å›¾è°±æ„å»ºå¤±è´¥: {e}")
        
        # ========================================
        # ã€å®Œæˆé˜¶æ®µã€‘æ›´æ–°ä»»åŠ¡çŠ¶æ€
        # ========================================
        end_time = datetime.utcnow()
        execution_time = (end_time - start_time).total_seconds()
        
        task_record.status = TaskStatus.COMPLETED
        task_record.completed_at = end_time
        task_record.execution_time = execution_time
        task_record.crawled_count = len(all_news)
        task_record.saved_count = saved_count
        task_record.result = {
            "stock_code": code,
            "stock_name": stock_name,
            "total_found": len(all_news),
            "saved": saved_count,
            "duplicates": duplicate_count,
            "akshare_info": bool(akshare_info),  # æ˜¯å¦è·å–åˆ° akshare æ•°æ®
            "core_keywords": core_keywords[:5],  # æ ¸å¿ƒå…³é”®è¯
            "search_queries": search_queries,  # å®é™…æœç´¢çš„æŸ¥è¯¢
            "sources": {
                "bochaai": len(search_results),
            }
        }
        task_record.progress = {
            "current": 100,
            "total": 100,
            "message": f"å®Œæˆï¼æ–°å¢ {saved_count} æ¡æ–°é—»"
        }
        db.commit()
        
        logger.info(
            f"[Task {task_record.id}] âœ… å®šå‘çˆ¬å–å®Œæˆ! "
            f"è‚¡ç¥¨: {stock_name}({code}), æ‰¾åˆ°: {len(all_news)}, ä¿å­˜: {saved_count}, "
            f"è€—æ—¶: {execution_time:.2f}s"
        )
        
        return {
            "task_id": task_record.id,
            "status": "completed",
            "stock_code": code,
            "stock_name": stock_name,
            "crawled": len(all_news),
            "saved": saved_count,
            "duplicates": duplicate_count,
            "execution_time": execution_time,
            "timestamp": datetime.utcnow().isoformat(),
        }
        
    except Exception as e:
        logger.error(f"[Task {task_record.id if task_record else 'unknown'}] å®šå‘çˆ¬å–å¤±è´¥: {e}", exc_info=True)
        
        if task_record:
            task_record.status = TaskStatus.FAILED
            task_record.completed_at = datetime.utcnow()
            task_record.error_message = str(e)[:1000]
            task_record.progress = {
                "current": 0,
                "total": 100,
                "message": f"å¤±è´¥: {str(e)[:100]}"
            }
            db.commit()
        
        raise
    
    finally:
        db.close()


@celery_app.task(bind=True, name="app.tasks.crawl_tasks.build_knowledge_graph_task")
def build_knowledge_graph_task(self, stock_code: str, stock_name: str):
    """
    å¼‚æ­¥æ„å»ºçŸ¥è¯†å›¾è°±ä»»åŠ¡
    
    åœ¨æ— å†å²æ–°é—»æ•°æ®çš„è‚¡ç¥¨é¦–æ¬¡çˆ¬å–å®Œæˆåè§¦å‘ã€‚
    ä»æ•°æ®åº“ä¸­çš„æ–°é—»æ•°æ® + akshare åŸºç¡€ä¿¡æ¯æ„å»ºçŸ¥è¯†å›¾è°±ã€‚
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ SH600519ï¼‰
        stock_name: è‚¡ç¥¨åç§°ï¼ˆå¦‚ è´µå·èŒ…å°ï¼‰
    """
    db = get_sync_db_session()
    
    try:
        code = stock_code.upper()
        if code.startswith("SH") or code.startswith("SZ"):
            pure_code = code[2:]
        else:
            pure_code = code
            code = f"SH{code}" if code.startswith("6") else f"SZ{code}"
        
        logger.info(f"[GraphTask] ğŸ—ï¸ å¼€å§‹å¼‚æ­¥æ„å»ºçŸ¥è¯†å›¾è°±: {stock_name}({code})")
        
        from ..knowledge.graph_service import get_graph_service
        from ..knowledge.knowledge_extractor import (
            create_knowledge_extractor,
            AkshareKnowledgeExtractor
        )
        
        graph_service = get_graph_service()
        
        # 1. æ£€æŸ¥å›¾è°±æ˜¯å¦å·²å­˜åœ¨ï¼ˆé¿å…é‡å¤æ„å»ºï¼‰
        existing_graph = graph_service.get_company_graph(code)
        if existing_graph:
            logger.info(f"[GraphTask] âœ… å›¾è°±å·²å­˜åœ¨ï¼Œè·³è¿‡æ„å»º")
            return {"status": "skipped", "reason": "graph_exists"}
        
        # 2. ä» akshare è·å–åŸºç¡€å…¬å¸ä¿¡æ¯
        akshare_info = AkshareKnowledgeExtractor.extract_company_info(code)
        
        if akshare_info:
            extractor = create_knowledge_extractor()
            base_graph = asyncio.run(
                extractor.extract_from_akshare(code, stock_name, akshare_info)
            )
            graph_service.build_company_graph(base_graph)
            logger.info(f"[GraphTask] âœ… åŸºç¡€å›¾è°±æ„å»ºå®Œæˆ")
        else:
            logger.warning(f"[GraphTask] âš ï¸ akshare æœªè¿”å›æ•°æ®")
        
        # 3. ä»æ•°æ®åº“æ–°é—»ä¸­æå–ä¿¡æ¯æ›´æ–°å›¾è°±
        recent_news = db.execute(
            text("""
                SELECT title, content FROM news 
                WHERE stock_codes @> ARRAY[:code]::varchar[] 
                ORDER BY publish_time DESC LIMIT 50
            """).bindparams(code=pure_code)
        ).fetchall()
        
        if recent_news:
            news_data = [{"title": n[0], "content": n[1]} for n in recent_news]
            extractor = create_knowledge_extractor()
            
            extracted_info = asyncio.run(
                extractor.extract_from_news(code, stock_name, news_data)
            )
            
            if any(extracted_info.values()):
                graph_service.update_from_news(code, "", extracted_info)
                logger.info(f"[GraphTask] âœ… ä»æ–°é—»æ›´æ–°å›¾è°±å®Œæˆ")
        
        logger.info(f"[GraphTask] âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ: {stock_name}({code})")
        
        return {
            "status": "completed",
            "stock_code": code,
            "stock_name": stock_name,
            "news_count": len(recent_news) if recent_news else 0,
        }
        
    except Exception as e:
        logger.error(f"[GraphTask] âŒ çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥: {e}", exc_info=True)
        return {"status": "failed", "error": str(e)}
    
    finally:
        db.close()

