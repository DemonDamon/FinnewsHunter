"""
Celery çˆ¬å–ä»»åŠ¡ - Phase 2: å®æ—¶ç›‘æ§å‡çº§ç‰ˆ + å¤šæºæ”¯æŒ
"""
import logging
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
from sqlalchemy import select, create_engine
from sqlalchemy.orm import Session

from ..core.celery_app import celery_app
from ..core.config import settings
from ..core.redis_client import redis_client
from ..models.crawl_task import CrawlTask, CrawlMode, TaskStatus
from ..models.news import News
from ..tools import (
    SinaCrawlerTool,
    TencentCrawlerTool,
    JwviewCrawlerTool,
    EeoCrawlerTool,
    CaijingCrawlerTool,
    Jingji21CrawlerTool,
    NbdCrawlerTool,
    YicaiCrawlerTool,
    Netease163CrawlerTool,
    EastmoneyCrawlerTool,
    bochaai_search,
    NewsItem,
)
from ..tools.crawler_enhanced import EnhancedCrawler, crawl_url

logger = logging.getLogger(__name__)


def get_crawler_tool(source: str):
    """
    çˆ¬è™«å·¥å‚å‡½æ•°
    
    Args:
        source: æ–°é—»æºåç§°
        
    Returns:
        å¯¹åº”çš„çˆ¬è™«å®ä¾‹
    """
    crawlers = {
        "sina": SinaCrawlerTool,
        "tencent": TencentCrawlerTool,
        "jwview": JwviewCrawlerTool,
        "eeo": EeoCrawlerTool,
        "caijing": CaijingCrawlerTool,
        "jingji21": Jingji21CrawlerTool,
        "nbd": NbdCrawlerTool,
        "yicai": YicaiCrawlerTool,
        "163": Netease163CrawlerTool,
        "eastmoney": EastmoneyCrawlerTool,
    }
    
    crawler_class = crawlers.get(source)
    if not crawler_class:
        raise ValueError(f"Unknown news source: {source}")
    
    return crawler_class()


def get_sync_db_session():
    """è·å–åŒæ­¥æ•°æ®åº“ä¼šè¯ï¼ˆCeleryä»»åŠ¡ä¸­ä½¿ç”¨ï¼‰"""
    engine = create_engine(settings.SYNC_DATABASE_URL)
    return Session(engine)


@celery_app.task(bind=True, name="app.tasks.crawl_tasks.realtime_crawl_task")
def realtime_crawl_task(self, source: str = "sina", force_refresh: bool = False):
    """
    å®æ—¶çˆ¬å–ä»»åŠ¡ (Phase 2 å‡çº§ç‰ˆ)
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. Redis ç¼“å­˜æ£€æŸ¥ï¼ˆé¿å…é¢‘ç¹çˆ¬å–ï¼‰
    2. æ™ºèƒ½æ—¶é—´è¿‡æ»¤ï¼ˆåŸºäºé…ç½®çš„ NEWS_RETENTION_HOURSï¼‰
    3. åªçˆ¬å–æœ€æ–°ä¸€é¡µ
    
    Args:
        source: æ–°é—»æºï¼ˆsina, jrjç­‰ï¼‰
        force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆè·³è¿‡ç¼“å­˜ï¼‰
    """
    db = get_sync_db_session()
    task_record = None
    cache_key = f"news:{source}:latest"
    cache_time_key = f"{cache_key}:timestamp"
    
    try:
        # ===== Phase 2.1: æ£€æŸ¥ Redis ç¼“å­˜ =====
        if not force_refresh and redis_client.is_available():
            cache_metadata = redis_client.get_cache_metadata(cache_key)
            
            if cache_metadata:
                age_seconds = cache_metadata['age_seconds']
                # æ ¹æ®ä¸åŒæºè·å–å¯¹åº”çš„çˆ¬å–é—´éš”
                interval_map = {
                    "sina": settings.CRAWL_INTERVAL_SINA,
                    "tencent": settings.CRAWL_INTERVAL_TENCENT,
                    "jwview": settings.CRAWL_INTERVAL_JWVIEW,
                    "eeo": settings.CRAWL_INTERVAL_EEO,
                    "caijing": settings.CRAWL_INTERVAL_CAIJING,
                    "jingji21": settings.CRAWL_INTERVAL_JINGJI21,
                    "nbd": 60,  # æ¯æ—¥ç»æµæ–°é—»
                    "yicai": 60,  # ç¬¬ä¸€è´¢ç»
                    "163": 60,  # ç½‘æ˜“è´¢ç»
                    "eastmoney": 60,  # ä¸œæ–¹è´¢å¯Œ
                }
                interval = interval_map.get(source, 60)  # é»˜è®¤60ç§’
                
                # å¦‚æœç¼“å­˜æ—¶é—´ < çˆ¬å–é—´éš”ï¼Œä½¿ç”¨ç¼“å­˜
                if age_seconds < interval:
                    logger.info(
                        f"[{source}] ä½¿ç”¨ç¼“å­˜æ•°æ® (age: {age_seconds:.0f}s < {interval}s)"
                    )
                    return {
                        "status": "cached",
                        "source": source,
                        "cache_age": age_seconds,
                        "message": f"ç¼“å­˜æ•°æ®ä»ç„¶æœ‰æ•ˆï¼Œè·ä¸Šæ¬¡çˆ¬å– {age_seconds:.0f} ç§’"
                    }
        
        # ===== 1. åˆ›å»ºä»»åŠ¡è®°å½• =====
        task_record = CrawlTask(
            celery_task_id=self.request.id,
            mode=CrawlMode.REALTIME,
            status=TaskStatus.RUNNING,
            source=source,
            config={
                "page_limit": 1, 
                "retention_hours": settings.NEWS_RETENTION_HOURS,
                "force_refresh": force_refresh
            },
            started_at=datetime.utcnow(),
        )
        db.add(task_record)
        db.commit()
        db.refresh(task_record)
        
        logger.info(f"[Task {task_record.id}] ğŸš€ å¼€å§‹å®æ—¶çˆ¬å–: {source}")
        
        # ===== 2. åˆ›å»ºçˆ¬è™«ï¼ˆä½¿ç”¨å·¥å‚å‡½æ•°ï¼‰ =====
        try:
            crawler = get_crawler_tool(source)
        except ValueError as e:
            logger.error(f"[Task {task_record.id}] âŒ {e}")
            raise
        
        # ===== 3. æ‰§è¡Œçˆ¬å–ï¼ˆåªçˆ¬ç¬¬ä¸€é¡µï¼‰ =====
        start_time = datetime.utcnow()
        news_list = crawler.crawl(start_page=1, end_page=1)
        
        logger.info(f"[Task {task_record.id}] ğŸ“° çˆ¬å–åˆ° {len(news_list)} æ¡æ–°é—»")
        
        # ===== Phase 2.2: æ™ºèƒ½æ—¶é—´è¿‡æ»¤ =====
        cutoff_time = datetime.utcnow() - timedelta(hours=settings.NEWS_RETENTION_HOURS)
        recent_news = [
            news for news in news_list
            if news.publish_time and news.publish_time > cutoff_time
        ] if news_list else []
        
        logger.info(
            f"[Task {task_record.id}] â±ï¸  è¿‡æ»¤åå‰©ä½™ {len(recent_news)} æ¡æ–°é—» "
            f"(ä¿ç•™ {settings.NEWS_RETENTION_HOURS} å°æ—¶å†…)"
        )
        
        # ===== 4. å»é‡å¹¶ä¿å­˜ =====
        saved_count = 0
        duplicate_count = 0
        
        for news_item in recent_news:
            # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
            existing = db.execute(
                select(News).where(News.url == news_item.url)
            ).scalar_one_or_none()
            
            if existing:
                duplicate_count += 1
                logger.debug(f"[Task {task_record.id}] â­ï¸  è·³è¿‡é‡å¤æ–°é—»: {news_item.title[:30]}...")
                continue
            
            # åˆ›å»ºæ–°è®°å½•
            news = News(
                title=news_item.title,
                content=news_item.content,
                raw_html=news_item.raw_html,  # ä¿å­˜åŸå§‹ HTML
                url=news_item.url,
                source=news_item.source,
                publish_time=news_item.publish_time,
                author=news_item.author,
                keywords=news_item.keywords,
                stock_codes=news_item.stock_codes,
            )
            
            db.add(news)
            saved_count += 1
        
        db.commit()
        
        logger.info(
            f"[Task {task_record.id}] ğŸ’¾ ä¿å­˜ {saved_count} æ¡æ–°æ–°é—» "
            f"(é‡å¤: {duplicate_count})"
        )
        
        # ===== Phase 2.3: æ›´æ–° Redis ç¼“å­˜ =====
        if redis_client.is_available() and recent_news:
            # å°†æ–°é—»åˆ—è¡¨åºåˆ—åŒ–åå­˜å…¥ç¼“å­˜
            cache_data = [
                {
                    "title": n.title,
                    "url": n.url,
                    "publish_time": n.publish_time.isoformat() if n.publish_time else None,
                    "source": n.source,
                }
                for n in recent_news
            ]
            success = redis_client.set_with_metadata(
                cache_key, 
                cache_data, 
                ttl=settings.CACHE_TTL
            )
            if success:
                logger.info(f"[Task {task_record.id}] ğŸ’¾ Redis ç¼“å­˜å·²æ›´æ–° (TTL: {settings.CACHE_TTL}s)")
        
        # ===== 5. æ›´æ–°ä»»åŠ¡çŠ¶æ€ =====
        end_time = datetime.utcnow()
        execution_time = (end_time - start_time).total_seconds()
        
        task_record.status = TaskStatus.COMPLETED
        task_record.completed_at = end_time
        task_record.execution_time = execution_time
        task_record.crawled_count = len(recent_news)
        task_record.saved_count = saved_count
        task_record.result = {
            "total_crawled": len(news_list),
            "filtered": len(recent_news),
            "saved": saved_count,
            "duplicates": duplicate_count,
            "retention_hours": settings.NEWS_RETENTION_HOURS,
        }
        db.commit()
        
        logger.info(
            f"[Task {task_record.id}] âœ… å®Œæˆ! "
            f"çˆ¬å–: {len(news_list)} â†’ è¿‡æ»¤: {len(recent_news)} â†’ ä¿å­˜: {saved_count}, "
            f"è€—æ—¶: {execution_time:.2f}s"
        )
        
        return {
            "task_id": task_record.id,
            "status": "completed",
            "source": source,
            "crawled": len(news_list),
            "filtered": len(recent_news),
            "saved": saved_count,
            "duplicates": duplicate_count,
            "execution_time": execution_time,
            "timestamp": datetime.utcnow().isoformat(),
        }
        
    except Exception as e:
        logger.error(f"[Task {task_record.id if task_record else 'unknown'}] çˆ¬å–å¤±è´¥: {e}", exc_info=True)
        
        if task_record:
            task_record.status = TaskStatus.FAILED
            task_record.completed_at = datetime.utcnow()
            task_record.error_message = str(e)[:1000]
            db.commit()
        
        # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®© Celery è®°å½•
        raise
    
    finally:
        db.close()


@celery_app.task(bind=True, name="app.tasks.crawl_tasks.cold_start_crawl_task")
def cold_start_crawl_task(
    self,
    source: str = "sina",
    start_page: int = 1,
    end_page: int = 50,
):
    """
    å†·å¯åŠ¨æ‰¹é‡çˆ¬å–ä»»åŠ¡
    
    Args:
        source: æ–°é—»æº
        start_page: èµ·å§‹é¡µ
        end_page: ç»“æŸé¡µ
    """
    db = get_sync_db_session()
    task_record = None
    
    try:
        # 1. åˆ›å»ºä»»åŠ¡è®°å½•
        task_record = CrawlTask(
            celery_task_id=self.request.id,
            mode=CrawlMode.COLD_START,
            status=TaskStatus.RUNNING,
            source=source,
            config={
                "start_page": start_page,
                "end_page": end_page,
            },
            total_pages=end_page - start_page + 1,
            started_at=datetime.utcnow(),
        )
        db.add(task_record)
        db.commit()
        db.refresh(task_record)
        
        logger.info(f"[Task {task_record.id}] å¼€å§‹å†·å¯åŠ¨çˆ¬å–: {source}, é¡µç  {start_page}-{end_page}")
        
        # 2. åˆ›å»ºçˆ¬è™«
        if source == "sina":
            crawler = SinaCrawlerTool()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–°é—»æº: {source}")
        
        # 3. åˆ†é¡µçˆ¬å–
        start_time = datetime.utcnow()
        total_crawled = 0
        total_saved = 0
        
        for page in range(start_page, end_page + 1):
            try:
                # æ›´æ–°è¿›åº¦
                task_record.current_page = page
                task_record.progress = {
                    "current_page": page,
                    "total_pages": task_record.total_pages,
                    "percentage": round((page - start_page + 1) / task_record.total_pages * 100, 2),
                }
                db.commit()
                
                # çˆ¬å–å•é¡µ
                news_list = crawler.crawl(start_page=page, end_page=page)
                total_crawled += len(news_list)
                
                # ä¿å­˜æ–°é—»
                page_saved = 0
                for news_item in news_list:
                    existing = db.execute(
                        select(News).where(News.url == news_item.url)
                    ).scalar_one_or_none()
                    
                    if not existing:
                        news = News(
                            title=news_item.title,
                            content=news_item.content,
                            raw_html=news_item.raw_html,  # ä¿å­˜åŸå§‹ HTML
                            url=news_item.url,
                            source=news_item.source,
                            publish_time=news_item.publish_time,
                            author=news_item.author,
                            keywords=news_item.keywords,
                            stock_codes=news_item.stock_codes,
                        )
                        db.add(news)
                        page_saved += 1
                
                db.commit()
                total_saved += page_saved
                
                logger.info(
                    f"[Task {task_record.id}] é¡µ {page}/{end_page}: "
                    f"çˆ¬å– {len(news_list)} æ¡, ä¿å­˜ {page_saved} æ¡"
                )
                
            except Exception as e:
                logger.error(f"[Task {task_record.id}] é¡µ {page} çˆ¬å–å¤±è´¥: {e}")
                continue
        
        # 4. æ›´æ–°ä»»åŠ¡çŠ¶æ€
        end_time = datetime.utcnow()
        execution_time = (end_time - start_time).total_seconds()
        
        task_record.status = TaskStatus.COMPLETED
        task_record.completed_at = end_time
        task_record.execution_time = execution_time
        task_record.crawled_count = total_crawled
        task_record.saved_count = total_saved
        task_record.result = {
            "pages_crawled": end_page - start_page + 1,
            "total_crawled": total_crawled,
            "total_saved": total_saved,
            "duplicates": total_crawled - total_saved,
        }
        db.commit()
        
        logger.info(
            f"[Task {task_record.id}] å†·å¯åŠ¨å®Œæˆ! "
            f"é¡µæ•°: {end_page - start_page + 1}, çˆ¬å–: {total_crawled}, ä¿å­˜: {total_saved}, "
            f"è€—æ—¶: {execution_time:.2f}s"
        )
        
        return {
            "task_id": task_record.id,
            "status": "completed",
            "crawled": total_crawled,
            "saved": total_saved,
            "execution_time": execution_time,
        }
        
    except Exception as e:
        logger.error(f"[Task {task_record.id if task_record else 'unknown'}] å†·å¯åŠ¨å¤±è´¥: {e}", exc_info=True)
        
        if task_record:
            task_record.status = TaskStatus.FAILED
            task_record.completed_at = datetime.utcnow()
            task_record.error_message = str(e)[:1000]
            db.commit()
        
        raise
    
    finally:
        db.close()


@celery_app.task(bind=True, name="app.tasks.crawl_tasks.targeted_stock_crawl_task")
def targeted_stock_crawl_task(
    self,
    stock_code: str,
    stock_name: str,
    days: int = 30
):
    """
    å®šå‘çˆ¬å–æŸåªè‚¡ç¥¨çš„ç›¸å…³æ–°é—»
    
    æ•°æ®æ¥æºï¼š
    1. BochaAI æœç´¢å¼•æ“ API
    2. ä¸œæ–¹è´¢å¯Œç­‰è´¢ç»ç½‘ç«™ï¼ˆå¯æ‰©å±•ï¼‰
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ SH600519ï¼‰
        stock_name: è‚¡ç¥¨åç§°ï¼ˆå¦‚ è´µå·èŒ…å°ï¼‰
        days: æœç´¢æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼‰ï¼Œé»˜è®¤30å¤©
    """
    db = get_sync_db_session()
    task_record = None
    
    try:
        # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
        code = stock_code.upper()
        if code.startswith("SH") or code.startswith("SZ"):
            pure_code = code[2:]
        else:
            pure_code = code
            code = f"SH{code}" if code.startswith("6") else f"SZ{code}"
        
        # 1. åˆ›å»ºä»»åŠ¡è®°å½•
        task_record = CrawlTask(
            celery_task_id=self.request.id,
            mode=CrawlMode.TARGETED,
            status=TaskStatus.RUNNING,
            source="targeted",
            config={
                "stock_code": code,
                "stock_name": stock_name,
                "days": days,
            },
            started_at=datetime.utcnow(),
        )
        db.add(task_record)
        db.commit()
        db.refresh(task_record)
        
        logger.info(f"[Task {task_record.id}] ğŸ¯ å¼€å§‹å®šå‘çˆ¬å–: {stock_name}({code}), æ—¶é—´èŒƒå›´: {days}å¤©")
        
        start_time = datetime.utcnow()
        all_news = []
        search_results = []
        filtered_news = []
        
        # 2. ä½¿ç”¨ BochaAI æœç´¢å¼•æ“æœç´¢æ–°é—»
        if bochaai_search.is_available():
            logger.info(f"[Task {task_record.id}] ğŸ” ä½¿ç”¨ BochaAI æœç´¢...")
            
            search_results = bochaai_search.search_stock_news(
                stock_name=stock_name,
                stock_code=pure_code,
                days=days,
                count=100,  # è·å–100æ¡æ–°é—»
                max_age_days=90  # åªè·å–æœ€è¿‘3ä¸ªæœˆçš„æ–°é—»
            )
            
            logger.info(f"[Task {task_record.id}] ğŸ“° BochaAI æœç´¢åˆ° {len(search_results)} æ¡ç»“æœ")
            
            # åˆ›å»ºå¢å¼ºçˆ¬è™«å®ä¾‹ï¼Œç”¨äºäºŒæ¬¡çˆ¬å–å®Œæ•´å†…å®¹
            enhanced_crawler = EnhancedCrawler(use_cache=True)
            
            # è½¬æ¢æœç´¢ç»“æœä¸º NewsItemï¼Œå¹¶äºŒæ¬¡çˆ¬å–å®Œæ•´å†…å®¹
            for idx, result in enumerate(search_results):
                # è§£æå‘å¸ƒæ—¶é—´
                publish_time = None
                if result.date_published:
                    try:
                        # å°è¯•è§£æ ISO æ ¼å¼
                        publish_time = datetime.fromisoformat(
                            result.date_published.replace('Z', '+00:00')
                        )
                    except (ValueError, AttributeError):
                        pass
                
                # äºŒæ¬¡çˆ¬å–å®Œæ•´å†…å®¹
                full_content = result.snippet  # é»˜è®¤ä½¿ç”¨æ‘˜è¦
                raw_html = None  # åŸå§‹ HTML
                try:
                    logger.info(f"[Task {task_record.id}] ğŸ”— [{idx+1}/{len(search_results)}] çˆ¬å–å®Œæ•´å†…å®¹: {result.url[:60]}...")
                    article = enhanced_crawler.crawl(result.url, engine='auto')
                    if article and article.content and len(article.content) > len(result.snippet):
                        full_content = article.content
                        raw_html = article.html_content  # ä¿å­˜åŸå§‹ HTML
                        logger.info(f"[Task {task_record.id}] âœ… è·å–å®Œæ•´å†…å®¹: {len(full_content)} å­—ç¬¦, HTML: {len(raw_html) if raw_html else 0} å­—ç¬¦")
                    else:
                        logger.warning(f"[Task {task_record.id}] âš ï¸ å®Œæ•´å†…å®¹è·å–å¤±è´¥æˆ–å†…å®¹æ›´çŸ­ï¼Œä½¿ç”¨æ‘˜è¦")
                except Exception as e:
                    logger.warning(f"[Task {task_record.id}] âš ï¸ äºŒæ¬¡çˆ¬å–å¤±è´¥: {e}, ä½¿ç”¨æ‘˜è¦")
                
                news_item = NewsItem(
                    title=result.title,
                    content=full_content,  # ä½¿ç”¨å®Œæ•´å†…å®¹
                    url=result.url,
                    source=result.site_name or "web_search",
                    publish_time=publish_time,
                    stock_codes=[pure_code, code],  # å…³è”è‚¡ç¥¨ä»£ç 
                    raw_html=raw_html,  # åŸå§‹ HTML
                )
                all_news.append(news_item)
        else:
            logger.warning(f"[Task {task_record.id}] âš ï¸ BochaAI API Key æœªé…ç½®ï¼Œè·³è¿‡æœç´¢å¼•æ“æœç´¢")
        
        # 3. ä½¿ç”¨å¤šä¸ªçˆ¬è™«ä½œä¸ºè¡¥å……æ¥æº
        # å®šä¹‰è¦ä½¿ç”¨çš„çˆ¬è™«åˆ—è¡¨ï¼ˆçˆ¬è™«åç§°, çˆ¬å–é¡µæ•°, å›¾æ ‡ï¼‰
        crawler_configs = [
            ("eastmoney", 3, "ğŸ’"),  # ä¸œæ–¹è´¢å¯Œ
            ("sina", 2, "ğŸŒ"),       # æ–°æµªè´¢ç»
            ("tencent", 2, "ğŸ§"),    # è…¾è®¯è´¢ç»
            ("163", 2, "ğŸ“§"),        # ç½‘æ˜“è´¢ç»
            ("nbd", 2, "ğŸ“°"),        # æ¯æ—¥ç»æµæ–°é—»
            ("yicai", 2, "ğŸ¯"),      # ç¬¬ä¸€è´¢ç»
            ("caijing", 2, "ğŸ“ˆ"),    # è´¢ç»ç½‘
            ("jingji21", 2, "ğŸ“‰"),   # 21ç»æµç½‘
            ("eeo", 2, "ğŸ“Š"),        # ç»æµè§‚å¯Ÿç½‘
            ("jwview", 2, "ğŸ’°"),     # é‡‘èç•Œ
        ]
        
        total_crawlers = len(crawler_configs)
        for idx, (crawler_name, pages, icon) in enumerate(crawler_configs):
            try:
                logger.info(f"[Task {task_record.id}] {icon} [{idx+1}/{total_crawlers}] ä½¿ç”¨ {crawler_name} çˆ¬è™«...")
                
                # æ›´æ–°è¿›åº¦
                task_record.progress = {
                    "current": idx + 1,
                    "total": total_crawlers,
                    "message": f"æ­£åœ¨çˆ¬å– {crawler_name}..."
                }
                db.commit()
                
                crawler = get_crawler_tool(crawler_name)
                crawler_news = crawler.crawl(start_page=1, end_page=pages)
                
                # è¿‡æ»¤åŒ…å«è‚¡ç¥¨åç§°æˆ–ä»£ç çš„æ–°é—»
                matched_count = 0
                for news in crawler_news:
                    # æ£€æŸ¥æ ‡é¢˜æˆ–å†…å®¹æ˜¯å¦åŒ…å«è‚¡ç¥¨åç§°æˆ–ä»£ç 
                    title_match = (stock_name in news.title or pure_code in news.title)
                    content_match = (stock_name in (news.content or '') or pure_code in (news.content or ''))
                    
                    if title_match or content_match:
                        # æ·»åŠ è‚¡ç¥¨ä»£ç å…³è”
                        if not news.stock_codes:
                            news.stock_codes = []
                        if pure_code not in news.stock_codes:
                            news.stock_codes.append(pure_code)
                        if code not in news.stock_codes:
                            news.stock_codes.append(code)
                        filtered_news.append(news)
                        matched_count += 1
                
                logger.info(f"[Task {task_record.id}] {icon} {crawler_name} çˆ¬å– {len(crawler_news)} æ¡ï¼ŒåŒ¹é… {matched_count} æ¡")
                
            except Exception as e:
                logger.warning(f"[Task {task_record.id}] âš ï¸ {crawler_name} çˆ¬å–å¤±è´¥: {e}")
                continue
        
        # åˆå¹¶æ‰€æœ‰çˆ¬è™«è·å–çš„æ–°é—»
        all_news.extend(filtered_news)
        logger.info(f"[Task {task_record.id}] ğŸ“° å¤šçˆ¬è™«å…±è¿‡æ»¤å‡º {len(filtered_news)} æ¡ç›¸å…³æ–°é—»")
        
        # 4. å»é‡å¹¶ä¿å­˜
        saved_count = 0
        duplicate_count = 0
        
        logger.info(f"[Task {task_record.id}] ğŸ’¾ å¼€å§‹ä¿å­˜ {len(all_news)} æ¡æ–°é—»...")
        
        for news_item in all_news:
            # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
            existing = db.execute(
                select(News).where(News.url == news_item.url)
            ).scalar_one_or_none()
            
            if existing:
                duplicate_count += 1
                # å¦‚æœå·²å­˜åœ¨ä½†æ²¡æœ‰å…³è”è¿™ä¸ªè‚¡ç¥¨ï¼Œæ›´æ–°å…³è”
                if existing.stock_codes is None:
                    existing.stock_codes = []
                if pure_code not in existing.stock_codes:
                    existing.stock_codes = existing.stock_codes + [pure_code]
                    db.commit()
                continue
            
            # åˆ›å»ºæ–°è®°å½•
            news = News(
                title=news_item.title,
                content=news_item.content,
                raw_html=news_item.raw_html,  # ä¿å­˜åŸå§‹ HTML
                url=news_item.url,
                source=news_item.source,
                publish_time=news_item.publish_time,
                author=news_item.author,
                keywords=news_item.keywords,
                stock_codes=news_item.stock_codes or [pure_code, code],
            )
            
            db.add(news)
            saved_count += 1
        
        db.commit()
        
        logger.info(
            f"[Task {task_record.id}] ğŸ’¾ ä¿å­˜ {saved_count} æ¡æ–°é—» "
            f"(é‡å¤: {duplicate_count})"
        )
        
        # 5. æ›´æ–°ä»»åŠ¡çŠ¶æ€
        end_time = datetime.utcnow()
        execution_time = (end_time - start_time).total_seconds()
        
        task_record.status = TaskStatus.COMPLETED
        task_record.completed_at = end_time
        task_record.execution_time = execution_time
        task_record.crawled_count = len(all_news)
        task_record.saved_count = saved_count
        task_record.result = {
            "stock_code": code,
            "stock_name": stock_name,
            "total_found": len(all_news),
            "saved": saved_count,
            "duplicates": duplicate_count,
            "sources": {
                "bochaai": len(search_results),
                "eastmoney": len(filtered_news),
            }
        }
        task_record.progress = {
            "current": 100,
            "total": 100,
            "message": f"å®Œæˆï¼æ–°å¢ {saved_count} æ¡æ–°é—»"
        }
        db.commit()
        
        logger.info(
            f"[Task {task_record.id}] âœ… å®šå‘çˆ¬å–å®Œæˆ! "
            f"è‚¡ç¥¨: {stock_name}({code}), æ‰¾åˆ°: {len(all_news)}, ä¿å­˜: {saved_count}, "
            f"è€—æ—¶: {execution_time:.2f}s"
        )
        
        return {
            "task_id": task_record.id,
            "status": "completed",
            "stock_code": code,
            "stock_name": stock_name,
            "crawled": len(all_news),
            "saved": saved_count,
            "duplicates": duplicate_count,
            "execution_time": execution_time,
            "timestamp": datetime.utcnow().isoformat(),
        }
        
    except Exception as e:
        logger.error(f"[Task {task_record.id if task_record else 'unknown'}] å®šå‘çˆ¬å–å¤±è´¥: {e}", exc_info=True)
        
        if task_record:
            task_record.status = TaskStatus.FAILED
            task_record.completed_at = datetime.utcnow()
            task_record.error_message = str(e)[:1000]
            task_record.progress = {
                "current": 0,
                "total": 100,
                "message": f"å¤±è´¥: {str(e)[:100]}"
            }
            db.commit()
        
        raise
    
    finally:
        db.close()

