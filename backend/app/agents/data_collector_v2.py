"""
æ•°æ®ä¸“å‘˜æ™ºèƒ½ä½“ V2 (DataCollectorAgent)

ç»Ÿä¸€è´Ÿè´£æ‰€æœ‰æ•°æ®è·å–ä»»åŠ¡ï¼Œæ”¯æŒï¼š
- è¾©è®ºå‰çš„åˆå§‹æ•°æ®æ”¶é›†
- è¾©è®ºä¸­çš„åŠ¨æ€æ•°æ®è¡¥å……
- ç”¨æˆ·è¿½é—®æ—¶çš„æŒ‰éœ€æœç´¢

æ ¸å¿ƒç‰¹æ€§ï¼š
1. è®¡åˆ’/æ‰§è¡Œåˆ†ç¦»ï¼šå…ˆç”Ÿæˆæœç´¢è®¡åˆ’ï¼Œç”¨æˆ·ç¡®è®¤åå†æ‰§è¡Œ
2. å¤šæ•°æ®æºæ”¯æŒï¼šAkShareã€BochaAIã€ç½‘é¡µæœç´¢ã€çŸ¥è¯†åº“
3. æ™ºèƒ½æ„å›¾è¯†åˆ«ï¼šæ ¹æ®ç”¨æˆ·é—®é¢˜è‡ªåŠ¨é€‰æ‹©æ•°æ®æº
"""
import logging
import re
import asyncio
from typing import Dict, Any, List, Optional, ClassVar, Pattern
from datetime import datetime
from enum import Enum
from pydantic import BaseModel, Field

from agenticx.core.agent import Agent
from ..services.llm_service import get_llm_provider
from ..services.stock_data_service import stock_data_service
from ..tools.bochaai_search import bochaai_search, SearchResult
from ..tools.interactive_crawler import InteractiveCrawler

logger = logging.getLogger(__name__)


class SearchSource(str, Enum):
    """æœç´¢æ•°æ®æºç±»å‹"""
    AKSHARE = "akshare"           # AkShare è´¢åŠ¡/è¡Œæƒ…æ•°æ®
    BOCHAAI = "bochaai"           # BochaAI Webæœç´¢
    BROWSER = "browser"           # äº¤äº’å¼æµè§ˆå™¨æœç´¢
    KNOWLEDGE_BASE = "kb"         # å†…éƒ¨çŸ¥è¯†åº“
    ALL = "all"                   # æ‰€æœ‰æ¥æº


class SearchTask(BaseModel):
    """å•ä¸ªæœç´¢ä»»åŠ¡"""
    id: str = Field(..., description="ä»»åŠ¡ID")
    source: SearchSource = Field(..., description="æ•°æ®æº")
    query: str = Field(..., description="æœç´¢æŸ¥è¯¢")
    description: str = Field("", description="ä»»åŠ¡æè¿°ï¼ˆç”¨äºå±•ç¤ºç»™ç”¨æˆ·ï¼‰")
    data_type: Optional[str] = Field(None, description="æ•°æ®ç±»å‹ï¼ˆå¦‚ financial, news, klineï¼‰")
    icon: str = Field("ğŸ”", description="å›¾æ ‡ï¼ˆç”¨äºUIå±•ç¤ºï¼‰")
    estimated_time: int = Field(3, description="é¢„è®¡è€—æ—¶ï¼ˆç§’ï¼‰")


class SearchPlan(BaseModel):
    """æœç´¢è®¡åˆ’"""
    plan_id: str = Field(..., description="è®¡åˆ’ID")
    stock_code: str = Field(..., description="è‚¡ç¥¨ä»£ç ")
    stock_name: str = Field("", description="è‚¡ç¥¨åç§°")
    user_query: str = Field(..., description="ç”¨æˆ·åŸå§‹é—®é¢˜")
    tasks: List[SearchTask] = Field(default_factory=list, description="æœç´¢ä»»åŠ¡åˆ—è¡¨")
    total_estimated_time: int = Field(0, description="æ€»é¢„è®¡è€—æ—¶ï¼ˆç§’ï¼‰")
    created_at: str = Field(default_factory=lambda: datetime.utcnow().isoformat())
    status: str = Field("pending", description="çŠ¶æ€ï¼špending, confirmed, executing, completed, cancelled")


class SearchResult(BaseModel):
    """æœç´¢ç»“æœ"""
    task_id: str
    source: str
    success: bool
    data: Dict[str, Any] = Field(default_factory=dict)
    summary: str = ""
    error: Optional[str] = None
    execution_time: float = 0


class DataCollectorAgentV2(Agent):
    """
    æ•°æ®ä¸“å‘˜æ™ºèƒ½ä½“ V2
    
    æ”¯æŒ"ç¡®è®¤ä¼˜å…ˆ"æ¨¡å¼ï¼š
    1. ç”¨æˆ· @æ•°æ®ä¸“å‘˜ æé—®
    2. ç”Ÿæˆæœç´¢è®¡åˆ’ï¼ˆä¸æ‰§è¡Œï¼‰
    3. ç”¨æˆ·ç¡®è®¤åæ‰§è¡Œ
    4. è¿”å›ç»“æœ
    """
    
    # å…³é”®è¯åˆ°æ•°æ®æºçš„æ˜ å°„
    KEYWORD_SOURCE_MAP: ClassVar[Dict[str, tuple]] = {
        # è´¢åŠ¡ç›¸å…³ -> AkShare
        "è´¢åŠ¡": (SearchSource.AKSHARE, "financial", "ğŸ“Š"),
        "pe": (SearchSource.AKSHARE, "financial", "ğŸ“Š"),
        "pb": (SearchSource.AKSHARE, "financial", "ğŸ“Š"),
        "roe": (SearchSource.AKSHARE, "financial", "ğŸ“Š"),
        "åˆ©æ¶¦": (SearchSource.AKSHARE, "financial", "ğŸ“Š"),
        "è¥æ”¶": (SearchSource.AKSHARE, "financial", "ğŸ“Š"),
        "ä¼°å€¼": (SearchSource.AKSHARE, "financial", "ğŸ“Š"),
        "å¸‚ç›ˆ": (SearchSource.AKSHARE, "financial", "ğŸ“Š"),
        "å¸‚å‡€": (SearchSource.AKSHARE, "financial", "ğŸ“Š"),
        "æŠ¥è¡¨": (SearchSource.AKSHARE, "financial", "ğŸ“Š"),
        
        # èµ„é‡‘/è¡Œæƒ… -> AkShare
        "èµ„é‡‘": (SearchSource.AKSHARE, "fund_flow", "ğŸ’°"),
        "ä¸»åŠ›": (SearchSource.AKSHARE, "fund_flow", "ğŸ’°"),
        "æµå…¥": (SearchSource.AKSHARE, "fund_flow", "ğŸ’°"),
        "æµå‡º": (SearchSource.AKSHARE, "fund_flow", "ğŸ’°"),
        "è¡Œæƒ…": (SearchSource.AKSHARE, "realtime", "ğŸ“ˆ"),
        "ä»·æ ¼": (SearchSource.AKSHARE, "realtime", "ğŸ“ˆ"),
        "æ¶¨è·Œ": (SearchSource.AKSHARE, "realtime", "ğŸ“ˆ"),
        "kçº¿": (SearchSource.AKSHARE, "kline", "ğŸ“ˆ"),
        "èµ°åŠ¿": (SearchSource.AKSHARE, "kline", "ğŸ“ˆ"),
        
        # æ–°é—»ç›¸å…³ -> BochaAI
        "æ–°é—»": (SearchSource.BOCHAAI, "news", "ğŸ“°"),
        "èµ„è®¯": (SearchSource.BOCHAAI, "news", "ğŸ“°"),
        "æŠ¥é“": (SearchSource.BOCHAAI, "news", "ğŸ“°"),
        "å…¬å‘Š": (SearchSource.BOCHAAI, "news", "ğŸ“°"),
        "æ¶ˆæ¯": (SearchSource.BOCHAAI, "news", "ğŸ“°"),
        
        # ä¸Šä¸‹æ¸¸/äº§ä¸šé“¾ -> å¤šæºæœç´¢
        "ä¸Šä¸‹æ¸¸": (SearchSource.BROWSER, "industry", "ğŸ”—"),
        "ä¾›åº”é“¾": (SearchSource.BROWSER, "industry", "ğŸ”—"),
        "å®¢æˆ·": (SearchSource.BROWSER, "industry", "ğŸ”—"),
        "ä¾›åº”å•†": (SearchSource.BROWSER, "industry", "ğŸ”—"),
        "åˆä½œ": (SearchSource.BROWSER, "industry", "ğŸ”—"),
        "äº§ä¸šé“¾": (SearchSource.BROWSER, "industry", "ğŸ”—"),
    }
    
    def __init__(self, llm_provider=None, organization_id: str = "finnews"):
        super().__init__(
            name="DataCollector",
            role="æ•°æ®ä¸“å‘˜",
            goal="æ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼Œä»å¤šä¸ªæ•°æ®æºæœé›†å’Œæ•´ç†ç›¸å…³ä¿¡æ¯ï¼Œæ”¯æŒè¾©è®ºå‰å‡†å¤‡å’Œè¾©è®ºä¸­è¿½é—®",
            backstory="""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èæ•°æ®ä¸“å®¶ï¼Œç²¾é€šå„ç±»é‡‘èæ•°æ®æºçš„ä½¿ç”¨ã€‚
ä½ çš„èŒè´£æ˜¯ï¼š
1. ç†è§£ç”¨æˆ·çš„æ•°æ®éœ€æ±‚
2. åˆ¶å®šåˆç†çš„æœç´¢è®¡åˆ’
3. ä»å¤šä¸ªæ•°æ®æºè·å–æ•°æ®
4. æ•´ç†å¹¶æ ¼å¼åŒ–æ•°æ®

ä½ èƒ½å¤Ÿè®¿é—®çš„æ•°æ®æºåŒ…æ‹¬ï¼š
- AkShare: è‚¡ç¥¨è´¢åŠ¡æŒ‡æ ‡ã€Kçº¿è¡Œæƒ…ã€èµ„é‡‘æµå‘ç­‰
- BochaAI: å®æ—¶æ–°é—»æœç´¢ã€è´¢ç»æŠ¥é“
- ç½‘é¡µæœç´¢: ç™¾åº¦èµ„è®¯ã€æœç‹—ç­‰
- çŸ¥è¯†åº“: å†å²æ–°é—»å’Œåˆ†ææ•°æ®""",
            organization_id=organization_id
        )
        
        if llm_provider is None:
            llm_provider = get_llm_provider()
        object.__setattr__(self, '_llm_provider', llm_provider)
        
        # åˆå§‹åŒ–æœç´¢å·¥å…·
        self._interactive_crawler = InteractiveCrawler(timeout=20)
        
        logger.info(f"âœ… Initialized DataCollectorV2 with multi-source search capabilities")
    
    async def generate_search_plan(
        self,
        query: str,
        stock_code: str,
        stock_name: str = ""
    ) -> SearchPlan:
        """
        ç”Ÿæˆæœç´¢è®¡åˆ’ï¼ˆä¸æ‰§è¡Œï¼‰
        
        æ ¹æ®ç”¨æˆ·é—®é¢˜åˆ†æéœ€è¦å“ªäº›æ•°æ®ï¼Œç”Ÿæˆå¾…ç¡®è®¤çš„æœç´¢è®¡åˆ’
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            stock_code: è‚¡ç¥¨ä»£ç 
            stock_name: è‚¡ç¥¨åç§°
            
        Returns:
            SearchPlan å¯¹è±¡
        """
        logger.info(f"ğŸ“‹ DataCollector: ä¸º '{query}' ç”Ÿæˆæœç´¢è®¡åˆ’...")
        
        plan_id = f"plan_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}_{stock_code}"
        
        plan = SearchPlan(
            plan_id=plan_id,
            stock_code=stock_code,
            stock_name=stock_name or stock_code,
            user_query=query,
            tasks=[],
            status="pending"
        )
        
        query_lower = query.lower()
        
        # 1. åŸºäºå…³é”®è¯åŒ¹é…ç”Ÿæˆä»»åŠ¡
        matched_sources = set()
        for keyword, (source, data_type, icon) in self.KEYWORD_SOURCE_MAP.items():
            if keyword in query_lower:
                if (source, data_type) not in matched_sources:
                    matched_sources.add((source, data_type))
                    task = self._create_task(
                        source=source,
                        data_type=data_type,
                        icon=icon,
                        query=query,
                        stock_code=stock_code,
                        stock_name=stock_name
                    )
                    plan.tasks.append(task)
        
        # 2. å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•å…³é”®è¯ï¼Œä½¿ç”¨ LLM åˆ†æ
        if not plan.tasks:
            plan.tasks = await self._analyze_with_llm(query, stock_code, stock_name)
        
        # 3. å¦‚æœè¿˜æ˜¯æ²¡æœ‰ä»»åŠ¡ï¼Œæ·»åŠ é»˜è®¤çš„ç»¼åˆæœç´¢
        if not plan.tasks:
            plan.tasks = [
                SearchTask(
                    id=f"task_{plan_id}_1",
                    source=SearchSource.BOCHAAI,
                    query=f"{stock_name or stock_code} {query}",
                    description=f"æœç´¢ {stock_name} ç›¸å…³æ–°é—»",
                    icon="ğŸ“°",
                    estimated_time=3
                ),
                SearchTask(
                    id=f"task_{plan_id}_2",
                    source=SearchSource.AKSHARE,
                    query=query,
                    description="è·å–æœ€æ–°è´¢åŠ¡å’Œè¡Œæƒ…æ•°æ®",
                    data_type="overview",
                    icon="ğŸ“Š",
                    estimated_time=2
                )
            ]
        
        # è®¡ç®—æ€»è€—æ—¶
        plan.total_estimated_time = sum(t.estimated_time for t in plan.tasks)
        
        logger.info(f"âœ… ç”Ÿæˆæœç´¢è®¡åˆ’: {len(plan.tasks)} ä¸ªä»»åŠ¡ï¼Œé¢„è®¡è€—æ—¶ {plan.total_estimated_time}s")
        
        return plan
    
    def _create_task(
        self,
        source: SearchSource,
        data_type: str,
        icon: str,
        query: str,
        stock_code: str,
        stock_name: str
    ) -> SearchTask:
        """åˆ›å»ºæœç´¢ä»»åŠ¡"""
        task_id = f"task_{datetime.utcnow().strftime('%H%M%S%f')}"
        
        # æ ¹æ®æ•°æ®ç±»å‹ç”Ÿæˆæè¿°
        descriptions = {
            "financial": f"è·å– {stock_name or stock_code} è´¢åŠ¡æŒ‡æ ‡ï¼ˆPE/PB/ROEç­‰ï¼‰",
            "fund_flow": f"è·å– {stock_name or stock_code} èµ„é‡‘æµå‘ï¼ˆä¸»åŠ›/æ•£æˆ·ï¼‰",
            "realtime": f"è·å– {stock_name or stock_code} å®æ—¶è¡Œæƒ…",
            "kline": f"è·å– {stock_name or stock_code} Kçº¿èµ°åŠ¿",
            "news": f"æœç´¢ {stock_name or stock_code} æœ€æ–°æ–°é—»",
            "industry": f"æœç´¢ {stock_name or stock_code} äº§ä¸šé“¾/ä¸Šä¸‹æ¸¸ä¿¡æ¯",
        }
        
        # æ ¹æ®æ•°æ®ç±»å‹ç”ŸæˆæŸ¥è¯¢
        queries = {
            "financial": stock_code,
            "fund_flow": stock_code,
            "realtime": stock_code,
            "kline": stock_code,
            "news": f"{stock_name or stock_code} {query}",
            "industry": f"{stock_name or stock_code} {query}",
        }
        
        return SearchTask(
            id=task_id,
            source=source,
            query=queries.get(data_type, query),
            description=descriptions.get(data_type, f"æœç´¢: {query}"),
            data_type=data_type,
            icon=icon,
            estimated_time=3 if source != SearchSource.BROWSER else 5
        )
    
    async def _analyze_with_llm(
        self,
        query: str,
        stock_code: str,
        stock_name: str
    ) -> List[SearchTask]:
        """ä½¿ç”¨ LLM åˆ†æéœ€è¦å“ªäº›æ•°æ®"""
        try:
            prompt = f"""åˆ†æä»¥ä¸‹ç”¨æˆ·é—®é¢˜ï¼Œåˆ¤æ–­éœ€è¦æœç´¢å“ªäº›æ•°æ®ï¼š

ç”¨æˆ·é—®é¢˜: "{query}"
è‚¡ç¥¨: {stock_name}({stock_code})

å¯ç”¨æ•°æ®æº:
1. akshare - è´¢åŠ¡æ•°æ®ï¼ˆPE/PB/ROEç­‰ï¼‰ã€èµ„é‡‘æµå‘ã€å®æ—¶è¡Œæƒ…ã€Kçº¿
2. bochaai - æ–°é—»æœç´¢ã€è´¢ç»æŠ¥é“
3. browser - ç½‘é¡µæœç´¢ï¼ˆé€‚åˆæœç´¢äº§ä¸šé“¾ã€ä¸Šä¸‹æ¸¸ã€åˆä½œæ–¹ç­‰ï¼‰
4. kb - å†å²æ–°é—»æ•°æ®åº“

è¯·è¿”å›éœ€è¦æœç´¢çš„å†…å®¹ï¼Œæ ¼å¼å¦‚ä¸‹ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰:
SOURCE:æ•°æ®æº|TYPE:æ•°æ®ç±»å‹|QUERY:æœç´¢è¯|DESC:æè¿°

ç¤ºä¾‹:
SOURCE:bochaai|TYPE:news|QUERY:STå›½å ä¸Šä¸‹æ¸¸|DESC:æœç´¢STå›½åä¸Šä¸‹æ¸¸ç›¸å…³æ–°é—»
SOURCE:akshare|TYPE:financial|QUERY:002074|DESC:è·å–å›½è½©é«˜ç§‘è´¢åŠ¡æ•°æ®

åªè¾“å‡º2-4ä¸ªæœ€ç›¸å…³çš„æœç´¢ä»»åŠ¡ã€‚"""

            response = self._llm_provider.invoke([
                {"role": "system", "content": "ä½ æ˜¯æ•°æ®æœç´¢ä¸“å®¶ï¼Œå¸®åŠ©åˆ†æéœ€è¦å“ªäº›æ•°æ®ã€‚"},
                {"role": "user", "content": prompt}
            ])
            
            content = response.content if hasattr(response, 'content') else str(response)
            
            tasks = []
            for line in content.strip().split('\n'):
                if 'SOURCE:' in line:
                    try:
                        parts = {}
                        for part in line.split('|'):
                            if ':' in part:
                                key, value = part.split(':', 1)
                                parts[key.strip().upper()] = value.strip()
                        
                        if 'SOURCE' in parts:
                            source_str = parts['SOURCE'].lower()
                            source = SearchSource(source_str) if source_str in [s.value for s in SearchSource] else SearchSource.BOCHAAI
                            
                            tasks.append(SearchTask(
                                id=f"task_llm_{len(tasks)+1}",
                                source=source,
                                query=parts.get('QUERY', query),
                                description=parts.get('DESC', f"æœç´¢: {query}"),
                                data_type=parts.get('TYPE', 'general'),
                                icon=self._get_icon_for_source(source),
                                estimated_time=3
                            ))
                    except Exception as e:
                        logger.debug(f"è§£æ LLM å“åº”è¡Œå¤±è´¥: {e}")
            
            return tasks
            
        except Exception as e:
            logger.warning(f"LLM åˆ†æå¤±è´¥: {e}")
            return []
    
    def _get_icon_for_source(self, source: SearchSource) -> str:
        """è·å–æ•°æ®æºå¯¹åº”çš„å›¾æ ‡"""
        icons = {
            SearchSource.AKSHARE: "ğŸ“Š",
            SearchSource.BOCHAAI: "ğŸ“°",
            SearchSource.BROWSER: "ğŸŒ",
            SearchSource.KNOWLEDGE_BASE: "ğŸ“š",
            SearchSource.ALL: "ğŸ”"
        }
        return icons.get(source, "ğŸ”")
    
    async def execute_search_plan(
        self,
        plan: SearchPlan
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæœç´¢è®¡åˆ’
        
        Args:
            plan: å·²ç¡®è®¤çš„æœç´¢è®¡åˆ’
            
        Returns:
            æœç´¢ç»“æœæ±‡æ€»
        """
        logger.info(f"ğŸš€ DataCollector: å¼€å§‹æ‰§è¡Œæœç´¢è®¡åˆ’ {plan.plan_id}...")
        
        plan.status = "executing"
        start_time = datetime.utcnow()
        
        results = {
            "plan_id": plan.plan_id,
            "stock_code": plan.stock_code,
            "stock_name": plan.stock_name,
            "user_query": plan.user_query,
            "task_results": [],
            "combined_data": {},
            "summary": "",
            "success": False,
            "execution_time": 0
        }
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        async_tasks = []
        for task in plan.tasks:
            async_tasks.append(self._execute_task(task, plan.stock_code, plan.stock_name))
        
        task_results = await asyncio.gather(*async_tasks, return_exceptions=True)
        
        # æ”¶é›†ç»“æœ
        for i, result in enumerate(task_results):
            if isinstance(result, Exception):
                logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {result}")
                results["task_results"].append(SearchResult(
                    task_id=plan.tasks[i].id,
                    source=plan.tasks[i].source.value,
                    success=False,
                    error=str(result)
                ).dict())
            else:
                results["task_results"].append(result.dict() if hasattr(result, 'dict') else result)
                if result.get("success"):
                    # åˆå¹¶æ•°æ®
                    source = result.get("source", "unknown")
                    if source not in results["combined_data"]:
                        results["combined_data"][source] = {}
                    results["combined_data"][source].update(result.get("data", {}))
        
        # ç”Ÿæˆç»¼åˆæ‘˜è¦
        results["summary"] = await self._generate_combined_summary(
            plan.user_query,
            results["combined_data"],
            plan.stock_name
        )
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        end_time = datetime.utcnow()
        results["execution_time"] = (end_time - start_time).total_seconds()
        results["success"] = any(r.get("success") for r in results["task_results"])
        
        plan.status = "completed"
        
        logger.info(f"âœ… æœç´¢è®¡åˆ’æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶ {results['execution_time']:.1f}s")
        
        return results
    
    async def _execute_task(
        self,
        task: SearchTask,
        stock_code: str,
        stock_name: str
    ) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªæœç´¢ä»»åŠ¡"""
        logger.info(f"ğŸ” æ‰§è¡Œä»»åŠ¡: {task.description}")
        
        start_time = datetime.utcnow()
        result = {
            "task_id": task.id,
            "source": task.source.value,
            "success": False,
            "data": {},
            "summary": "",
            "execution_time": 0
        }
        
        try:
            if task.source == SearchSource.AKSHARE:
                data = await self._search_akshare(task.query, stock_code, task.data_type)
                result["data"] = data or {}
                result["success"] = bool(data)
                
            elif task.source == SearchSource.BOCHAAI:
                data = await self._search_bochaai(task.query, stock_name)
                result["data"] = data or {}
                result["success"] = bool(data)
                
            elif task.source == SearchSource.BROWSER:
                data = await self._search_browser(task.query)
                result["data"] = data or {}
                result["success"] = bool(data)
                
            elif task.source == SearchSource.KNOWLEDGE_BASE:
                data = await self._search_knowledge_base(task.query, stock_code)
                result["data"] = data or {}
                result["success"] = bool(data)
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡ {task.id} æ‰§è¡Œå¤±è´¥: {e}")
            result["error"] = str(e)
        
        end_time = datetime.utcnow()
        result["execution_time"] = (end_time - start_time).total_seconds()
        
        return result
    
    async def _search_akshare(
        self,
        query: str,
        stock_code: str,
        data_type: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """ä» AkShare è·å–æ•°æ®"""
        data = {}
        
        try:
            if data_type == "financial" or data_type == "overview":
                financial = await stock_data_service.get_financial_indicators(stock_code)
                if financial:
                    data["financial_indicators"] = financial
            
            if data_type == "fund_flow" or data_type == "overview":
                fund_flow = await stock_data_service.get_fund_flow(stock_code, days=10)
                if fund_flow:
                    data["fund_flow"] = fund_flow
            
            if data_type == "realtime" or data_type == "overview":
                realtime = await stock_data_service.get_realtime_quote(stock_code)
                if realtime:
                    data["realtime_quote"] = realtime
            
            if data_type == "kline":
                kline = await stock_data_service.get_kline_data(stock_code, period="daily", limit=30)
                if kline:
                    data["kline_summary"] = {
                        "period": "daily",
                        "count": len(kline),
                        "latest": kline[-1] if kline else None,
                        "recent_5": kline[-5:] if len(kline) >= 5 else kline
                    }
            
            if data:
                logger.info(f"âœ… AkShare è¿”å›æ•°æ®: {list(data.keys())}")
                return data
                
        except Exception as e:
            logger.warning(f"AkShare æœç´¢å‡ºé”™: {e}")
        
        return None
    
    async def _search_bochaai(
        self,
        query: str,
        stock_name: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """ä» BochaAI æœç´¢æ–°é—»"""
        if not bochaai_search.is_available():
            logger.debug("BochaAI æœªé…ç½®ï¼Œè·³è¿‡")
            return None
        
        try:
            results = bochaai_search.search(
                query=query,
                freshness="oneWeek",
                count=10
            )
            
            if results:
                news_list = [
                    {
                        "title": r.title,
                        "snippet": r.snippet[:200] if r.snippet else "",
                        "url": r.url,
                        "source": r.site_name or "unknown",
                        "date": r.date_published or ""
                    }
                    for r in results
                ]
                logger.info(f"âœ… BochaAI è¿”å› {len(news_list)} æ¡æ–°é—»")
                return {"news": news_list, "count": len(news_list)}
        
        except Exception as e:
            logger.warning(f"BochaAI æœç´¢å‡ºé”™: {e}")
        
        return None
    
    async def _search_browser(self, query: str) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨äº¤äº’å¼çˆ¬è™«æœç´¢"""
        try:
            loop = asyncio.get_event_loop()
            results = await loop.run_in_executor(
                None,
                lambda: self._interactive_crawler.interactive_search(
                    query=query,
                    engines=["baidu_news", "sogou"],
                    num_results=10,
                    search_type="news"
                )
            )
            
            if results:
                news_list = [
                    {
                        "title": r.get("title", ""),
                        "snippet": r.get("snippet", "")[:200],
                        "url": r.get("url", ""),
                        "source": "browser_search"
                    }
                    for r in results
                ]
                logger.info(f"âœ… Browser è¿”å› {len(news_list)} æ¡ç»“æœ")
                return {"search_results": news_list, "count": len(news_list)}
        
        except Exception as e:
            logger.warning(f"Browser æœç´¢å‡ºé”™: {e}")
        
        return None
    
    async def _search_knowledge_base(
        self,
        query: str,
        stock_code: str
    ) -> Optional[Dict[str, Any]]:
        """ä»çŸ¥è¯†åº“æœç´¢å†å²æ•°æ®"""
        try:
            from ..services.news_service import news_service
            
            if stock_code and news_service:
                news_list = await news_service.get_news_by_stock(stock_code, limit=10)
                if news_list:
                    kb_news = [
                        {
                            "title": getattr(news, 'title', ''),
                            "content": (getattr(news, 'content', '') or '')[:300],
                            "source": getattr(news, 'source', ''),
                            "date": news.publish_time.isoformat() if hasattr(news, 'publish_time') and news.publish_time else ""
                        }
                        for news in news_list
                    ]
                    logger.info(f"âœ… KB è¿”å› {len(kb_news)} æ¡å†å²æ–°é—»")
                    return {"historical_news": kb_news, "count": len(kb_news)}
        
        except Exception as e:
            logger.debug(f"KB æœç´¢å‡ºé”™: {e}")
        
        return None
    
    async def _generate_combined_summary(
        self,
        query: str,
        data: Dict[str, Any],
        stock_name: str
    ) -> str:
        """ç”Ÿæˆç»¼åˆæ‘˜è¦"""
        summary_parts = [f"## æœç´¢ç»“æœ: {query}\n"]
        summary_parts.append(f"**è‚¡ç¥¨**: {stock_name}\n")
        
        # AkShare æ•°æ®
        if "akshare" in data:
            ak_data = data["akshare"]
            summary_parts.append("### ğŸ“Š è´¢åŠ¡/è¡Œæƒ…æ•°æ®\n")
            
            if "financial_indicators" in ak_data:
                fi = ak_data["financial_indicators"]
                summary_parts.append(f"- PE: {fi.get('pe_ratio', 'N/A')}, PB: {fi.get('pb_ratio', 'N/A')}")
                summary_parts.append(f"- ROE: {fi.get('roe', 'N/A')}%")
            
            if "realtime_quote" in ak_data:
                rt = ak_data["realtime_quote"]
                summary_parts.append(f"- å½“å‰ä»·: {rt.get('price', 'N/A')}å…ƒ, æ¶¨è·Œå¹…: {rt.get('change_percent', 'N/A')}%")
            
            if "fund_flow" in ak_data:
                ff = ak_data["fund_flow"]
                summary_parts.append(f"- èµ„é‡‘æµå‘: {ff.get('main_flow_trend', 'N/A')}")
            
            summary_parts.append("")
        
        # BochaAI æ–°é—»
        if "bochaai" in data:
            news = data["bochaai"].get("news", [])
            if news:
                summary_parts.append("### ğŸ“° æœ€æ–°æ–°é—»\n")
                for i, n in enumerate(news[:5], 1):
                    summary_parts.append(f"{i}. **{n['title'][:50]}**")
                    if n.get('snippet'):
                        summary_parts.append(f"   {n['snippet'][:100]}...")
                summary_parts.append("")
        
        # Browser ç»“æœ
        if "browser" in data:
            results = data["browser"].get("search_results", [])
            if results:
                summary_parts.append("### ğŸŒ ç½‘é¡µæœç´¢ç»“æœ\n")
                for i, r in enumerate(results[:5], 1):
                    summary_parts.append(f"{i}. {r['title'][:50]}")
                summary_parts.append("")
        
        # KB å†å²æ•°æ®
        if "kb" in data:
            kb_news = data["kb"].get("historical_news", [])
            if kb_news:
                summary_parts.append("### ğŸ“š å†å²èµ„æ–™\n")
                for i, n in enumerate(kb_news[:3], 1):
                    summary_parts.append(f"{i}. {n['title'][:50]}")
                summary_parts.append("")
        
        return "\n".join(summary_parts)
    
    # ============ å…¼å®¹æ—§ API ============
    
    async def collect_data(
        self,
        stock_code: str,
        stock_name: str,
        data_requirements: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        æœé›†è‚¡ç¥¨ç›¸å…³æ•°æ®ï¼ˆå…¼å®¹æ—§ APIï¼‰
        """
        # åˆ›å»ºå¹¶æ‰§è¡Œä¸€ä¸ªå…¨é¢çš„æœç´¢è®¡åˆ’
        plan = await self.generate_search_plan(
            query="ç»¼åˆæ•°æ®æœé›†",
            stock_code=stock_code,
            stock_name=stock_name
        )
        
        # æ·»åŠ æ‰€æœ‰åŸºç¡€æ•°æ®ä»»åŠ¡
        plan.tasks = [
            SearchTask(
                id=f"task_init_1",
                source=SearchSource.AKSHARE,
                query=stock_code,
                description="è·å–è´¢åŠ¡å’Œè¡Œæƒ…æ•°æ®",
                data_type="overview",
                icon="ğŸ“Š",
                estimated_time=3
            ),
            SearchTask(
                id=f"task_init_2",
                source=SearchSource.KNOWLEDGE_BASE,
                query=stock_code,
                description="è·å–å†å²æ–°é—»",
                data_type="news",
                icon="ğŸ“š",
                estimated_time=2
            )
        ]
        
        return await self.execute_search_plan(plan)


# å¿«é€Ÿåˆ†æå¸ˆï¼ˆä¿æŒä¸å˜ï¼‰
class QuickAnalystAgent(Agent):
    """å¿«é€Ÿåˆ†æå¸ˆæ™ºèƒ½ä½“"""
    
    def __init__(self, llm_provider=None, organization_id: str = "finnews"):
        super().__init__(
            name="QuickAnalyst",
            role="å¿«é€Ÿåˆ†æå¸ˆ",
            goal="å¿«é€Ÿç»¼åˆå¤šè§’åº¦ç»™å‡ºæŠ•èµ„å»ºè®®",
            backstory="""ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„é‡åŒ–åˆ†æå¸ˆï¼Œæ“…é•¿å¿«é€Ÿåˆ†æå’Œå†³ç­–ã€‚
ä½ èƒ½å¤Ÿåœ¨çŸ­æ—¶é—´å†…ç»¼åˆè€ƒè™‘å¤šç©ºå› ç´ ï¼Œç»™å‡ºç®€æ´æ˜äº†çš„æŠ•èµ„å»ºè®®ã€‚
ä½ çš„åˆ†æé£æ ¼æ˜¯ï¼šå¿«é€Ÿã€å‡†ç¡®ã€å®ç”¨ã€‚""",
            organization_id=organization_id
        )
        if llm_provider is None:
            llm_provider = get_llm_provider()
        object.__setattr__(self, '_llm_provider', llm_provider)
        logger.info(f"Initialized {self.name} agent")
    
    async def quick_analyze(
        self,
        stock_code: str,
        stock_name: str,
        context: str
    ) -> Dict[str, Any]:
        """å¿«é€Ÿåˆ†æ"""
        current_time = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
        
        prompt = f"""è¯·å¯¹ {stock_name}({stock_code}) è¿›è¡Œå¿«é€ŸæŠ•èµ„åˆ†æã€‚

ã€å½“å‰æ—¶é—´ã€‘
{current_time}

èƒŒæ™¯èµ„æ–™:
{context}

è¯·åœ¨1åˆ†é’Ÿå†…ç»™å‡ºï¼š
1. æ ¸å¿ƒè§‚ç‚¹ï¼ˆä¸€å¥è¯ï¼‰
2. çœ‹å¤šå› ç´ ï¼ˆ3ç‚¹ï¼‰
3. çœ‹ç©ºå› ç´ ï¼ˆ3ç‚¹ï¼‰
4. æŠ•èµ„å»ºè®®ï¼ˆä¹°å…¥/æŒæœ‰/å–å‡ºï¼‰
5. ç›®æ ‡ä»·ä½å’Œæ­¢æŸä»·ä½

è¯·ç”¨ç®€æ´çš„è¯­è¨€ï¼Œç›´æ¥ç»™å‡ºç»“è®ºã€‚"""

        try:
            response = self._llm_provider.invoke([
                {"role": "system", "content": "ä½ æ˜¯å¿«é€Ÿåˆ†æå¸ˆï¼Œæ“…é•¿å¿«é€Ÿç»™å‡ºæŠ•èµ„å»ºè®®ã€‚"},
                {"role": "user", "content": prompt}
            ])
            content = response.content if hasattr(response, 'content') else str(response)
            return {
                "success": True,
                "analysis": content,
                "timestamp": datetime.utcnow().isoformat()
            }
        except Exception as e:
            logger.error(f"Quick analysis failed: {e}")
            return {
                "success": False,
                "error": str(e)
            }


# å·¥å‚å‡½æ•°
def create_data_collector(llm_provider=None) -> DataCollectorAgentV2:
    """åˆ›å»ºæ•°æ®ä¸“å‘˜å®ä¾‹"""
    return DataCollectorAgentV2(llm_provider=llm_provider)

