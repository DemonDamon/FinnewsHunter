"""
æœç´¢åˆ†æå¸ˆæ™ºèƒ½ä½“ (SearchAnalystAgent)

è´Ÿè´£åœ¨è¾©è®ºè¿‡ç¨‹ä¸­åŠ¨æ€æœé›†æ•°æ®ï¼Œæ”¯æŒå¤šç§æ•°æ®æºï¼š
- AkShare: è´¢åŠ¡æŒ‡æ ‡ã€Kçº¿æ•°æ®ã€èµ„é‡‘æµå‘ã€æœºæ„æŒä»“
- BochaAI: å®æ—¶æ–°é—»æœç´¢ã€åˆ†æå¸ˆæŠ¥å‘Š
- InteractiveCrawler: å¤šå¼•æ“ç½‘é¡µæœç´¢ (ç™¾åº¦ã€æœç‹—ã€360ç­‰)
- Knowledge Base: å†å²æ–°é—»å’Œä¸Šä¸‹æ–‡ (å‘é‡æ•°æ®åº“)
"""
import logging
import re
import asyncio
from typing import Dict, Any, List, Optional, ClassVar, Pattern
from datetime import datetime
from enum import Enum

from agenticx.core.agent import Agent
from ..services.llm_service import get_llm_provider
from ..services.stock_data_service import stock_data_service
from ..tools.bochaai_search import bochaai_search, SearchResult
from ..tools.interactive_crawler import InteractiveCrawler

logger = logging.getLogger(__name__)


class SearchSource(Enum):
    """æœç´¢æ•°æ®æºç±»å‹"""
    AKSHARE = "akshare"           # AkShare è´¢åŠ¡/è¡Œæƒ…æ•°æ®
    BOCHAAI = "bochaai"           # BochaAI Webæœç´¢
    BROWSER = "browser"           # äº¤äº’å¼æµè§ˆå™¨æœç´¢
    KNOWLEDGE_BASE = "kb"         # å†…éƒ¨çŸ¥è¯†åº“
    ALL = "all"                   # æ‰€æœ‰æ¥æº


class SearchAnalystAgent(Agent):
    """
    æœç´¢åˆ†æå¸ˆæ™ºèƒ½ä½“
    
    åœ¨è¾©è®ºè¿‡ç¨‹ä¸­è¢«å…¶ä»–æ™ºèƒ½ä½“è°ƒç”¨ï¼ŒåŠ¨æ€è·å–æ‰€éœ€æ•°æ®ã€‚
    æ”¯æŒè§£æç»“æ„åŒ–æœç´¢è¯·æ±‚ï¼Œå¹¶è¿”å›æ ¼å¼åŒ–çš„æ•°æ®ã€‚
    """
    
    # æœç´¢è¯·æ±‚çš„æ­£åˆ™æ¨¡å¼ [SEARCH: "query" source:xxx]
    # ä½¿ç”¨ ClassVar é¿å… Pydantic å°†å…¶è§†ä¸ºæ¨¡å‹å­—æ®µ
    SEARCH_PATTERN: ClassVar[Pattern] = re.compile(
        r'\[SEARCH:\s*["\']([^"\']+)["\']\s*(?:source:(\w+))?\]',
        re.IGNORECASE
    )
    
    def __init__(self, llm_provider=None, organization_id: str = "finnews"):
        super().__init__(
            name="SearchAnalyst",
            role="æœç´¢åˆ†æå¸ˆ",
            goal="æ ¹æ®è¾©è®ºä¸­çš„æ•°æ®éœ€æ±‚ï¼Œå¿«é€Ÿä»å¤šä¸ªæ•°æ®æºè·å–ç›¸å…³ä¿¡æ¯",
            backstory="""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èæ•°æ®æœç´¢ä¸“å®¶ï¼Œç²¾é€šå„ç±»é‡‘èæ•°æ®æºçš„ä½¿ç”¨ã€‚
ä½ çš„èŒè´£æ˜¯ï¼š
1. è§£æè¾©è®ºæ™ºèƒ½ä½“çš„æ•°æ®è¯·æ±‚
2. é€‰æ‹©æœ€åˆé€‚çš„æ•°æ®æºè¿›è¡ŒæŸ¥è¯¢
3. æ•´ç†å¹¶æ ¼å¼åŒ–æ•°æ®ï¼Œä½¿å…¶ä¾¿äºè¾©è®ºä½¿ç”¨
4. å¯¹æ•°æ®è´¨é‡è¿›è¡Œåˆæ­¥è¯„ä¼°

ä½ èƒ½å¤Ÿè®¿é—®çš„æ•°æ®æºåŒ…æ‹¬ï¼š
- AkShare: è‚¡ç¥¨è´¢åŠ¡æŒ‡æ ‡ã€Kçº¿è¡Œæƒ…ã€èµ„é‡‘æµå‘ã€æœºæ„æŒä»“ç­‰
- BochaAI: å®æ—¶æ–°é—»æœç´¢ã€è´¢ç»æŠ¥é“
- å¤šå¼•æ“æœç´¢: ç™¾åº¦èµ„è®¯ã€æœç‹—ã€360ç­‰
- å†…éƒ¨çŸ¥è¯†åº“: å†å²æ–°é—»å’Œåˆ†ææ•°æ®""",
            organization_id=organization_id
        )
        
        if llm_provider is None:
            llm_provider = get_llm_provider()
        object.__setattr__(self, '_llm_provider', llm_provider)
        
        # åˆå§‹åŒ–æœç´¢å·¥å…·
        self._interactive_crawler = InteractiveCrawler(timeout=20)
        
        logger.info(f"âœ… Initialized {self.name} agent with multi-source search capabilities")
    
    def extract_search_requests(self, text: str) -> List[Dict[str, Any]]:
        """
        ä»æ–‡æœ¬ä¸­æå–æœç´¢è¯·æ±‚
        
        æ”¯æŒæ ¼å¼:
        - [SEARCH: "query"]
        - [SEARCH: "query" source:akshare]
        - [SEARCH: "query" source:bochaai]
        - [SEARCH: "query" source:browser]
        
        Args:
            text: åŒ…å«æœç´¢è¯·æ±‚çš„æ–‡æœ¬
            
        Returns:
            æœç´¢è¯·æ±‚åˆ—è¡¨ [{"query": "...", "source": "..."}]
        """
        requests = []
        matches = self.SEARCH_PATTERN.findall(text)
        
        for match in matches:
            query = match[0].strip()
            source = match[1].lower() if match[1] else "all"
            
            # éªŒè¯ source
            valid_sources = [s.value for s in SearchSource]
            if source not in valid_sources:
                source = "all"
            
            requests.append({
                "query": query,
                "source": source
            })
            logger.info(f"ğŸ” æå–æœç´¢è¯·æ±‚: query='{query}', source={source}")
        
        return requests
    
    async def search(
        self,
        query: str,
        source: str = "all",
        stock_code: Optional[str] = None,
        stock_name: Optional[str] = None,
        context: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæœç´¢è¯·æ±‚
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            source: æ•°æ®æº (akshare, bochaai, browser, kb, all)
            stock_code: è‚¡ç¥¨ä»£ç  (ç”¨äº akshare æŸ¥è¯¢)
            stock_name: è‚¡ç¥¨åç§° (ç”¨äºæ–°é—»æœç´¢)
            context: é¢å¤–ä¸Šä¸‹æ–‡
            
        Returns:
            æœç´¢ç»“æœå­—å…¸
        """
        logger.info(f"ğŸ” SearchAnalyst: æ‰§è¡Œæœç´¢ query='{query}', source={source}")
        
        result = {
            "query": query,
            "source": source,
            "timestamp": datetime.utcnow().isoformat(),
            "data": {},
            "summary": "",
            "success": False
        }
        
        try:
            if source == SearchSource.AKSHARE.value or source == SearchSource.ALL.value:
                akshare_data = await self._search_akshare(query, stock_code)
                if akshare_data:
                    result["data"]["akshare"] = akshare_data
            
            if source == SearchSource.BOCHAAI.value or source == SearchSource.ALL.value:
                bochaai_data = await self._search_bochaai(query, stock_name)
                if bochaai_data:
                    result["data"]["bochaai"] = bochaai_data
            
            if source == SearchSource.BROWSER.value or source == SearchSource.ALL.value:
                browser_data = await self._search_browser(query)
                if browser_data:
                    result["data"]["browser"] = browser_data
            
            if source == SearchSource.KNOWLEDGE_BASE.value or source == SearchSource.ALL.value:
                kb_data = await self._search_knowledge_base(query, stock_code, stock_name)
                if kb_data:
                    result["data"]["knowledge_base"] = kb_data
            
            # ç”Ÿæˆæ‘˜è¦
            if result["data"]:
                result["summary"] = await self._generate_summary(query, result["data"])
                result["success"] = True
            else:
                result["summary"] = f"æœªæ‰¾åˆ°ä¸'{query}'ç›¸å…³çš„æ•°æ®"
            
        except Exception as e:
            logger.error(f"SearchAnalyst æœç´¢å¤±è´¥: {e}", exc_info=True)
            result["error"] = str(e)
        
        return result
    
    async def _search_akshare(
        self,
        query: str,
        stock_code: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """ä» AkShare è·å–æ•°æ®"""
        if not stock_code:
            logger.debug("AkShare æœç´¢éœ€è¦è‚¡ç¥¨ä»£ç ï¼Œè·³è¿‡")
            return None
        
        data = {}
        query_lower = query.lower()
        
        try:
            # æ ¹æ®æŸ¥è¯¢å†…å®¹å†³å®šè·å–å“ªäº›æ•°æ®
            if any(kw in query_lower for kw in ["è´¢åŠ¡", "pe", "pb", "roe", "åˆ©æ¶¦", "ä¼°å€¼", "å¸‚ç›ˆ", "å¸‚å‡€"]):
                financial = await stock_data_service.get_financial_indicators(stock_code)
                if financial:
                    data["financial_indicators"] = financial
            
            if any(kw in query_lower for kw in ["èµ„é‡‘", "ä¸»åŠ›", "æµå…¥", "æµå‡º", "æ•£æˆ·", "æœºæ„"]):
                fund_flow = await stock_data_service.get_fund_flow(stock_code, days=10)
                if fund_flow:
                    data["fund_flow"] = fund_flow
            
            if any(kw in query_lower for kw in ["è¡Œæƒ…", "ä»·æ ¼", "æ¶¨è·Œ", "æˆäº¤", "é‡"]):
                realtime = await stock_data_service.get_realtime_quote(stock_code)
                if realtime:
                    data["realtime_quote"] = realtime
            
            if any(kw in query_lower for kw in ["kçº¿", "èµ°åŠ¿", "å†å²", "å‡çº¿", "è¶‹åŠ¿"]):
                kline = await stock_data_service.get_kline_data(stock_code, period="daily", limit=30)
                if kline:
                    # åªè¿”å›æœ€è¿‘10å¤©çš„ç®€è¦æ•°æ®
                    data["kline_summary"] = {
                        "period": "daily",
                        "count": len(kline),
                        "latest": kline[-1] if kline else None,
                        "recent_5": kline[-5:] if len(kline) >= 5 else kline
                    }
            
            # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ç‰¹å®šæŸ¥è¯¢ï¼Œè·å–ç»¼åˆæ•°æ®
            if not data:
                context_data = await stock_data_service.get_debate_context(stock_code)
                if context_data:
                    data = context_data
            
            if data:
                logger.info(f"âœ… AkShare è¿”å›æ•°æ®: {list(data.keys())}")
                return data
            
        except Exception as e:
            logger.warning(f"AkShare æœç´¢å‡ºé”™: {e}")
        
        return None
    
    async def _search_bochaai(
        self,
        query: str,
        stock_name: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """ä» BochaAI æœç´¢æ–°é—»"""
        if not bochaai_search.is_available():
            logger.debug("BochaAI æœªé…ç½®ï¼Œè·³è¿‡")
            return None
        
        try:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_query = query
            if stock_name and stock_name not in query:
                search_query = f"{stock_name} {query}"
            
            results = bochaai_search.search(
                query=search_query,
                freshness="oneWeek",
                count=10
            )
            
            if results:
                news_list = [
                    {
                        "title": r.title,
                        "snippet": r.snippet[:200] if r.snippet else "",
                        "url": r.url,
                        "source": r.site_name or "unknown",
                        "date": r.date_published or ""
                    }
                    for r in results
                ]
                logger.info(f"âœ… BochaAI è¿”å› {len(news_list)} æ¡æ–°é—»")
                return {"news": news_list, "count": len(news_list)}
        
        except Exception as e:
            logger.warning(f"BochaAI æœç´¢å‡ºé”™: {e}")
        
        return None
    
    async def _search_browser(self, query: str) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨äº¤äº’å¼çˆ¬è™«æœç´¢"""
        try:
            loop = asyncio.get_event_loop()
            results = await loop.run_in_executor(
                None,
                lambda: self._interactive_crawler.interactive_search(
                    query=query,
                    engines=["baidu_news", "sogou"],
                    num_results=10,
                    search_type="news"
                )
            )
            
            if results:
                news_list = [
                    {
                        "title": r.get("title", ""),
                        "snippet": r.get("snippet", "")[:200],
                        "url": r.get("url", ""),
                        "source": "browser_search"
                    }
                    for r in results
                ]
                logger.info(f"âœ… Browser è¿”å› {len(news_list)} æ¡ç»“æœ")
                return {"search_results": news_list, "count": len(news_list)}
        
        except Exception as e:
            logger.warning(f"Browser æœç´¢å‡ºé”™: {e}")
        
        return None
    
    async def _search_knowledge_base(
        self,
        query: str,
        stock_code: Optional[str] = None,
        stock_name: Optional[str] = None
    ) -> Optional[Dict[str, Any]]:
        """ä»çŸ¥è¯†åº“æœç´¢å†å²æ•°æ®"""
        try:
            # å°è¯•å¯¼å…¥ news_serviceï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰
            try:
                from ..services.news_service import news_service
            except ImportError:
                logger.debug("news_service æœªé…ç½®ï¼Œè·³è¿‡çŸ¥è¯†åº“æœç´¢")
                return None
            
            # å°è¯•ä»æ•°æ®åº“è·å–ç›¸å…³æ–°é—»
            if stock_code and news_service:
                news_list = await news_service.get_news_by_stock(stock_code, limit=10)
                if news_list:
                    kb_news = [
                        {
                            "title": getattr(news, 'title', ''),
                            "content": (getattr(news, 'content', '') or '')[:300],
                            "source": getattr(news, 'source', ''),
                            "date": news.published_at.isoformat() if hasattr(news, 'published_at') and news.published_at else ""
                        }
                        for news in news_list
                    ]
                    logger.info(f"âœ… KB è¿”å› {len(kb_news)} æ¡å†å²æ–°é—»")
                    return {"historical_news": kb_news, "count": len(kb_news)}
        
        except Exception as e:
            logger.debug(f"KB æœç´¢å‡ºé”™: {e}")
        
        return None
    
    async def _generate_summary(self, query: str, data: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ•°æ®æ‘˜è¦"""
        summary_parts = [f"## æœç´¢ç»“æœ: {query}\n"]
        
        # AkShare æ•°æ®æ‘˜è¦
        if "akshare" in data:
            ak_data = data["akshare"]
            summary_parts.append("### ğŸ“Š è´¢åŠ¡/è¡Œæƒ…æ•°æ® (AkShare)\n")
            
            if "financial_indicators" in ak_data:
                fi = ak_data["financial_indicators"]
                summary_parts.append(f"- PE: {fi.get('pe_ratio', 'N/A')}, PB: {fi.get('pb_ratio', 'N/A')}")
                summary_parts.append(f"- ROE: {fi.get('roe', 'N/A')}%, å‡€åˆ©æ¶¦åŒæ¯”: {fi.get('profit_yoy', 'N/A')}%")
            
            if "realtime_quote" in ak_data:
                rt = ak_data["realtime_quote"]
                summary_parts.append(f"- å½“å‰ä»·: {rt.get('price', 'N/A')}å…ƒ, æ¶¨è·Œå¹…: {rt.get('change_percent', 'N/A')}%")
            
            if "fund_flow" in ak_data:
                ff = ak_data["fund_flow"]
                main_net = ff.get('total_main_net', 0)
                trend = ff.get('main_flow_trend', 'N/A')
                summary_parts.append(f"- èµ„é‡‘æµå‘: è¿‘{ff.get('period_days', 5)}æ—¥ä¸»åŠ›{trend}")
            
            summary_parts.append("")
        
        # BochaAI æ–°é—»æ‘˜è¦
        if "bochaai" in data:
            news = data["bochaai"].get("news", [])
            if news:
                summary_parts.append("### ğŸ“° æœ€æ–°æ–°é—» (BochaAI)\n")
                for i, n in enumerate(news[:5], 1):
                    summary_parts.append(f"{i}. **{n['title'][:50]}**")
                    if n.get('snippet'):
                        summary_parts.append(f"   {n['snippet'][:100]}...")
                summary_parts.append("")
        
        # Browser æœç´¢ç»“æœæ‘˜è¦
        if "browser" in data:
            results = data["browser"].get("search_results", [])
            if results:
                summary_parts.append("### ğŸŒ ç½‘é¡µæœç´¢ç»“æœ\n")
                for i, r in enumerate(results[:5], 1):
                    summary_parts.append(f"{i}. {r['title'][:50]}")
                summary_parts.append("")
        
        # KB å†å²æ•°æ®æ‘˜è¦
        if "knowledge_base" in data:
            kb_news = data["knowledge_base"].get("historical_news", [])
            if kb_news:
                summary_parts.append("### ğŸ“š å†å²èµ„æ–™ (çŸ¥è¯†åº“)\n")
                for i, n in enumerate(kb_news[:3], 1):
                    summary_parts.append(f"{i}. {n['title'][:50]}")
                summary_parts.append("")
        
        return "\n".join(summary_parts)
    
    async def process_debate_speech(
        self,
        speech_text: str,
        stock_code: str,
        stock_name: str,
        agent_name: str = "Unknown"
    ) -> Dict[str, Any]:
        """
        å¤„ç†è¾©è®ºå‘è¨€ä¸­çš„æœç´¢è¯·æ±‚
        
        Args:
            speech_text: è¾©è®ºå‘è¨€æ–‡æœ¬
            stock_code: è‚¡ç¥¨ä»£ç 
            stock_name: è‚¡ç¥¨åç§°
            agent_name: å‘è¨€æ™ºèƒ½ä½“åç§°
            
        Returns:
            å¤„ç†ç»“æœï¼ŒåŒ…å«æ‰€æœ‰æœç´¢ç»“æœå’Œç»¼åˆæ‘˜è¦
        """
        logger.info(f"ğŸ” SearchAnalyst: å¤„ç† {agent_name} çš„å‘è¨€ï¼Œæ£€æµ‹æœç´¢è¯·æ±‚...")
        
        result = {
            "agent_name": agent_name,
            "requests_found": 0,
            "search_results": [],
            "combined_summary": "",
            "success": False
        }
        
        # æå–æœç´¢è¯·æ±‚
        requests = self.extract_search_requests(speech_text)
        result["requests_found"] = len(requests)
        
        if not requests:
            logger.info(f"ğŸ“ {agent_name} çš„å‘è¨€ä¸­æœªåŒ…å«æœç´¢è¯·æ±‚")
            result["success"] = True
            return result
        
        logger.info(f"ğŸ“‹ ä» {agent_name} çš„å‘è¨€ä¸­æå–åˆ° {len(requests)} ä¸ªæœç´¢è¯·æ±‚")
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æœç´¢
        search_tasks = []
        for req in requests:
            task = self.search(
                query=req["query"],
                source=req["source"],
                stock_code=stock_code,
                stock_name=stock_name
            )
            search_tasks.append(task)
        
        search_results = await asyncio.gather(*search_tasks, return_exceptions=True)
        
        # æ”¶é›†ç»“æœ
        summaries = []
        for i, res in enumerate(search_results):
            if isinstance(res, Exception):
                logger.error(f"æœç´¢è¯·æ±‚ {i+1} å¤±è´¥: {res}")
                continue
            
            if res.get("success"):
                result["search_results"].append(res)
                summaries.append(res.get("summary", ""))
        
        # ç”Ÿæˆç»¼åˆæ‘˜è¦
        if summaries:
            result["combined_summary"] = "\n---\n".join(summaries)
            result["success"] = True
        
        logger.info(f"âœ… SearchAnalyst: ä¸º {agent_name} å®Œæˆ {len(result['search_results'])} ä¸ªæœç´¢è¯·æ±‚")
        
        return result
    
    async def smart_data_supplement(
        self,
        stock_code: str,
        stock_name: str,
        existing_context: str,
        debate_history: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        æ™ºèƒ½æ•°æ®è¡¥å……
        
        åˆ†æè¾©è®ºå†å²å’Œç°æœ‰ä¸Šä¸‹æ–‡ï¼Œä¸»åŠ¨è¯†åˆ«ç¼ºå¤±çš„å…³é”®æ•°æ®å¹¶è¡¥å……
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            stock_name: è‚¡ç¥¨åç§°
            existing_context: ç°æœ‰ä¸Šä¸‹æ–‡
            debate_history: è¾©è®ºå†å²
            
        Returns:
            è¡¥å……çš„æ•°æ®å’Œæ‘˜è¦
        """
        logger.info(f"ğŸ§  SearchAnalyst: æ™ºèƒ½åˆ†ææ•°æ®ç¼ºå£...")
        
        # ä½¿ç”¨ LLM åˆ†æéœ€è¦ä»€ä¹ˆæ•°æ®
        analysis_prompt = f"""ä½ æ˜¯ä¸€ä½é‡‘èæ•°æ®åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹è¾©è®ºæƒ…å†µï¼Œåˆ¤æ–­è¿˜éœ€è¦å“ªäº›æ•°æ®æ”¯æ’‘ï¼š

ã€è‚¡ç¥¨ã€‘{stock_name} ({stock_code})

ã€ç°æœ‰æ•°æ®ã€‘
{existing_context[:1500]}

ã€è¾©è®ºå†å²ã€‘
{self._format_debate_history(debate_history[-4:])}

è¯·åˆ¤æ–­ï¼š
1. çœ‹å¤šæ–¹ç¼ºå°‘ä»€ä¹ˆå…³é”®æ•°æ®ï¼Ÿ
2. çœ‹ç©ºæ–¹ç¼ºå°‘ä»€ä¹ˆå…³é”®æ•°æ®ï¼Ÿ
3. è¿˜éœ€è¦æœç´¢ä»€ä¹ˆä¿¡æ¯ï¼Ÿ

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºéœ€è¦æœç´¢çš„å†…å®¹ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰ï¼š
[SEARCH: "æœç´¢å†…å®¹" source:æ•°æ®æº]

å¯ç”¨æ•°æ®æºï¼šakshareï¼ˆè´¢åŠ¡/è¡Œæƒ…ï¼‰, bochaaiï¼ˆæ–°é—»ï¼‰, browserï¼ˆç½‘é¡µæœç´¢ï¼‰

åªè¾“å‡º3-5ä¸ªæœ€å…³é”®çš„æœç´¢è¯·æ±‚ã€‚"""

        try:
            response = self._llm_provider.invoke([
                {"role": "system", "content": f"ä½ æ˜¯{self.role}ï¼Œ{self.backstory}"},
                {"role": "user", "content": analysis_prompt}
            ])
            
            llm_response = response.content if hasattr(response, 'content') else str(response)
            
            # å¤„ç† LLM å»ºè®®çš„æœç´¢
            return await self.process_debate_speech(
                speech_text=llm_response,
                stock_code=stock_code,
                stock_name=stock_name,
                agent_name="SmartSupplement"
            )
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½æ•°æ®è¡¥å……å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def _format_debate_history(self, history: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–è¾©è®ºå†å²"""
        if not history:
            return "ï¼ˆæš‚æ— è¾©è®ºå†å²ï¼‰"
        
        lines = []
        for item in history:
            agent = item.get("agent", "Unknown")
            content = item.get("content", "")[:300]
            lines.append(f"[{agent}]: {content}")
        
        return "\n\n".join(lines)


# å·¥å‚å‡½æ•°
def create_search_analyst(llm_provider=None) -> SearchAnalystAgent:
    """åˆ›å»ºæœç´¢åˆ†æå¸ˆå®ä¾‹"""
    return SearchAnalystAgent(llm_provider=llm_provider)

