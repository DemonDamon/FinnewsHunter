"""
äº¤äº’å¼ç½‘é¡µçˆ¬è™«
ä½¿ç”¨ requests + BeautifulSoup è¿›è¡Œç½‘é¡µçˆ¬å–
ç‰¹åˆ«ç”¨äºæœç´¢ç»“æœè¡¥å……ï¼Œå½“ BochaAI ç»“æœä¸è¶³æ—¶ä½¿ç”¨

æ³¨æ„ï¼šä¸»è¦æœç´¢å¼•æ“ï¼ˆBingã€ç™¾åº¦ï¼‰éƒ½æœ‰åçˆ¬æœºåˆ¶ï¼Œæœ¬æ¨¡å—å·²åšç›¸åº”ä¼˜åŒ–ï¼š
1. æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¯·æ±‚å¤´
2. æ£€æµ‹éªŒè¯é¡µé¢å¹¶è‡ªåŠ¨é™çº§
3. å¤šå¼•æ“è½®æ¢å¤‡é€‰
"""
import logging
import re
import time
import random
from typing import List, Dict, Any, Optional
from urllib.parse import quote_plus, urljoin, urlparse

import requests
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

# æ›´å®Œå–„çš„ User-Agentï¼Œæ¨¡æ‹Ÿæœ€æ–°çš„ Chrome æµè§ˆå™¨
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
]

# éªŒè¯é¡µé¢å…³é”®è¯ï¼ˆç”¨äºæ£€æµ‹è¢«æ‹¦æˆªï¼‰
CAPTCHA_KEYWORDS = [
    'ç¡®è®¤æ‚¨æ˜¯çœŸäºº', 'äººæœºéªŒè¯', 'captcha', 'verify you are human',
    'éªŒè¯ç ', 'è¯·å®ŒæˆéªŒè¯', 'å®‰å…¨éªŒè¯', 'å¼‚å¸¸è®¿é—®', 'è¯·è¾“å…¥éªŒè¯ç ',
    'æœ€åä¸€æ­¥', 'è¯·è§£å†³ä»¥ä¸‹éš¾é¢˜'
]


class InteractiveCrawler:
    """äº¤äº’å¼ç½‘é¡µçˆ¬è™«ï¼ˆçº¯ requests å®ç°ï¼‰"""
    
    def __init__(self, timeout: int = 15):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.timeout = timeout
        self.session = requests.Session()
        self._user_agent = random.choice(USER_AGENTS)
        # æ›´å®Œå–„çš„è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': self._user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'sec-ch-ua': '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"',
        })
    
    def _is_captcha_page(self, html_content: str, soup: BeautifulSoup = None) -> bool:
        """
        æ£€æµ‹é¡µé¢æ˜¯å¦ä¸ºéªŒè¯ç /äººæœºéªŒè¯é¡µé¢
        
        Args:
            html_content: HTML åŸå§‹å†…å®¹
            soup: å·²è§£æçš„ BeautifulSoup å¯¹è±¡
            
        Returns:
            True å¦‚æœæ˜¯éªŒè¯é¡µé¢
        """
        text_to_check = html_content.lower()
        if soup:
            text_to_check = soup.get_text().lower()
        
        for keyword in CAPTCHA_KEYWORDS:
            if keyword.lower() in text_to_check:
                return True
        return False
    
    def search_on_bing(
        self,
        query: str,
        num_results: int = 10
    ) -> List[Dict[str, str]]:
        """
        åœ¨ Bing ä¸Šæœç´¢å¹¶è·å–ç»“æœ
        
        Args:
            query: æœç´¢å…³é”®è¯
            num_results: è·å–çš„ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨ [{"url": "...", "title": "...", "snippet": "..."}]
        """
        results = []
        
        try:
            # ä½¿ç”¨å›½é™…ç‰ˆ Bingï¼Œä¸­å›½ç‰ˆæœ‰æ›´ä¸¥æ ¼çš„åçˆ¬
            search_url = f"https://www.bing.com/search?q={quote_plus(query)}&count={num_results}"
            
            logger.info(f"ğŸ” Bing æœç´¢: {query}")
            logger.debug(f"æœç´¢URL: {search_url}")
            
            response = self.session.get(search_url, timeout=self.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ========== æ£€æµ‹éªŒè¯ç é¡µé¢ ==========
            if self._is_captcha_page(response.text, soup):
                logger.warning("âš ï¸ Bing è§¦å‘äººæœºéªŒè¯ï¼Œè·³è¿‡æ­¤å¼•æ“")
                return []  # è¿”å›ç©ºï¼Œè®©è°ƒç”¨è€…ä½¿ç”¨å…¶ä»–å¼•æ“
            
            # ========== è°ƒè¯•ï¼šæ‰“å°æ‰¾åˆ°çš„å…ƒç´  ==========
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            b_algo_items = soup.select('.b_algo')
            logger.info(f"ğŸ“Š Bing HTMLè§£æ: .b_algo={len(b_algo_items)}ä¸ª")
            
            # å¦‚æœ .b_algo æ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨
            if not b_algo_items:
                # å°è¯•æŸ¥æ‰¾æ‰€æœ‰åŒ…å«é“¾æ¥çš„ li å…ƒç´ 
                li_items = soup.select('#b_results > li')
                logger.info(f"ğŸ“Š å°è¯• #b_results > li: {len(li_items)}ä¸ª")
                
                # æ‰“å°é¡µé¢ä¸­æ‰€æœ‰é“¾æ¥ä¾›è°ƒè¯•
                all_links = soup.select('a[href^="http"]')
                logger.info(f"ğŸ“Š é¡µé¢æ€»é“¾æ¥æ•°: {len(all_links)}ä¸ª")
                
                # æ‰“å°å‰10ä¸ªé“¾æ¥
                for i, link in enumerate(all_links[:15]):
                    href = link.get('href', '')
                    text = link.get_text(strip=True)[:50]
                    # è¿‡æ»¤æ‰ Bing å†…éƒ¨é“¾æ¥
                    if 'bing.com' not in href and 'microsoft.com' not in href:
                        logger.info(f"  é“¾æ¥{i+1}: {text} -> {href[:80]}")
            
            # ========== æå–æœç´¢ç»“æœ ==========
            # æ–¹æ³•1: æ ‡å‡† .b_algo é€‰æ‹©å™¨
            for result in b_algo_items[:num_results]:
                try:
                    # æå–æ ‡é¢˜å’Œé“¾æ¥
                    title_elem = result.select_one('h2 a')
                    if not title_elem:
                        title_elem = result.select_one('a')  # å¤‡é€‰
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    url = title_elem.get('href', '')
                    
                    # æå–æ‘˜è¦
                    snippet_elem = result.select_one('.b_caption p, p')
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''
                    
                    if url and title and 'bing.com' not in url:
                        results.append({
                            "url": url,
                            "title": title,
                            "snippet": snippet[:300],
                            "source": "bing"
                        })
                        logger.debug(f"  âœ… æå–: {title[:40]} -> {url[:60]}")
                        
                except Exception as e:
                    logger.debug(f"è§£æ Bing ç»“æœå¤±è´¥: {e}")
                    continue
            
            # æ–¹æ³•2: å¦‚æœ .b_algo æ²¡æœ‰ç»“æœï¼Œå¯èƒ½æ˜¯éªŒè¯é¡µé¢çš„æ®‹ç•™é“¾æ¥ï¼Œä¸å†ä½¿ç”¨å¤‡é€‰æå–
            if not results and b_algo_items:
                logger.info("âš ï¸ Bing æ— æœ‰æ•ˆç»“æœ")
            
            logger.info(f"âœ… Bing æœç´¢å®Œæˆï¼Œè·å¾— {len(results)} æ¡ç»“æœ")
            
        except requests.exceptions.Timeout:
            logger.warning(f"âš ï¸ Bing æœç´¢è¶…æ—¶: {query}")
        except requests.exceptions.RequestException as e:
            logger.warning(f"âš ï¸ Bing æœç´¢è¯·æ±‚å¤±è´¥: {e}")
        except Exception as e:
            logger.error(f"âŒ Bing æœç´¢å¤±è´¥: {e}")
        
        return results
    
    def search_on_baidu(
        self,
        query: str,
        num_results: int = 10
    ) -> List[Dict[str, str]]:
        """
        åœ¨ç™¾åº¦ä¸Šæœç´¢å¹¶è·å–ç»“æœï¼ˆç™¾åº¦å¯¹ç®€å•çˆ¬è™«ç›¸å¯¹å‹å¥½ï¼‰
        
        Args:
            query: æœç´¢å…³é”®è¯
            num_results: è·å–çš„ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            # ç™¾åº¦æœç´¢ URL
            search_url = f"https://www.baidu.com/s?wd={quote_plus(query)}&rn={num_results}"
            
            logger.info(f"ğŸ” ç™¾åº¦æœç´¢: {query}")
            logger.debug(f"æœç´¢URL: {search_url}")
            
            # ç™¾åº¦éœ€è¦ç‰¹å®šçš„è¯·æ±‚å¤´
            headers = {
                'User-Agent': self._user_agent,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9',
                'Accept-Encoding': 'gzip, deflate',
                'Referer': 'https://www.baidu.com/',
                'Connection': 'keep-alive',
            }
            
            response = self.session.get(search_url, headers=headers, timeout=self.timeout)
            response.encoding = 'utf-8'
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ£€æµ‹éªŒè¯ç 
            if self._is_captcha_page(response.text, soup):
                logger.warning("âš ï¸ ç™¾åº¦è§¦å‘éªŒè¯ï¼Œè·³è¿‡æ­¤å¼•æ“")
                return []
            
            # ç™¾åº¦æœç´¢ç»“æœé€‰æ‹©å™¨ï¼ˆå¤šç§å°è¯•ï¼‰
            result_items = soup.select('.result.c-container, .c-container, div[class*="result"]')
            logger.info(f"ğŸ“Š ç™¾åº¦HTMLè§£æ: ç»“æœå®¹å™¨={len(result_items)}ä¸ª")
            
            for result in result_items[:num_results]:
                try:
                    # æå–æ ‡é¢˜å’Œé“¾æ¥
                    title_elem = result.select_one('h3 a, .t a, a[href]')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    url = title_elem.get('href', '')
                    
                    # ç™¾åº¦ä½¿ç”¨è·³è½¬é“¾æ¥ï¼Œéœ€è¦æå–çœŸå®URL
                    # ä½†é€šå¸¸è·³è½¬é“¾æ¥ä¹Ÿèƒ½ç”¨
                    
                    # æå–æ‘˜è¦
                    snippet_elem = result.select_one('.c-abstract, .c-span-last, .content-right_8Zs40')
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''
                    
                    if url and title and 'baidu.com' not in url:
                        results.append({
                            "url": url,
                            "title": title,
                            "snippet": snippet[:300],
                            "source": "baidu"
                        })
                        logger.debug(f"  âœ… æå–: {title[:40]}")
                        
                except Exception as e:
                    logger.debug(f"è§£æç™¾åº¦ç»“æœå¤±è´¥: {e}")
                    continue
            
            # å¤‡é€‰æ–¹æ³•ï¼šä»æ‰€æœ‰æ ‡é¢˜é“¾æ¥æå–
            if not results:
                logger.info("âš ï¸ ç™¾åº¦æ ‡å‡†é€‰æ‹©å™¨æ— ç»“æœï¼Œå°è¯•æå– h3 é“¾æ¥...")
                h3_links = soup.select('h3 a')
                for link in h3_links[:num_results]:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    
                    if not href or not text or len(text) < 3:
                        continue
                    if href in [r['url'] for r in results]:
                        continue
                    
                    results.append({
                        "url": href,
                        "title": text[:100],
                        "snippet": "",
                        "source": "baidu"
                    })
                    
                    if len(results) >= num_results:
                        break
            
            logger.info(f"âœ… ç™¾åº¦æœç´¢å®Œæˆï¼Œè·å¾— {len(results)} æ¡ç»“æœ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç™¾åº¦æœç´¢å¤±è´¥: {e}")
        
        return results
    
    def search_on_baidu_news(
        self,
        query: str,
        num_results: int = 10
    ) -> List[Dict[str, str]]:
        """
        åœ¨ç™¾åº¦æ–°é—»æœç´¢ï¼ˆnews.baidu.comï¼‰è·å–æ–°é—»ç»“æœ
        
        ä½¿ç”¨ news.baidu.com å…¥å£ï¼Œè¿”å›çš„ URL æ˜¯çœŸå®çš„ç¬¬ä¸‰æ–¹æ–°é—»é“¾æ¥ï¼Œ
        ä¸æ˜¯ç™¾åº¦è·³è½¬é“¾æ¥ï¼Œé¿å…ä¹±ç é—®é¢˜ã€‚
        
        Args:
            query: æœç´¢å…³é”®è¯
            num_results: è·å–çš„ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            # ä½¿ç”¨ç™¾åº¦æ–°é—»å…¥å£ï¼ˆnews.baidu.comï¼‰ï¼Œè¿”å›çœŸå®çš„ç¬¬ä¸‰æ–¹ URL
            search_url = f"https://news.baidu.com/ns?word={quote_plus(query)}&tn=news&from=news&cl=2&rn={num_results}&ct=1"
            
            logger.info(f"ğŸ” ç™¾åº¦æ–°é—»æœç´¢: {query}")
            logger.debug(f"æœç´¢URL: {search_url}")
            
            # ç™¾åº¦éœ€è¦ç‰¹å®šçš„è¯·æ±‚å¤´
            headers = {
                'User-Agent': self._user_agent,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9',
                'Accept-Encoding': 'gzip, deflate',
                'Referer': 'https://news.baidu.com/',
                'Connection': 'keep-alive',
            }
            
            response = self.session.get(search_url, headers=headers, timeout=self.timeout, allow_redirects=True)
            response.encoding = 'utf-8'
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ£€æµ‹éªŒè¯ç 
            if self._is_captcha_page(response.text, soup):
                logger.warning("âš ï¸ ç™¾åº¦æ–°é—»è§¦å‘éªŒè¯ï¼Œè·³è¿‡")
                return []
            
            # ç™¾åº¦æ–°é—»æœç´¢ç»“æœé€‰æ‹©å™¨
            # æ–°é—»æ ‡é¢˜åœ¨ h3 > a ä¸­ï¼Œé“¾æ¥æ˜¯çœŸå®çš„ç¬¬ä¸‰æ–¹ URL
            news_h3_links = soup.select('h3 a[href^="http"]')
            logger.info(f"ğŸ“Š ç™¾åº¦æ–°é—»HTMLè§£æ: h3é“¾æ¥={len(news_h3_links)}ä¸ª")
            
            for link in news_h3_links[:num_results * 2]:  # å¤šå–ä¸€äº›ï¼Œåé¢è¿‡æ»¤
                try:
                    url = link.get('href', '')
                    title = link.get_text(strip=True)
                    
                    # æ¸…ç†æ ‡é¢˜ï¼ˆå»æ‰"æ ‡é¢˜ï¼š"å‰ç¼€ï¼‰
                    if title.startswith('æ ‡é¢˜ï¼š'):
                        title = title[3:]
                    
                    # è¿‡æ»¤æ— æ•ˆç»“æœ
                    if not url or not title or len(title) < 5:
                        continue
                    # è¿‡æ»¤ç™¾åº¦å†…éƒ¨é“¾æ¥ï¼ˆä½†ä¿ç•™ç™¾å®¶å· baijiahao.baidu.comï¼‰
                    if 'baidu.com' in url and 'baijiahao.baidu.com' not in url:
                        continue
                    if url in [r['url'] for r in results]:
                        continue  # å»é‡
                    
                    # å°è¯•æ‰¾åˆ°çˆ¶å®¹å™¨è·å–æ‘˜è¦
                    parent = link.find_parent(['div', 'li'])
                    snippet = ''
                    news_source = ''
                    publish_time = ''
                    
                    if parent:
                        # æå–æ‘˜è¦ï¼ˆé€šå¸¸åœ¨ generic æˆ– p å…ƒç´ ä¸­ï¼‰
                        snippet_elem = parent.select_one('[class*="summary"], [class*="abstract"], p')
                        if snippet_elem:
                            snippet = snippet_elem.get_text(strip=True)[:300]
                        
                        # æå–æ¥æºï¼ˆé€šå¸¸åœ¨åŒ…å«"æ¥æº"çš„é“¾æ¥ä¸­ï¼‰
                        source_links = parent.select('a')
                        for src_link in source_links:
                            src_text = src_link.get_text(strip=True)
                            if src_text and src_text != title[:20] and len(src_text) < 20:
                                # å¯èƒ½æ˜¯æ¥æºï¼ˆå¦‚"åŒèŠ±é¡ºè´¢ç»"ã€"æ–°æµªè´¢ç»"ï¼‰
                                if 'æ–°é—»æ¥æº' in (src_link.get('aria-label', '') or ''):
                                    news_source = src_text
                                    break
                                elif not news_source and not src_text.startswith('æ ‡é¢˜'):
                                    news_source = src_text
                    
                    results.append({
                        "url": url,
                        "title": title,
                        "snippet": snippet,
                        "source": "baidu_news",
                        "news_source": news_source  # æ–°é—»æ¥æºï¼ˆå¦‚"åŒèŠ±é¡ºè´¢ç»"ï¼‰
                    })
                    logger.debug(f"  âœ… æ–°é—»: {title[:40]} | {news_source}")
                    
                    if len(results) >= num_results:
                        break
                        
                except Exception as e:
                    logger.debug(f"è§£æç™¾åº¦æ–°é—»ç»“æœå¤±è´¥: {e}")
                    continue
            
            logger.info(f"âœ… ç™¾åº¦æ–°é—»æœç´¢å®Œæˆï¼Œè·å¾— {len(results)} æ¡æ–°é—»")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç™¾åº¦æ–°é—»æœç´¢å¤±è´¥: {e}")
        
        return results
    
    def search_on_sogou(
        self,
        query: str,
        num_results: int = 10
    ) -> List[Dict[str, str]]:
        """
        åœ¨æœç‹—ä¸Šæœç´¢å¹¶è·å–ç»“æœï¼ˆå¤‡ç”¨æœç´¢å¼•æ“ï¼‰
        
        Args:
            query: æœç´¢å…³é”®è¯
            num_results: è·å–çš„ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            # æ„å»ºæœç‹—æœç´¢ URL
            search_url = f"https://www.sogou.com/web?query={quote_plus(query)}"
            
            logger.info(f"ğŸ” æœç‹—æœç´¢: {query}")
            logger.debug(f"æœç´¢URL: {search_url}")
            
            response = self.session.get(search_url, timeout=self.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ£€æµ‹éªŒè¯ç 
            if self._is_captcha_page(response.text, soup):
                logger.warning("âš ï¸ æœç‹—è§¦å‘éªŒè¯ï¼Œè·³è¿‡æ­¤å¼•æ“")
                return []
            
            # ========== è°ƒè¯•ï¼šæ‰“å°æ‰¾åˆ°çš„å…ƒç´  ==========
            vrwrap_items = soup.select('.vrwrap, .rb, .results .vrwrap')
            logger.info(f"ğŸ“Š æœç‹—HTMLè§£æ: .vrwrap/.rb={len(vrwrap_items)}ä¸ª")
            
            # æœç‹—æœç´¢ç»“æœé€‰æ‹©å™¨
            for result in vrwrap_items[:num_results]:
                try:
                    title_elem = result.select_one('h3 a, .vr-title a, a[href]')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    url = title_elem.get('href', '')
                    
                    snippet_elem = result.select_one('.str_info, .str-text, p, .txt-info')
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''
                    
                    if url and title and 'sogou.com' not in url:
                        results.append({
                            "url": url,
                            "title": title,
                            "snippet": snippet[:300],
                            "source": "sogou"
                        })
                        logger.debug(f"  âœ… æå–: {title[:40]} -> {url[:60]}")
                        
                except Exception as e:
                    logger.debug(f"è§£ææœç‹—ç»“æœå¤±è´¥: {e}")
                    continue
            
            # å¤‡é€‰æ–¹æ³•ï¼šä»é¡µé¢é“¾æ¥æå–
            if not results:
                logger.info("âš ï¸ æœç‹—æ ‡å‡†é€‰æ‹©å™¨æ— ç»“æœï¼Œå°è¯•ä»é¡µé¢é“¾æ¥æå–...")
                all_links = soup.select('a[href^="http"]')
                for link in all_links[:num_results * 3]:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    
                    if not href or not text or len(text) < 5:
                        continue
                    if 'sogou.com' in href:
                        continue
                    if href in [r['url'] for r in results]:
                        continue
                    
                    results.append({
                        "url": href,
                        "title": text[:100],
                        "snippet": "",
                        "source": "sogou"
                    })
                    
                    if len(results) >= num_results:
                        break
            
            logger.info(f"âœ… æœç‹—æœç´¢å®Œæˆï¼Œè·å¾— {len(results)} æ¡ç»“æœ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ æœç‹—æœç´¢å¤±è´¥: {e}")
        
        return results
    
    def search_on_360(
        self,
        query: str,
        num_results: int = 10
    ) -> List[Dict[str, str]]:
        """
        åœ¨ 360 æœç´¢ä¸Šæœç´¢å¹¶è·å–ç»“æœ
        
        Args:
            query: æœç´¢å…³é”®è¯
            num_results: è·å–çš„ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            # æ„å»º 360 æœç´¢ URL
            search_url = f"https://www.so.com/s?q={quote_plus(query)}"
            
            logger.info(f"ğŸ” 360æœç´¢: {query}")
            logger.debug(f"æœç´¢URL: {search_url}")
            
            response = self.session.get(search_url, timeout=self.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ£€æµ‹éªŒè¯ç 
            if self._is_captcha_page(response.text, soup):
                logger.warning("âš ï¸ 360è§¦å‘éªŒè¯ï¼Œè·³è¿‡æ­¤å¼•æ“")
                return []
            
            # ========== è°ƒè¯•ï¼šæ‰“å°æ‰¾åˆ°çš„å…ƒç´  ==========
            res_items = soup.select('.res-list, .result, li.res-list')
            logger.info(f"ğŸ“Š 360 HTMLè§£æ: .res-list/.result={len(res_items)}ä¸ª")
            
            # 360 æœç´¢ç»“æœé€‰æ‹©å™¨
            for result in res_items[:num_results]:
                try:
                    title_elem = result.select_one('h3 a, .res-title a, a[href]')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    url = title_elem.get('href', '')
                    
                    snippet_elem = result.select_one('.res-desc, p.res-summary, p, .res-comm-con')
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ''
                    
                    if url and title and 'so.com' not in url and '360.cn' not in url:
                        results.append({
                            "url": url,
                            "title": title,
                            "snippet": snippet[:300],
                            "source": "360"
                        })
                        logger.debug(f"  âœ… æå–: {title[:40]} -> {url[:60]}")
                        
                except Exception as e:
                    logger.debug(f"è§£æ 360 ç»“æœå¤±è´¥: {e}")
                    continue
            
            # å¤‡é€‰æ–¹æ³•ï¼šä»é¡µé¢é“¾æ¥æå–
            if not results:
                logger.info("âš ï¸ 360 æ ‡å‡†é€‰æ‹©å™¨æ— ç»“æœï¼Œå°è¯•ä»é¡µé¢é“¾æ¥æå–...")
                all_links = soup.select('a[href^="http"]')
                for link in all_links[:num_results * 3]:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    
                    if not href or not text or len(text) < 5:
                        continue
                    if 'so.com' in href or '360.cn' in href:
                        continue
                    if href in [r['url'] for r in results]:
                        continue
                    
                    results.append({
                        "url": href,
                        "title": text[:100],
                        "snippet": "",
                        "source": "360"
                    })
                    
                    if len(results) >= num_results:
                        break
            
            logger.info(f"âœ… 360æœç´¢å®Œæˆï¼Œè·å¾— {len(results)} æ¡ç»“æœ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ 360æœç´¢å¤±è´¥: {e}")
        
        return results
    
    def interactive_search(
        self,
        query: str,
        engines: List[str] = None,
        num_results: int = 10,
        search_type: str = "news",  # æ–°å¢å‚æ•°ï¼šnewsï¼ˆæ–°é—»ï¼‰æˆ– webï¼ˆç½‘é¡µï¼‰
        **kwargs  # å…¼å®¹æ—§æ¥å£
    ) -> List[Dict[str, str]]:
        """
        ä½¿ç”¨å¤šä¸ªæœç´¢å¼•æ“è¿›è¡Œæœç´¢
        
        Args:
            query: æœç´¢å…³é”®è¯
            engines: æœç´¢å¼•æ“åˆ—è¡¨ ['baidu_news', 'baidu', 'sogou', '360', 'bing']
            num_results: æ¯ä¸ªå¼•æ“çš„ç»“æœæ•°é‡
            search_type: æœç´¢ç±»å‹ 'news'ï¼ˆæ–°é—»ä¼˜å…ˆï¼‰æˆ– 'web'ï¼ˆç½‘é¡µï¼‰
            
        Returns:
            åˆå¹¶çš„æœç´¢ç»“æœ
        """
        if engines is None:
            if search_type == "news":
                # æ–°é—»æœç´¢ï¼šä¼˜å…ˆä½¿ç”¨ç™¾åº¦èµ„è®¯
                engines = ["baidu_news", "sogou"]
            else:
                # æ™®é€šç½‘é¡µæœç´¢
                engines = ["baidu", "sogou"]
        
        all_results = []
        engines_tried = []
        
        for engine in engines:
            try:
                engine_lower = engine.lower()
                if engine_lower == "baidu_news":
                    results = self.search_on_baidu_news(query, num_results)
                elif engine_lower == "baidu":
                    results = self.search_on_baidu(query, num_results)
                elif engine_lower == "bing":
                    results = self.search_on_bing(query, num_results)
                elif engine_lower == "sogou":
                    results = self.search_on_sogou(query, num_results)
                elif engine_lower == "360":
                    results = self.search_on_360(query, num_results)
                else:
                    logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„æœç´¢å¼•æ“: {engine}")
                    continue
                
                if results:
                    all_results.extend(results)
                    engines_tried.append(engine_lower)
                    logger.info(f"âœ… {engine} è¿”å› {len(results)} æ¡ç»“æœ")
                else:
                    logger.info(f"âš ï¸ {engine} æ— ç»“æœæˆ–è¢«æ‹¦æˆª")
                
                # æœç´¢é—´éš”ï¼Œé¿å…è¢«å°
                if len(engines) > 1:
                    time.sleep(random.uniform(0.8, 1.5))
                    
            except Exception as e:
                logger.error(f"âŒ ä½¿ç”¨ {engine} æœç´¢å¤±è´¥: {e}")
                continue
        
        # å¦‚æœæ‰€æœ‰å¼•æ“éƒ½å¤±è´¥äº†ï¼Œå°è¯•å¤‡ç”¨å¼•æ“
        if not all_results:
            backup_engines = ["baidu_news", "360", "baidu", "sogou"]
            for backup in backup_engines:
                if backup not in [e.lower() for e in engines]:
                    logger.info(f"ğŸ”„ å°è¯•å¤‡ç”¨å¼•æ“: {backup}")
                    try:
                        if backup == "baidu_news":
                            results = self.search_on_baidu_news(query, num_results)
                        elif backup == "360":
                            results = self.search_on_360(query, num_results)
                        elif backup == "baidu":
                            results = self.search_on_baidu(query, num_results)
                        elif backup == "sogou":
                            results = self.search_on_sogou(query, num_results)
                        
                        if results:
                            all_results.extend(results)
                            engines_tried.append(backup)
                            logger.info(f"âœ… å¤‡ç”¨å¼•æ“ {backup} è¿”å› {len(results)} æ¡ç»“æœ")
                            break
                    except Exception as e:
                        logger.warning(f"å¤‡ç”¨å¼•æ“ {backup} ä¹Ÿå¤±è´¥: {e}")
                        continue
        
        # å»é‡
        seen_urls = set()
        unique_results = []
        for r in all_results:
            if r["url"] not in seen_urls:
                seen_urls.add(r["url"])
                unique_results.append(r)
        
        logger.info(f"äº¤äº’å¼æœç´¢å®Œæˆ: {len(all_results)} -> {len(unique_results)} (å»é‡å), ä½¿ç”¨å¼•æ“: {engines_tried}")
        return unique_results
    
    def crawl_page(self, url: str) -> Optional[Dict[str, Any]]:
        """
        çˆ¬å–å•ä¸ªé¡µé¢å†…å®¹
        
        Args:
            url: é¡µé¢ URL
            
        Returns:
            {"url": "...", "title": "...", "content": "...", "text": "...", "html": "..."} æˆ– None
        """
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.encoding = response.apparent_encoding or 'utf-8'
            
            # ä¿å­˜åŸå§‹ HTMLï¼ˆæ¸…ç† NUL å­—ç¬¦ï¼‰
            raw_html = response.text.replace('\x00', '').replace('\0', '')
            
            soup = BeautifulSoup(raw_html, 'html.parser')
            
            # è·å–æ ‡é¢˜ï¼ˆåœ¨ç§»é™¤å…ƒç´ ä¹‹å‰ï¼‰
            title = ''
            title_elem = soup.find('title')
            if title_elem:
                title = title_elem.get_text(strip=True)
            
            # å°è¯•è·å– h1 ä½œä¸ºæ›´å¥½çš„æ ‡é¢˜
            h1_elem = soup.find('h1')
            if h1_elem:
                h1_text = h1_elem.get_text(strip=True)
                if h1_text and len(h1_text) > 5:
                    title = h1_text
            
            # ç§»é™¤æ— å…³å…ƒç´ ï¼ˆç”¨äºæå–æ­£æ–‡ï¼‰
            for elem in soup.select('script, style, iframe, nav, footer, header, aside, .ad, .advertisement, .comment, .sidebar'):
                elem.decompose()
            
            # è·å–ä¸»è¦å†…å®¹
            # ä¼˜å…ˆé€‰æ‹© article, main, .content ç­‰
            main_content = None
            content_selectors = [
                'article', 'main', '.content', '.post-content', '.article-content', 
                '#content', '.main-content', '.news-content', '.article-body',
                '.entry-content', '.post-body', '[itemprop="articleBody"]'
            ]
            for selector in content_selectors:
                main_content = soup.select_one(selector)
                if main_content:
                    break
            
            if not main_content:
                main_content = soup.find('body') or soup
            
            # æå–æ–‡æœ¬
            text_content = main_content.get_text(separator='\n', strip=True)
            
            # æ¸…ç†æ–‡æœ¬
            text_content = re.sub(r'\n{3,}', '\n\n', text_content)
            # ä¸å†æˆªæ–­å†…å®¹ï¼Œä¿ç•™å®Œæ•´æ­£æ–‡ï¼ˆæ•°æ®åº“å­—æ®µåº”è¯¥æ”¯æŒé•¿æ–‡æœ¬ï¼‰
            # text_content = text_content[:5000]  # ç§»é™¤æˆªæ–­
            
            logger.debug(f"ğŸ“„ çˆ¬å–å®Œæˆ: {title[:40]}... | æ­£æ–‡{len(text_content)}å­—ç¬¦ | HTML{len(raw_html) if raw_html else 0}å­—ç¬¦")
            
            return {
                "url": url,
                "title": title,
                "content": text_content,  # å®Œæ•´æ­£æ–‡
                "text": text_content,  # å…¼å®¹å­—æ®µ
                "html": raw_html if raw_html else None  # å®Œæ•´åŸå§‹ HTML
            }
            
        except requests.exceptions.Timeout:
            logger.warning(f"âš ï¸ çˆ¬å–é¡µé¢è¶…æ—¶: {url[:60]}...")
        except Exception as e:
            logger.warning(f"âš ï¸ çˆ¬å–é¡µé¢å¤±è´¥ {url[:60]}...: {e}")
        
        return None
    
    def crawl_search_results(
        self,
        search_results: List[Dict[str, str]],
        max_results: int = 5
    ) -> List[Dict[str, Any]]:
        """
        çˆ¬å–æœç´¢ç»“æœä¸­çš„é¡µé¢å†…å®¹
        
        Args:
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            max_results: æœ€å¤šçˆ¬å–å¤šå°‘ä¸ªé¡µé¢
            
        Returns:
            çˆ¬å–ç»“æœåˆ—è¡¨ [{"url": "...", "title": "...", "content": "..."}]
        """
        crawled = []
        
        for i, result in enumerate(search_results[:max_results]):
            url = result.get("url")
            if not url:
                continue
            
            logger.info(f"ğŸ“„ çˆ¬å–é¡µé¢ {i+1}/{min(max_results, len(search_results))}: {url[:60]}...")
            
            page_data = self.crawl_page(url)
            
            if page_data and page_data.get("content"):
                page_data["snippet"] = result.get("snippet", "")
                page_data["source"] = result.get("source", "web")
                crawled.append(page_data)
                logger.debug(f"âœ… çˆ¬å–æˆåŠŸ: {page_data['title'][:50]}...")
            else:
                # çˆ¬å–å¤±è´¥æ—¶ï¼Œä½¿ç”¨æœç´¢ç»“æœçš„æ‘˜è¦
                crawled.append({
                    "url": url,
                    "title": result.get("title", ""),
                    "content": result.get("snippet", ""),
                    "snippet": result.get("snippet", ""),
                    "source": result.get("source", "web")
                })
                logger.debug(f"âš ï¸ ä½¿ç”¨æ‘˜è¦ä»£æ›¿: {result.get('title', 'N/A')[:50]}...")
            
            # çˆ¬å–é—´éš”
            if i < max_results - 1:
                time.sleep(random.uniform(0.3, 0.8))
        
        logger.info(f"ğŸ“„ é¡µé¢çˆ¬å–å®Œæˆ: {len(crawled)} ä¸ªæˆåŠŸ")
        return crawled


# ä¾¿æ·å‡½æ•°
def create_interactive_crawler(headless: bool = True, **kwargs) -> InteractiveCrawler:
    """åˆ›å»ºäº¤äº’å¼çˆ¬è™«ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
    return InteractiveCrawler()


def search_and_crawl(
    query: str,
    engines: List[str] = None,
    max_search_results: int = 10,
    max_crawl_results: int = 5,
    **kwargs  # å…¼å®¹æ—§æ¥å£
) -> Dict[str, Any]:
    """
    ä¸€ä½“åŒ–æœç´¢å’Œçˆ¬å–å‡½æ•°
    
    Args:
        query: æœç´¢å…³é”®è¯
        engines: æœç´¢å¼•æ“åˆ—è¡¨
        max_search_results: æœ€å¤šè·å–å¤šå°‘ä¸ªæœç´¢ç»“æœ
        max_crawl_results: æœ€å¤šçˆ¬å–å¤šå°‘ä¸ªé¡µé¢
        
    Returns:
        {
            "search_results": [...],
            "crawled_results": [...],
            "total_results": int
        }
    """
    crawler = InteractiveCrawler()
    
    logger.info(f"ğŸ” å¼€å§‹æœç´¢: {query}")
    search_results = crawler.interactive_search(
        query,
        engines=engines,
        num_results=max_search_results
    )
    
    if not search_results:
        logger.warning(f"æœç´¢æœªè¿”å›ç»“æœ: {query}")
        return {
            "search_results": [],
            "crawled_results": [],
            "total_results": 0
        }
    
    logger.info(f"ğŸ“„ å¼€å§‹çˆ¬å–å‰ {max_crawl_results} ä¸ªç»“æœ")
    crawled_results = crawler.crawl_search_results(
        search_results,
        max_results=max_crawl_results
    )
    
    return {
        "search_results": search_results,
        "crawled_results": crawled_results,
        "total_results": len(crawled_results)
    }
