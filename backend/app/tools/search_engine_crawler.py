"""
æœç´¢å¼•æ“çˆ¬è™«å·¥å…·
ç›´æ¥çˆ¬å–æœç´¢å¼•æ“ç»“æœé¡µé¢ï¼ˆBing/Baiduï¼‰
"""
import logging
import re
import requests
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from urllib.parse import quote_plus
from bs4 import BeautifulSoup
import time

logger = logging.getLogger(__name__)


class SearchEngineCrawler:
    """
    æœç´¢å¼•æ“çˆ¬è™«
    ç›´æ¥çˆ¬å– Bing/Baidu æœç´¢ç»“æœ
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æœç´¢å¼•æ“çˆ¬è™«"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        logger.info("ğŸ”§ æœç´¢å¼•æ“çˆ¬è™«å·²åˆå§‹åŒ–")
    
    def _fetch_url(self, url: str, timeout: int = 10) -> Optional[str]:
        """
        çˆ¬å–URLå†…å®¹
        
        Args:
            url: ç›®æ ‡URL
            timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            HTMLå†…å®¹
        """
        try:
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            
            # å°è¯•æ£€æµ‹ç¼–ç 
            if response.encoding == 'ISO-8859-1':
                # å¯¹äºä¸­æ–‡ç½‘ç«™ï¼Œå°è¯•ä½¿ç”¨ gb2312 æˆ– utf-8
                encodings = ['utf-8', 'gb2312', 'gbk']
                for enc in encodings:
                    try:
                        response.encoding = enc
                        _ = response.text
                        break
                    except:
                        continue
            
            return response.text
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–å¤±è´¥ {url}: {e}")
            return None
    
    def search_with_engine(
        self,
        query: str,
        engine: str = "bing",
        days: int = 30,
        max_results: int = 50
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨æœç´¢å¼•æ“æœç´¢æ–°é—»
        
        Args:
            query: æœç´¢å…³é”®è¯
            engine: æœç´¢å¼•æ“ (bing/baidu)
            days: æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼‰
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        if engine not in self.search_engines:
            logger.error(f"âŒ ä¸æ”¯æŒçš„æœç´¢å¼•æ“: {engine}")
            return []
        
        # æ„å»ºæœç´¢URL
        search_query = self._build_search_query(query, days)
        search_url = self.search_engines[engine].format(query=quote_plus(search_query))
        
        logger.info(f"ğŸ” æœç´¢å¼•æ“çˆ¬å–: {engine} - {search_query}")
        logger.info(f"    URL: {search_url}")
        
        # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
        with tempfile.TemporaryDirectory() as temp_dir:
            # çˆ¬å–æœç´¢ç»“æœé¡µé¢
            result = self._call_mcp_crawl(search_url, temp_dir)
            
            if not result:
                logger.warning(f"âš ï¸ æœç´¢å¼•æ“çˆ¬å–å¤±è´¥: {search_url}")
                return []
            
            # è§£ææœç´¢ç»“æœ
            news_items = self._parse_search_results(
                content=result.get("content", ""),
                engine=engine,
                max_results=max_results
            )
            
            logger.info(f"âœ… ä» {engine} æå–åˆ° {len(news_items)} æ¡ç»“æœ")
            return news_items
    
    def _build_search_query(self, query: str, days: int) -> str:
        """
        æ„å»ºæœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆæ·»åŠ æ—¶é—´é™åˆ¶ï¼‰
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            days: æ—¶é—´èŒƒå›´
            
        Returns:
            å¢å¼ºçš„æœç´¢æŸ¥è¯¢
        """
        # æ·»åŠ æ—¶é—´èŒƒå›´ï¼ˆå¯¹äº Bing å’Œ Baiduï¼‰
        # Bing: æ”¯æŒ "query site:xxx.com"
        # å¯ä»¥æ·»åŠ æ–°é—»æºé™åˆ¶
        
        # å¯é€‰ï¼šé™åˆ¶åˆ°æ–°é—»ç½‘ç«™
        news_sites = [
            "sina.com.cn",
            "163.com",
            "eastmoney.com",
            "cnstock.com",
            "stcn.com",
            "caijing.com.cn",
            "yicai.com",
        ]
        
        # æ„å»ºåŸºç¡€æŸ¥è¯¢
        enhanced_query = f"{query} æ–°é—»"
        
        # æ·»åŠ æ—¶é—´æç¤ºè¯
        if days <= 7:
            enhanced_query += " æœ€è¿‘ä¸€å‘¨"
        elif days <= 30:
            enhanced_query += " æœ€è¿‘ä¸€ä¸ªæœˆ"
        
        return enhanced_query
    
    def _parse_search_results(
        self,
        content: str,
        engine: str,
        max_results: int
    ) -> List[Dict[str, Any]]:
        """
        è§£ææœç´¢å¼•æ“è¿”å›çš„å†…å®¹ï¼Œæå–æ–°é—»é“¾æ¥å’Œæ ‡é¢˜
        
        Args:
            content: çˆ¬å–çš„é¡µé¢å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰
            engine: æœç´¢å¼•æ“ç±»å‹
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ–°é—»æ¡ç›®åˆ—è¡¨
        """
        news_items = []
        
        # ä» Markdown å†…å®¹ä¸­æå–é“¾æ¥
        # æ ¼å¼ï¼š[æ ‡é¢˜](URL)
        link_pattern = r'\[([^\]]+)\]\(([^\)]+)\)'
        matches = re.findall(link_pattern, content)
        
        for title, url in matches[:max_results]:
            # è¿‡æ»¤æ‰æœç´¢å¼•æ“è‡ªèº«çš„é“¾æ¥
            if engine in url.lower():
                continue
            
            # è¿‡æ»¤æ‰éæ–°é—»é“¾æ¥
            if not self._is_news_url(url):
                continue
            
            news_items.append({
                "title": title.strip(),
                "url": url.strip(),
                "snippet": "",  # æš‚æ—¶ä¸ºç©ºï¼Œåç»­å¯ä»¥ä» content ä¸­æå–
                "source": self._extract_source_from_url(url),
                "engine": engine
            })
        
        return news_items
    
    def _is_news_url(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ–°é—»URL"""
        news_domains = [
            "sina.com", "163.com", "eastmoney.com", "cnstock.com",
            "stcn.com", "caijing.com", "yicai.com", "nbd.com",
            "jwview.com", "eeo.com.cn", "finance.qq.com"
        ]
        return any(domain in url.lower() for domain in news_domains)
    
    def _extract_source_from_url(self, url: str) -> str:
        """ä»URLæå–æ¥æº"""
        domain_mapping = {
            "sina.com": "æ–°æµªè´¢ç»",
            "163.com": "ç½‘æ˜“è´¢ç»",
            "eastmoney.com": "ä¸œæ–¹è´¢å¯Œ",
            "cnstock.com": "ä¸­å›½è¯åˆ¸ç½‘",
            "stcn.com": "è¯åˆ¸æ—¶æŠ¥",
            "caijing.com": "è´¢ç»ç½‘",
            "yicai.com": "ç¬¬ä¸€è´¢ç»",
            "nbd.com": "æ¯æ—¥ç»æµæ–°é—»",
            "jwview.com": "é‡‘èç•Œ",
            "eeo.com.cn": "ç»æµè§‚å¯Ÿç½‘",
            "qq.com": "è…¾è®¯è´¢ç»",
        }
        
        for domain, source in domain_mapping.items():
            if domain in url.lower():
                return source
        
        return "æœªçŸ¥æ¥æº"
    
    def search_stock_news(
        self,
        stock_name: str,
        stock_code: str,
        days: int = 30,
        engines: Optional[List[str]] = None,
        max_per_engine: int = 30
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢è‚¡ç¥¨æ–°é—»ï¼ˆå¤šæœç´¢å¼•æ“ï¼‰
        
        Args:
            stock_name: è‚¡ç¥¨åç§°
            stock_code: è‚¡ç¥¨ä»£ç 
            days: æ—¶é—´èŒƒå›´
            engines: æœç´¢å¼•æ“åˆ—è¡¨ï¼Œé»˜è®¤ ["bing"]
            max_per_engine: æ¯ä¸ªæœç´¢å¼•æ“æœ€å¤§ç»“æœæ•°
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        if engines is None:
            engines = ["bing"]  # é»˜è®¤åªç”¨ Bingï¼ˆBaidu å¯èƒ½éœ€è¦å¤„ç†åçˆ¬ï¼‰
        
        all_news = []
        
        # æ„å»ºæœç´¢å…³é”®è¯
        queries = [
            stock_name,
            f"{stock_name} {stock_code}",
            f"{stock_name} å…¬å‘Š",
        ]
        
        for engine in engines:
            for query in queries:
                try:
                    news = self.search_with_engine(
                        query=query,
                        engine=engine,
                        days=days,
                        max_results=max_per_engine
                    )
                    all_news.extend(news)
                except Exception as e:
                    logger.error(f"âŒ æœç´¢å¤±è´¥ [{engine}] {query}: {e}")
        
        # å»é‡ï¼ˆæŒ‰URLï¼‰
        seen_urls = set()
        unique_news = []
        for news in all_news:
            url = news.get("url")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_news.append(news)
        
        logger.info(f"âœ… å¤šå¼•æ“æœç´¢å®Œæˆ: æ€»è®¡ {len(unique_news)} æ¡ï¼ˆå»é‡åï¼‰")
        return unique_news


# ä¾¿æ·å‡½æ•°
def create_search_engine_crawler(mcp_server_path: Optional[str] = None) -> SearchEngineCrawler:
    """åˆ›å»ºæœç´¢å¼•æ“çˆ¬è™«å®ä¾‹"""
    return SearchEngineCrawler(mcp_server_path)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    crawler = create_search_engine_crawler()
    
    # æµ‹è¯•æœç´¢
    results = crawler.search_stock_news(
        stock_name="æ·±æŒ¯ä¸šA",
        stock_code="000006",
        days=7,
        engines=["bing"],
        max_per_engine=10
    )
    
    print(f"\nâœ… æœç´¢åˆ° {len(results)} æ¡æ–°é—»:")
    for i, news in enumerate(results[:5], 1):
        print(f"{i}. {news['title']}")
        print(f"   æ¥æº: {news['source']}")
        print(f"   URL: {news['url']}")

