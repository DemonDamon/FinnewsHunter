# 上市公司新闻文本分析与分类预测

 ![image](https://github.com/DemonDamon/Listed-company-news-crawl-and-text-analysis/blob/master/docs/images/FINNEWS-HUNTER.jpg)

-------------------------------

## 简介

上市公司新闻文本分析与分类预测的基本步骤如下：

 - 从新浪财经、每经网、金融界、中国证券网、证券时报网上，爬取上市公司（个股）的历史新闻文本数据（包括时间、网址、标题、正文）
 - 从Tushare上获取沪深股票日线数据（开、高、低、收、成交量和持仓量）和基本信息（包括股票代码、股票名称、所属行业、所属地区、PE值、总资产、流动资产、固定资产、留存资产等）
 - 对抓取的新闻文本按照，去停用词、加载新词、分词的顺序进行处理
 - 利用前两步中所获取的股票名称和分词后的结果，抽取出每条新闻里所包含的（0支、1支或多支）股票名称，并将所对应的所有股票代码，组合成与该条新闻相关的股票代码列表，并在历史数据表中增加一列相关股票代码数据
 - 从历史新闻数据库中抽取与某支股票相关的所有新闻文本，利用该支股票的日线数据（比如某一天发布的消息，在设定N天后如果价格上涨则认为是利好消息，反之则是利空消息）给每条新闻贴上“利好”和“利空”的标签，并存储到新的数据库中（或导出到CSV文件）
 - 实时抓取新闻数据，判断与该新闻相关的股票有哪些，利用上一步的结果，对与某支股票相关的所有历史新闻文本（已贴标签）进行文本分析（构建新的特征集），然后利用SVM（或随机森林）分类器对文本分析结果进行训练（如果已保存训练模型，可选择重新训练或直接加载模型），最后利用训练模型对实时抓取的新闻数据进行分类预测

开发环境`Python-v3(3.6)`：

 - gensim==3.2.0
 - jieba==0.39
 - scikit-learn==0.19.1
 - pandas==0.20.0
 - numpy==1.13.3+mkl
 - scipy==0.19.0
 - pymongo==3.6.0
 - beautifulsoup4==4.6.0
 - tushare==1.1.1
 - requests==2.18.4
 - gevent==1.2.1

## 文本处理(`text_processing.py`)

 - 文本处理包括去停用词处理、加载新词、中文分词、去掉出现次数少的分词
 - 生成字典和Bow向量，并基于Gensim转化模型（LSI、LDA、TF-IDF）转化Bow向量
 - 计算文本相似度
 - 打印词云

## 文本挖掘（`text_mining.py`）

 - 从新闻文本中抽取特定信息，并贴上新的文本标签方便往后训练模型
 - 从数据库中抽取与某支股票相关的所有新闻文本
 - 将贴好标签的历史新闻进行分类训练，利用训练好的模型对实时抓取的新闻文本进行分类预测

## 新闻爬取（`crawler_cnstock.py`，`crawler_jrj.py`，`crawler_nbd.py`，`crawler_sina.py`，`crawler_stcn.py`）

 - 分析网站结构，多线程（或协程）爬取上市公司历史新闻数据

## Tushare数据提取（`crawler_tushare.py`）

 - 获取沪深所有股票的基本信息，包括股票代码、股票名称、所属行业、所属地区等

## 用法

 - 配好运行环境以及安装MongoDB，最好再安装一个MongoDB的可视化管理工具Studio 3T
 - 先运行`run_crawler_cnstock.py`，`run_crawler_jrj.py`，`run_crawler_nbd.py`，`run_crawler_sina.py`，`run_crawler_stcn.py`这5个py文件，而且可能因为对方服务器没有响应而重复多次运行这几个文件才能抓取大量的历史数据
 - 接着运行`run_crawler_tushare.py`从Tushare获取基本信息和股票价格
 - 最后运行`run_main.py`文件，其中有4个步骤，除了第1步初始化外，其他几步最好单独运行
 - 注意：所有程序都必须在文件所在目录下运行

## 更新目标

 由于之前的项目代码是在初学Python的时候写的，很多写法都是入门级别，因此为了提高整体项目的质量，除了优化代码细节和已有的功能模块之外，还加入了多个功能模块，来支撑未来更加智能化和个性化的金融分析与交易。
 - 完成初步构想，重构该项目，将项目分成8大模块，分别是`数据获取模块`，`数据清洗与预处理模块`，`大数据可视化模块`，`基于机器学习的文本挖掘模块`，`金融知识图谱构建模块`，`任务导向多轮对话模块`，`金融交易模块`，`通用服务模块`
 (备注：项目在完善之后会重新更名为`Finnews Hunter`，命名的来源是出于对`《全职猎人》`的喜爱，与项目本质的结合，其中`Finnews`是`Financial News`的简写。上面提到的8个模块，分别由`《全职猎人》`中的本人最喜爱的8位角色命名，分别是
 - `数据获取模块`               -> `Gon` -> `网页爬虫、各种数据源API调用等`
 - `数据清洗与预处理模块`       -> `Killua` -> `数据清洗、数据转换(数据采样、类型转换、归一化等)、数据描述(数据可视化)、特征选择与组合(熵增益和分支定界等)、特征抽取(主成分分析、线性判别分析等)`
 - `大数据可视化模块`           -> `Kurapika` -> `基于多个可视化模块进行封装，包括提供Web可视化界面`
 - `自然语言处理模块`           -> `Leorio` -> `中文分词、词性标注、实体识别`
 - `基于机器学习的文本挖掘模块` -> `Hisoka`  -> ``
 - `金融知识图谱构建模块`       -> `Chrollo` -> ``
 - `任务导向多轮对话模块`       -> `Illumi` -> ``
 - `金融交易模块`               -> `Feitan` -> ``
 - `基础与Web服务模块`          -> `Kite` -> `基础服务集，包括基本参数配置文件(.py)、数据库的构建与连接、日志打印与收集、多线程服务、Web服务框架搭建以及其他函数`)
 
 ## 更新日志
 
 - 更新`crawler_tushare.py`代码为[stockinfospyder.py](https://github.com/DemonDamon/Listed-company-news-crawl-and-text-analysis/blob/master/src/Gon/stockinfospyder.py)，直接运行即可获取股票历史价格数据，并在每天15:30分后更新数据(目前只采集天数据)
 - 更新`crawler_cnstock.py`代码为`cnstockspyder.py`，直接运行即可获取中国证券网历史新闻数据，并可以实时更新采集
 - 更新`crawler_jrj.py`代码为`jrjspyder.py`，直接运行即可获取金融界历史新闻数据，并可以实时更新采集
 - 更新`crawler_nbd.py`代码为`nbdspyder.py`，直接运行即可获取每经网历史新闻数据，并可以实时更新采集
 - 更新`crawler_sina.py`代码为`sinaspyder.py`，直接运行即可获取新浪财经历史新闻数据(未更新)
 - 停止`证券时报网`爬虫代码的更新(旧代码已不可用)，新增`网易财经`和`凤凰财经`的爬虫代码(未更新)
 - 更新前使用jieba分词系统，在实体识别上需要不断维护新词表来提高识别精度；更新后，使用基于BERT预训练的FinBERT对金融领域实体进行识别
